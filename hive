
版本：V4.8.3

第1章 Hive入门
1.1 什么是Hive
1）Hive简介
Hive是由Facebook开源，基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能。
那为什么会有Hive呢？它是为了解决什么问题而诞生的呢？
下面通过一个案例，来快速了解一下Hive。
例如：需求，统计单词出现个数。
（1）在Hadoop课程中我们用MapReduce程序实现的，当时需要写Mapper、Reducer和Driver三个类，并实现对应逻辑，相对繁琐。
test表
id列

atguigu
atguigu
ss
ss
jiao
banzhang
xue
hadoop
（2）如果通过Hive SQL实现，一行就搞定了，简单方便，容易理解。
select count(*) from test group by id;
2）Hive本质
Hive是一个Hadoop客户端，用于将HQL（Hive SQL）转化成MapReduce程序。
（1）Hive中每张表的数据存储在HDFS
（2）Hive分析数据底层的实现是MapReduce（也可配置为Spark或者Tez） 
（3）执行程序运行在Yarn上
1.2 Hive架构原理
 
1）用户接口：Client
CLI（command-line interface）、JDBC/ODBC。
说明：JDBC和ODBC的区别。
（1）JDBC的移植性比ODBC好；（通常情况下，安装完ODBC驱动程序之后，还需要经过确定的配置才能够应用。而不相同的配置在不相同数据库服务器之间不能够通用。所以，安装一次就需要再配置一次。JDBC只需要选取适当的JDBC数据库驱动程序，就不需要额外的配置。在安装过程中，JDBC数据库驱动程序会自己完成有关的配置。）
（2）两者使用的语言不同，JDBC在Java编程时使用，ODBC一般在C/C++编程时使用。
2）元数据：Metastore
元数据包括：数据库（默认是default）、表名、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等。
默认存储在自带的derby数据库中，由于derby数据库只支持单客户端访问，生产环境中为了多人开发，推荐使用MySQL存储Metastore。
3）驱动器：Driver
（1）解析器（SQLParser）：将SQL字符串转换成抽象语法树（AST）
该组件主要完成两项工作，分别是词法分析和语法分析。
词法分析，会逐个字符扫描源程序（查询语句），根据预设规则识别每个单词符号（Token），将源程序字符串改造为单词列表。
语法分析，会根据预设的语法规则对上述单词列表进行分析，将单词组成“语句”或者“表达式”，并用一个树形结构来表达语法结构，这个树通常称为抽象语法树（AST）。
（2）语义分析（Semantic Analyzer）：将AST进一步划分为QeuryBlock
该组件会将抽象语法树划分为一个个的基本单元，每个基本单元会包含三个部分，分别是输入源、计算过程和输出。简单来讲，一个基本单元就是一个子查询。一个基本单元，在Hive中称为一个QueryBlock。除此之外，Hive还会为每个基本单元赋予详细的元数据信息（例如输入源和输出的路径信息）。
（3）逻辑计划生成器（Logical Plan Gen）：将语法树生成逻辑计划
该组件会将抽象语法树改造为逻辑操作树。逻辑操作树由一系列Hive预先定义的逻辑操作符（Operator）组成，每个逻辑操作符完成一个特定的操作。基本的操作符包括TableScanOperator、SelectOperator、FilterOperator、JoinOperator、GroupByOperator和ReduceSinkOperator等。
（4）逻辑优化器（Logical Optimizer）：对逻辑计划进行优化
该组件会对上述逻辑计划进行一系列优化，例如：常见的谓词下推优化就是在该阶段完成。谓词下推，会尽量将FilterOperator下推至逻辑操作树中靠前的位置。
（5）物理计划生成器（Physical Plan Gen）：根据优化后的逻辑计划生成物理计划
该组件会将优化后的逻辑执行计划转化为物理执行计划，也就是一系列MapReduce、Spark或者Tez任务，具体生成任务种类由使用的计算引擎决定。
（6）物理优化器（Physical Optimizer）：对物理计划进行优化
该组件会对上述物理执行计划进行一系列优化。例如：Hive中的MapJoin优化就是在该阶段完成的。
（7）执行器（Execution）：执行该计划，得到查询结果并返回给客户端
 
 
4）Hadoop
使用HDFS进行存储，可以选择MapReduce/Tez/Spark进行计算。
第2章 Hive安装
2.1 Hive安装地址
1）Hive官网地址：http://hive.apache.org/
2）文档查看地址：https://cwiki.apache.org/confluence/display/Hive/GettingStarted
3）下载地址：http://archive.apache.org/dist/hive/
4）github地：https://github.com/apache/hive
2.2 Hive安装部署
2.2.1 安装Hive
1）把apache-hive-3.1.3-bin.tar.gz上传到Linux的/opt/software目录下
2）解压apache-hive-3.1.3-bin.tar.gz到/opt/module/目录下面
[atguigu@hadoop102 software]$ tar -zxvf /opt/software/apache-hive-3.1.3-bin.tar.gz -C /opt/module/
3）修改apache-hive-3.1.3-bin.tar.gz的名称为hive
[atguigu@hadoop102 software]$ mv /opt/module/apache-hive-3.1.3-bin/ /opt/module/hive
4）修改/etc/profile.d/my_env.sh，添加环境变量
[atguigu@hadoop102 software]$ sudo vim /etc/profile.d/my_env.sh
（1）添加内容
#HIVE_HOME
export HIVE_HOME=/opt/module/hive
export PATH=$PATH:$HIVE_HOME/bin
（2）source一下
[atguigu@hadoop102 hive]$ source /etc/profile.d/my_env.sh
5）初始化元数据库（默认是derby数据库）
[atguigu@hadoop102 hive]$ bin/schematool -dbType derby -initSchema
2.2.2 使用Hive
1）启动Hive
[atguigu@hadoop102 hive]$ bin/hive
2）使用Hive
hive> show databases;
hive> show tables;
hive> create table stu(id int, name string);
hive> insert into stu values(1,"ss");
hive> select * from stu;
观察HDFS的路径/user/hive/warehouse/stu，体会Hive与Hadoop之间的关系。
Hive中的表在Hadoop中是目录；Hive中的数据在Hadoop中是文件。
 
 
3）在Xshell窗口中开启另一个窗口开启Hive，在/tmp/atguigu目录下监控hive.log文件
[atguigu@hadoop102 atguigu]$ tail -f hive.log


Caused by: ERROR XSDB6: Another instance of Derby may have already booted the database /opt/module/hive/metastore_db.
        at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
        at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
        at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)
        at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)
...
原因在于Hive默认使用的元数据库为derby。derby数据库的特点是同一时间只允许一个客户端访问。如果多个Hive客户端同时访问，就会报错。由于在企业开发中，都是多人协作开发，需要多客户端同时访问Hive，怎么解决呢？我们可以将Hive的元数据改为用MySQL存储，MySQL支持多客户端同时访问。
 
4）首先退出hive客户端。然后在Hive的安装目录下将derby.log和metastore_db删除，顺便将HDFS上目录删除
hive> quit;
[atguigu@hadoop102 hive]$ rm -rf derby.log metastore_db
[atguigu@hadoop102 hive]$ hadoop fs -rm -r /user
5）删除HDFS中/user/hive/warehouse/stu中数据
 
 
2.3 MySQL安装
2.3.1 安装MySQL
1）上传MySQL安装包以及MySQL驱动jar包
mysql-community-client-8.0.31-1.el7.x86_64.rpm
mysql-community-client-plugins-8.0.31-1.el7.x86_64.rpm
mysql-community-common-8.0.31-1.el7.x86_64.rpm
mysql-community-icu-data-files-8.0.31-1.el7.x86_64.rpm
mysql-community-libs-8.0.31-1.el7.x86_64.rpm
mysql-community-libs-compat-8.0.31-1.el7.x86_64.rpm
mysql-community-server-8.0.31-1.el7.x86_64.rpm
mysql-connector-j-8.0.31.jar
2）安装MySQL 
[atguigu@hadoop102 software]$ yum install -y *.rpm
3）启动MySQL
[atguigu@hadoop102 software]$ sudo systemctl start mysqld
4）更改MySQL密码级别
[atguigu@hadoop102 software]$ sudo vim /etc/my.cnf
# 在[mysqld]下面添加如下两行并保存
validate_password.length=4
validate_password.policy=0

# 保存过后重启MySQL
[atguigu@hadoop102 software]$ sudo systemctl start mysqld
5）查看MySQL密码
[atguigu@hadoop102 software]$ sudo cat /var/log/mysqld.log | grep "temporary password" | awk '{print $NF}'
2.3.2 配置MySQL
配置主要是root用户 + 密码，在任何主机上都能登录MySQL数据库。
1）用刚刚查到的密码进入MySQL（如果报错，给密码加单引号）
[atguigu@hadoop102 software]$ mysql -uroot -p'password'
2）设置简单好记的密码
mysql> set password=password("123456");
3）允许MySQL以本地密码认证以兼容旧版JDBC驱动
mysql> alter user 'root'@'%' identified with mysql_native_password by '123456';
4）进入MySQL库
mysql> use mysql
5）查询user表
mysql> select user, host from user;
6）修改user表，把Host表内容修改为%
mysql> update user set host="%" where user="root";
7）刷新
mysql> flush privileges;
8）退出
mysql> quit;
2.3.3 卸载MySQL
若因为安装失败或者其他原因，MySQL需要卸载重装，可参考以下内容。
1）清空原有数据
（1）停止MySQL服务
[atguigu@hadoop102 software]$ sudo systemctl stop mysqld
（1）通过/etc/my.cnf查看MySQL数据的存储位置
[atguigu@hadoop102 software]$ sudo cat /etc/my.cnf
[mysqld]
datadir=/var/lib/mysql
（2）去往/var/lib/mysql路径需要root权限
[atguigu@hadoop102 mysql]$ su - root
[root@hadoop102 ~]# cd /var/lib/mysql
[root@hadoop102 mysql]# rm -rf *  （注意敲击命令的位置）
2）卸载MySQL相关包
（1）查看安装过的MySQL相关包
[atguigu@hadoop102 software]$ sudo rpm -qa | grep -i -E mysql

mysql-community-libs-8.0.31-1.el7.x86_64
mysql-community-libs-compat-8.0.31-1.el7.x86_64
mysql-community-common-8.0.31-1.el7.x86_64
mysql-community-client-8.0.31-1.el7.x86_64
mysql-community-server-8.0.31-1.el7.x86_64
mysql-community-client-plugins-8.0.31-1.el7.x86_64
mysql-community-icu-data-files-8.0.31-1.el7.x86_64
（2）一键卸载命令
[atguigu@hadoop102 software]$ rpm -qa | grep -i -E mysql\|mariadb | xargs -n1 sudo rpm -e --nodeps
2.4 配置Hive元数据存储到MySQL
 
2.4.1 配置元数据到MySQL
1）新建Hive元数据库
#登录MySQL
[atguigu@hadoop102 software]$ mysql -uroot -p123456

#创建Hive元数据库
mysql> create database metastore;
mysql> quit;
2）将MySQL的JDBC驱动拷贝到Hive的lib目录下。
[atguigu@hadoop102 software]$ cp /opt/software/mysql-connector-j-8.0.31.jar $HIVE_HOME/lib
3）在$HIVE_HOME/conf目录下新建hive-site.xml文件
[atguigu@hadoop102 software]$ vim $HIVE_HOME/conf/hive-site.xml
添加如下内容：
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
    <!-- jdbc连接的URL -->
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://hadoop102:3306/metastore?useSSL=false&amp;allowPublicKeyRetrieval=true&amp;createDatabaseIfNotExist=true&amp;useUnicode=true&amp;characterEncoding=UTF-8</value>
    </property>    
    <!-- jdbc连接的Driver-->
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.cj.jdbc.Driver</value>
    </property>
    
    <!-- jdbc连接的username-->
    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>root</value>
    </property>

    <!-- jdbc连接的password -->
    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>123456</value>
    </property>

    <!-- Hive默认在HDFS的工作目录 -->
    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/user/hive/warehouse</value>
</property>
</configuration>
5）初始化Hive元数据库（修改为采用MySQL存储元数据）
[atguigu@hadoop102 hive]$ bin/schematool -dbType mysql -initSchema -verbose
2.4.2 验证元数据是否配置成功
1）再次启动Hive
[atguigu@hadoop102 hive]$ bin/hive
2）使用Hive
hive> show databases;
hive> show tables;
hive> create table stu(id int, name string);
hive> insert into stu values(1,"ss");
hive> select * from stu;
3）在Xshell窗口中开启另一个窗口开启Hive（两个窗口都可以操作Hive，没有出现异常）
hive> show databases;
hive> show tables;
hive> select * from stu;
2.4.3 查看MySQL中的元数据
1）登录MySQL
[atguigu@hadoop102 hive]$ mysql -uroot -p123456
2）查看元数据库metastore
mysql> show databases;
mysql> use metastore;
mysql> show tables;
（1）查看元数据库中存储的库信息
mysql> select * from DBS;
+-------+-----------------------+-------------------------------------------+---------+------------+------------+-----------+
| DB_ID | DESC                  | DB_LOCATION_URI                           | NAME    | OWNER_NAME | OWNER_TYPE | CTLG_NAME |
+-------+-----------------------+-------------------------------------------+---------+------------+------------+-----------+
|     1 | Default Hive database | hdfs://hadoop102:8020/user/hive/warehouse | default | public     | ROLE       | hive      |
+-------+-----------------------+-------------------------------------------+---------+------------+------------+-----------+
（2）查看元数据库中存储的表信息
mysql> select * from TBLS;
+--------+-------------+-------+------------------+---------+------------+-----------+-------+----------+---------------+
| TBL_ID | CREATE_TIME | DB_ID | LAST_ACCESS_TIME | OWNER   | OWNER_TYPE | RETENTION | SD_ID | TBL_NAME | TBL_TYPE      | 
+--------+-------------+-------+------------------+---------+------------+-----------+-------+----------+---------------+
|      1 |  1656318303 |     1 |                0 | atguigu | USER       |         0 |     1 | stu      | MANAGED_TABLE |
+--------+-------------+-------+------------------+---------+------------+-----------+-------+----------+---------------+
（3）查看元数据库中存储的表中列相关信息
mysql> select * from COLUMNS_V2;
+-------+----------+---------+------------+-------------+-------------+--------+
| CS_ID | CAT_NAME | DB_NAME | TABLE_NAME | COLUMN_NAME | COLUMN_TYPE | TBL_ID |
+-------+----------+---------+------------+-------------+-------------+--------+
|     1 | hive     | default | stu        | id          | int         |      1 |
|     2 | hive     | default | stu        | name        | string      |      1 |
+-------+----------+---------+------------+-------------+-------------+--------+
2.5 Hive服务部署
2.5.1 hiveserver2服务
Hive的hiveserver2服务的作用是提供jdbc/odbc接口，为用户提供远程访问Hive数据的功能，例如用户期望在个人电脑中访问远程服务中的Hive数据，就需要用到Hiveserver2。
 1）用户说明
在远程访问Hive数据时，客户端并未直接访问Hadoop集群，而是由Hivesever2代理访问。由于Hadoop集群中的数据具备访问权限控制，所以此时需考虑一个问题：那就是访问Hadoop集群的用户身份是谁？是Hiveserver2的启动用户？还是客户端的登录用户？
答案是都有可能，具体是谁，由Hiveserver2的hive.server2.enable.doAs参数决定，该参数的含义是是否启用Hiveserver2用户模拟的功能。若启用，则Hiveserver2会模拟成客户端的登录用户去访问Hadoop集群的数据，不启用，则Hivesever2会直接使用启动用户访问Hadoop集群数据。模拟用户的功能，默认是开启的。
具体逻辑如下：
（1）未开启用户模拟功能：
 
（2）开启用户模拟功能：
 	生产环境，推荐开启用户模拟功能，因为开启后才能保证各用户之间的权限隔离。
2）hiveserver2部署
（1）Hadoop端配置
hivesever2的模拟用户功能，依赖于Hadoop提供的proxy user（代理用户功能），只有Hadoop中的代理用户才能模拟其他用户的身份访问Hadoop集群。因此，需要将hiveserver2的启动用户设置为Hadoop的代理用户，配置方式如下：
修改配置文件core-site.xml，然后记得分发三台机器。
[atguigu@hadoop102 ~]$ cd $HADOOP_HOME/etc/hadoop
[atguigu@hadoop102 hadoop]$ vim core-site.xml
增加如下配置：
<!--配置所有节点的atguigu用户都可作为代理用户-->
<property>
    <name>hadoop.proxyuser.atguigu.hosts</name>
    <value>*</value>
</property>

<!--配置atguigu用户能够代理的用户组为任意组-->
<property>
    <name>hadoop.proxyuser.atguigu.groups</name>
    <value>*</value>
</property>

<!--配置atguigu用户能够代理的用户为任意用户-->
<property>
    <name>hadoop.proxyuser.atguigu.users</name>
    <value>*</value>
</property>
（2）Hive端配置
在hive-site.xml文件中添加如下配置信息。
[atguigu@hadoop102 conf]$ vim hive-site.xml

<!-- 指定hiveserver2连接的host -->
<property>
	<name>hive.server2.thrift.bind.host</name>
	<value>hadoop102</value>
</property>

<!-- 指定hiveserver2连接的端口号 -->
<property>
	<name>hive.server2.thrift.port</name>
	<value>10000</value>
</property>
3）测试
（1）启动hiveserver2
[atguigu@hadoop102 hive]$ bin/hive --service hiveserver2
（2）使用命令行客户端beeline进行远程访问
启动beeline客户端。
[atguigu@hadoop102 hive]$ bin/beeline -u jdbc:hive2://hadoop102:10000 -n atguigu
看到如下界面。
Connecting to jdbc:hive2://hadoop102:10000
Connected to: Apache Hive (version 3.1.3)
Driver: Hive JDBC (version 3.1.3)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Beeline version 3.1.3 by Apache Hive
0: jdbc:hive2://hadoop102:10000>
（3）使用Datagrip图形化客户端进行远程访问
4）配置DataGrip连接
（1）创建连接
 
（2）配置连接属性
所有属性配置，和Hive的beeline客户端配置一致即可。初次使用，配置过程会提示缺少JDBC驱动，按照提示下载即可。
 
（3）界面介绍
 
（4）测试sql执行
 
（5）修改数据库
 
2.5.2 metastore服务
Hive的metastore服务的作用是为Hive CLI或者Hiveserver2提供元数据访问接口。
1）metastore运行模式
metastore有两种运行模式，分别为嵌入式模式和独立服务模式。下面分别对两种模式进行说明：
（1）嵌入式模式
 
（2）独立服务模式
 
生产环境中，不推荐使用嵌入式模式。因为其存在以下两个问题：
（1）嵌入式模式下，每个Hive CLI都需要直接连接元数据库，当Hive CLI较多时，数据库压力会比较大。
（2）每个客户端都需要用户元数据库的读写权限，元数据库的安全得不到很好的保证。
2）metastore部署
（1）嵌入式模式
嵌入式模式下，只需保证Hiveserver2和每个Hive CLI的配置文件hive-site.xml中包含连接元数据库所需要的以下参数即可：
<!-- jdbc连接的URL -->
<property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://hadoop102:3306/metastore?useSSL=false</value>
</property>

<!-- jdbc连接的Driver-->
<property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>com.mysql.jdbc.Driver</value>
</property>

<!-- jdbc连接的username-->
<property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>root</value>
</property>

<!-- jdbc连接的password -->
<property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>123456</value>
</property>
（2）独立服务模式
独立服务模式需做以下配置：
首先，保证metastore服务的配置文件hive-site.xml中包含连接元数据库所需的以下参数：
<!-- jdbc连接的URL -->
<property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://hadoop102:3306/metastore?useSSL=false</value>
</property>

<!-- jdbc连接的Driver-->
<property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>com.mysql.jdbc.Driver</value>
</property>

<!-- jdbc连接的username-->
<property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>root</value>
</property>

<!-- jdbc连接的password -->
<property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>123456</value>
</property>
其次，保证Hiveserver2和每个Hive CLI的配置文件hive-site.xml中包含访问metastore服务所需的以下参数：
<!-- 指定metastore服务的地址 -->
<property>
	<name>hive.metastore.uris</name>
	<value>thrift://hadoop102:9083</value>
</property>
注意：主机名需要改为metastore服务所在节点，端口号无需修改，metastore服务的默认端口就是9083。
3）测试
此时启动Hive CLI，执行show databases语句，会出现一下错误提示信息：
hive (default)> show databases;
FAILED: HiveException java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
这是因为我们在Hive CLI的配置文件中配置了hive.metastore.uris参数，此时Hive CLI会去请求我们执行的metastore服务地址，所以必须启动metastore服务才能正常使用。
metastore服务的启动命令如下：
[atguigu@hadoop102 hive]$ hive --service metastore
2022-04-24 16:58:08: Starting Hive Metastore Server
注意：启动后该窗口不能再操作，需打开一个新的Xshell窗口来对Hive操作。
重新启动 Hive CLI，并执行show databases语句，就能正常访问了。
[atguigu@hadoop102 hive]$ bin/hive
2.5.3 编写Hive服务启动脚本（了解）
1）前台启动的方式导致需要打开多个Xshell窗口，可以使用如下方式后台方式启动
	nohup：放在命令开头，表示关闭终端进程也继续保持运行状态
	/dev/null：是Linux文件系统中的一个文件，被称为黑洞，所有写入该文件的内容都会被自动丢弃
	2>&1：表示将错误重定向到标准输出上
	&：放在命令结尾，表示后台运行
一般会组合使用：nohup  [xxx命令操作]> file  2>&1 &，表示将xxx命令运行的结果输出到file中，并保持命令启动的进程在后台运行。
如上命令不要求掌握。
[atguigu@hadoop202 hive]$ nohup hive --service metastore 2>&1 &
[atguigu@hadoop202 hive]$ nohup hive --service hiveserver2 2>&1 &
2）为了方便使用，可以直接编写脚本来管理服务的启动和关闭
[atguigu@hadoop102 hive]$ vim $HIVE_HOME/bin/hiveservices.sh
内容如下：此脚本的编写不要求掌握。直接拿来使用即可。
#!/bin/bash

HIVE_LOG_DIR=$HIVE_HOME/logs
if [ ! -d $HIVE_LOG_DIR ]
then
	mkdir -p $HIVE_LOG_DIR
fi

#检查进程是否运行正常，参数1为进程名，参数2为进程端口
function check_process()
{
    pid=$(ps -ef 2>/dev/null | grep -v grep | grep -i $1 | awk '{print $2}')
    ppid=$(netstat -nltp 2>/dev/null | grep $2 | awk '{print $7}' | cut -d '/' -f 1)
    echo $pid
    [[ "$pid" =~ "$ppid" ]] && [ "$ppid" ] && return 0 || return 1
}

function hive_start()
{
    metapid=$(check_process HiveMetastore 9083)
    cmd="nohup hive --service metastore >$HIVE_LOG_DIR/metastore.log 2>&1 &"
    [ -z "$metapid" ] && eval $cmd || echo "Metastroe服务已启动"
    server2pid=$(check_process HiveServer2 10000)
    cmd="nohup hive --service hiveserver2 >$HIVE_LOG_DIR/hiveServer2.log 2>&1 &"
    [ -z "$server2pid" ] && eval $cmd || echo "HiveServer2服务已启动"
}

function hive_stop()
{
metapid=$(check_process HiveMetastore 9083)
    [ "$metapid" ] && kill $metapid || echo "Metastore服务未启动"
    server2pid=$(check_process HiveServer2 10000)
    [ "$server2pid" ] && kill $server2pid || echo "HiveServer2服务未启动"
}

case $1 in
"start")
    hive_start
    ;;
"stop")
    hive_stop
    ;;
"restart")
    hive_stop
    sleep 2
    hive_start
    ;;
"status")
    check_process HiveMetastore 9083 >/dev/null && echo "Metastore服务运行正常" || echo "Metastore服务运行异常"
    check_process HiveServer2 10000 >/dev/null && echo "HiveServer2服务运行正常" || echo "HiveServer2服务运行异常"
    ;;
*)
    echo Invalid Args!
    echo 'Usage: '$(basename $0)' start|stop|restart|status'
    ;;
esac
3）添加执行权限
[atguigu@hadoop102 hive]$ chmod +x $HIVE_HOME/bin/hiveservices.sh
4）启动Hive后台服务
[atguigu@hadoop102 hive]$ hiveservices.sh start
2.6 Hive使用技巧
2.6.1 Hive常用交互命令
[atguigu@hadoop102 hive]$ bin/hive -help
usage: hive
 -d,--define <key=value>          Variable subsitution to apply to hive
                                  commands. e.g. -d A=B or --define A=B
    --database <databasename>     Specify the database to use
 -e <quoted-query-string>         SQL from command line
 -f <filename>                      SQL from files
 -H,--help                        Print help information
    --hiveconf <property=value>   Use value for given property
    --hivevar <key=value>         Variable subsitution to apply to hive
                                  commands. e.g. --hivevar A=B
 -i <filename>                    Initialization SQL file
 -S,--silent                      Silent mode in interactive shell
 -v,--verbose                     Verbose mode (echo executed SQL to the console)
1）在Hive命令行里创建一个表student，并插入1条数据
hive (default)> create table student(id int,name string);
OK
Time taken: 1.291 seconds

hive (default)> insert into table student values(1,"zhangsan");
hive (default)> select * from student;
OK
student.id	student.name
1	zhangsan
Time taken: 0.144 seconds, Fetched: 1 row(s)
2）“-e”不进入hive的交互窗口执行hql语句
[atguigu@hadoop102 hive]$ bin/hive -e "select id from student;"
3）“-f”执行脚本中的hql语句
（1）在/opt/module/hive/下创建datas目录并在datas目录下创建hivef.sql文件
[atguigu@hadoop102 hive]$ mkdir datas
[atguigu@hadoop102 datas]$ vim hivef.sql
（2）文件中写入正确的hql语句
select * from student;
（3）执行文件中的hql语句
[atguigu@hadoop102 hive]$ bin/hive -f /opt/module/hive/datas/hivef.sql
（4）执行文件中的hql语句并将结果写入文件中
[atguigu@hadoop102 hive]$ bin/hive -f /opt/module/hive/datas/hivef.sql  > /opt/module/hive/datas/hive_result.txt
2.6.2 Hive参数配置方式
1）查看当前所有的配置信息
hive>set;
2）参数的配置三种方式
（1）配置文件方式
	默认配置文件：hive-default.xml
	用户自定义配置文件：hive-site.xml
注意：用户自定义配置会覆盖默认配置。另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效。
（2）命令行参数方式
①启动Hive时，可以在命令行添加-hiveconf param=value来设定参数。例如：
[atguigu@hadoop103 hive]$ bin/hive -hiveconf mapreduce.job.reduces=10;
注意：仅对本次Hive启动有效。
②查看参数设置
hive (default)> set mapreduce.job.reduces;
（3）参数声明方式
可以在HQL中使用SET关键字设定参数，例如：
hive(default)> set mapreduce.job.reduces=10;
注意：仅对本次Hive启动有效。
查看参数设置：
hive(default)> set mapreduce.job.reduces;
上述三种设定方式的优先级依次递增。即配置文件 < 命令行参数 < 参数声明。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。
2.6.3 Hive常见属性配置
1）Hive客户端显示当前库和表头
（1）在hive-site.xml中加入如下两个配置
[atguigu@hadoop102 conf]$ vim hive-site.xml

<property>
    <name>hive.cli.print.header</name>
    <value>true</value>
    <description>Whether to print the names of the columns in query output.</description>
</property>
<property>
    <name>hive.cli.print.current.db</name>
    <value>true</value>
    <description>Whether to include the current database in the Hive prompt.</description>
</property>
（2）hive客户端在运行时可以显示当前使用的库和表头信息
[atguigu@hadoop102 conf]$ hive

hive (default)> select * from stu;
OK
stu.id	stu.name
1	ss
Time taken: 1.874 seconds, Fetched: 1 row(s)
hive (default)>
2）Hive运行日志路径配置
（1）Hive的log默认存放在/tmp/atguigu/hive.log目录下（当前用户名下）
[atguigu@hadoop102 atguigu]$ pwd
/tmp/atguigu
[atguigu@hadoop102 atguigu]$ ls
hive.log
hive.log.2022-06-27
（2）修改Hive的log存放日志到/opt/module/hive/logs
○1修改$HIVE_HOME/conf/hive-log4j2.properties.template文件名称为
hive-log4j2.properties
[atguigu@hadoop102 conf]$ pwd
/opt/module/hive/conf

[atguigu@hadoop102 conf]$ mv hive-log4j2.properties.template hive-log4j2.properties
○2在hive-log4j2.properties文件中修改log存放位置
[atguigu@hadoop102 conf]$ vim hive-log4j2.properties
修改配置如下。
property.hive.log.dir=/opt/module/hive/logs
3）Hive的JVM堆内存设置
新版本的Hive启动的时候，默认申请的JVM堆内存大小为256M，JVM堆内存申请的太小，导致后期开启本地模式，执行复杂的SQL时经常会报错：java.lang.OutOfMemoryError: Java heap space，因此最好提前调整一下HADOOP_HEAPSIZE这个参数。
（1）修改$HIVE_HOME/conf下的hive-env.sh.template为hive-env.sh
[atguigu@hadoop102 conf]$ pwd
/opt/module/hive/conf

[atguigu@hadoop102 conf]$ mv hive-env.sh.template hive-env.sh
（2）将hive-env.sh其中的参数 export HADOOP_HEAPSIZE修改为2048，重启Hive。
修改前。
# The heap size of the jvm stared by hive shell script can be controlled via:
# export HADOOP_HEAPSIZE=1024
	修改后。
# The heap size of the jvm stared by hive shell script can be controlled via:
export HADOOP_HEAPSIZE=2048
4）关闭Hadoop虚拟内存检查
在yarn-site.xml中关闭虚拟内存检查（虚拟内存校验，如果已经关闭了，就不需要配了）。
（1）修改前记得先停Hadoop
[atguigu@hadoop102 hadoop]$ pwd
/opt/module/hadoop-3.1.3/etc/hadoop

[atguigu@hadoop102 hadoop]$ vim yarn-site.xml
（2）添加如下配置
<property>
    <name>yarn.nodemanager.vmem-check-enabled</name>
    <value>false</value>
</property>
（3）修改完后记得分发yarn-site.xml，并重启yarn。
第3章 DDL数据定义语言
3.1 数据库（database）
3.1.1 创建数据库
1）语法
CREATE DATABASE [IF NOT EXISTS] database_name
[COMMENT database_comment]
[LOCATION hdfs_path]
[WITH DBPROPERTIES (property_name=property_value, ...)];
2）案例
（1）创建一个数据库，不指定路径
hive (default)> create database db_hive1;
注：若不指定路径，其默认路径为${hive.metastore.warehouse.dir}/database_name.db
（2）创建一个数据库，指定路径
hive (default)> create database db_hive2 location '/db_hive2';
（2）创建一个数据库，带有dbproperties
hive (default)> create database db_hive3 with dbproperties('create_date'='2022-11-18');
3.1.2 查询数据库
1）展示所有数据库
（1）语法
SHOW DATABASES [LIKE 'identifier_with_wildcards'];
注：like通配表达式说明：*表示任意个任意字符，|表示或的关系。
（2）案例
hive> show databases like 'db_hive*';
OK
db_hive_1
db_hive_2
2）查看数据库信息
（1）语法
DESCRIBE DATABASE [EXTENDED] db_name;
（2）案例
	查看基本信息
hive> desc database db_hive3;
OK
db_hive		hdfs://hadoop102:8020/user/hive/warehouse/db_hive.db	atguigu	USER
	查看更多信息
hive> desc database extended db_hive3;
OK
db_name	comment	location	owner_name	owner_type	parameters
db_hive3		hdfs://hadoop102:8020/user/hive/warehouse/db_hive3.db	atguigu	USER	{create_date=2022-11-18}
3.1.3 修改数据库
用户可以使用alter database命令修改数据库某些信息，其中能够修改的信息包括dbproperties、location、owner user。需要注意的是：修改数据库location，不会改变当前已有表的路径信息，而只是改变后续创建的新表的默认的父目录。
1）语法
--修改dbproperties
ALTER DATABASE database_name SET DBPROPERTIES (property_name=property_value, ...);

--修改location
ALTER DATABASE database_name SET LOCATION hdfs_path;

--修改owner user
ALTER DATABASE database_name SET OWNER USER user_name;
2）案例
（1）修改dbproperties
hive> ALTER DATABASE db_hive3 SET DBPROPERTIES ('create_date'='2022-11-20');
3.1.4 删除数据库
1）语法
DROP DATABASE [IF EXISTS] database_name [RESTRICT|CASCADE];
注：RESTRICT：严格模式，若数据库不为空，则会删除失败，默认为该模式。
   		 CASCADE：级联模式，若数据库不为空，则会将库中的表一并删除。
2）案例
（1）删除空数据库
hive> drop database db_hive2;
（2）删除非空数据库
hive> drop database db_hive3 cascade;
3.1.5 切换当前数据库
1）语法
USE database_name;
3.2 表（table）
3.2.1 创建表
3.2.1.1 创建普通表
1）完整语法
CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name   
[(col_name data_type [COMMENT col_comment], ...)]
[COMMENT table_comment]
[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]
[CLUSTERED BY (col_name, col_name, ...) 
[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]
[ROW FORMAT row_format] 
[STORED AS file_format]
[LOCATION hdfs_path]
[TBLPROPERTIES (property_name=property_value, ...)]
2）关键字说明：
（1）TEMPORARY
临时表，该表只在当前会话可见，会话结束，表会被删除。
（2）EXTERNAL（重点）
外部表，与之相对应的是内部表（管理表）。管理表意味着Hive会完全接管该表，包括元数据和HDFS中的数据。而外部表则意味着Hive只接管元数据，而不完全接管HDFS中的数据。
（3）data_type（重点）
Hive中的字段类型可分为基本数据类型和复杂数据类型。
基本数据类型如下：
Hive	说明	定义
tinyint	1byte有符号整数	
smallint	2byte有符号整数	
int	4byte有符号整数	
bigint	8byte有符号整数	
boolean	布尔类型，true或者false	
float	单精度浮点数	
double	双精度浮点数	
decimal	十进制精准数字类型	decimal(16,2)
varchar	字符序列，需指定最大长度，最大长度的范围是[1,65535]	varchar(32)
string	字符串，无需指定最大长度	
timestamp	时间类型	
binary	二进制数据	
复杂数据类型如下；
类型	说明	定义	取值
array	数组是一组相同类型的值的集合	array<string>	arr[0]
map	map是一组相同类型的键-值对集合 	map<string, int>	map['key']
struct	结构体由多个属性组成，每个属性都有自己的属性名和数据类型	struct<id:int, name:string>	struct.id
注：类型转换
Hive的基本数据类型可以做类型转换，转换的方式包括隐式转换以及显示转换。
方式一：隐式转换
具体规则如下：
①任何整数类型都可以隐式地转换为一个范围更广的类型，如tinyint可以转换成int，int可以转换成bigint。
②所有整数类型、float和string类型都可以隐式地转换成double。
③tinyint、smallint、int都可以转换为float。
④boolean类型不可以转换为任何其它的类型。
详情可参考Hive官方说明：Allowed Implicit Conversions
方式二：显示转换
可以借助cast函数完成显示的类型转换
①语法
cast(expr as <type>) 
②案例
hive (default)> select '1' + 2, cast('1' as int) + 2;

_c0	   _c1
3.0	    3
（4）PARTITIONED BY（重点）
创建分区表。
（5）CLUSTERED BY ... SORTED BY...INTO ... BUCKETS（重点）
创建分桶表。
（6）ROW FORMAT（重点）
指定SERDE，SERDE是Serializer and Deserializer的简写。Hive使用SERDE序列化和反序列化每行数据。详情可参考 Hive-Serde。语法说明如下：
语法一：DELIMITED关键字表示对文件中的每个字段按照特定分割符进行分割，其会使用默认的SERDE对每行数据进行序列化和反序列化。
ROW FORAMT DELIMITED 
[FIELDS TERMINATED BY char] 
[COLLECTION ITEMS TERMINATED BY char] 
[MAP KEYS TERMINATED BY char] 
[LINES TERMINATED BY char] 
[NULL DEFINED AS char]
注：
	fields terminated by ：列分隔符。
	collection items terminated by ： map、struct和array中每个元素之间的分隔符。
	map keys terminated by ：map中的key与value的分隔符。
	lines terminated by ：行分隔符。
语法二：SERDE关键字可用于指定其他内置的SERDE或者用户自定义的SERDE。例如JSON SERDE，可用于处理JSON字符串。
ROW FORMAT SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value,property_name=property_value, ...)] 
（7）STORED AS（重点）
指定文件格式，常用的文件格式有，textfile（默认值），sequence file，orc file、parquet file等。
（8）LOCATION
指定表所对应的HDFS路径，若不指定路径，其默认值为
${hive.metastore.warehouse.dir}/db_name.db/table_name
（9）TBLPROPERTIES
用于配置表的一些KV键值对参数。
3.2.1.2 案例演示
1）内部表与外部表
（1）内部表
Hive中默认创建的表都是的内部表，有时也被称为管理表。对于内部表，Hive会完全管理表的元数据和数据文件。
创建内部表如下：
create table if not exists student(
    id int, 
    name string
)
row format delimited fields terminated by '\t'
location '/user/hive/warehouse/student';
	准备其需要的文件如下，注意字段之间的分隔符。
[atguigu@hadoop102 datas]$ vim /opt/module/datas/student.txt

1001	student1
1002	student2
1003	student3
1004	student4
1005	student5
1006	student6
1007	student7
1008	student8
1009	student9
1010	student10
1011	student11
1012	student12
1013	student13
1014	student14
1015	student15
1016	student16
上传文件到Hive表指定的路径。
[atguigu@hadoop102 datas]$ hadoop fs -put student.txt /user/hive/warehouse/student
删除表，观察数据HDFS中的数据文件是否还在。
hive (default)> drop table student;
（2）外部表
	外部表通常可用于处理其他工具上传的数据文件，对于外部表，Hive只负责管理元数据，不负责管理HDFS中的数据文件。
创建外部表如下：
create external table if not exists student(
    id int, 
    name string
)
row format delimited fields terminated by '\t'
location '/user/hive/warehouse/student';
上传文件到Hive表指定的路径。
[atguigu@hadoop102 datas]$ hadoop fs -put student.txt /user/hive/warehouse/student
删除表，观察数据HDFS中的数据文件是否还在。
hive (default)> drop table student;
2）SERDE和复杂数据类型
本案例重点练习SERDE和复杂数据类型的使用。
若如下格式的JSON文件需要由Hive进行分析处理，请考虑如何设计表？
注：以下内容为格式化之后的结果，文件中每行数据为一个完整的JSON字符串。
{
    "name": "dasongsong",
    "friends": [
        "bingbing",
        "lili"
    ],
    "students": {
        "xiaohaihai": 18,
        "xiaoyangyang": 16
    },
    "address": {
        "street": "hui long guan",
        "city": "beijing",
        "postal_code": 10010
    }
}
我们可以考虑使用专门负责JSON文件的JSON Serde，设计表字段时，表的字段与JSON字符串中的一级字段保持一致，对于具有嵌套结构的JSON字符串，考虑使用合适复杂数据类型保存其内容。最终设计出的表结构如下：
hive>
create table teacher
(
    name     string,
    friends  array<string>,
    students map<string,int>,
    address  struct<city:string,street:string,postal_code:int>
)
row format serde 'org.apache.hadoop.hive.serde2.JsonSerDe'
location '/user/hive/warehouse/teacher';
创建该表，并准备以下文件。注意，需要确保文件中每行数据都是一个完整的JSON字符串，JSON SERDE才能正确的处理。
[atguigu@hadoop102 datas]$ vim /opt/module/datas/teacher.txt

{"name":"dasongsong","friends":["bingbing","lili"],"students":{"xiaohaihai":18,"xiaoyangyang":16},"address":{"street":"hui long guan","city":"beijing","postal_code":10010}}
上传文件到Hive表指定的路径。
[atguigu@hadoop102 datas]$ hadoop fs -put teacher.txt /user/hive/warehouse/teacher
尝试从复杂数据类型的字段中取值。
3.2.1.3 特殊建表
1）语法：Create Table As Select（CTAS）建表
该语法允许用户利用select查询语句返回的结果，直接建表，表的结构和查询语句的结构保持一致，且保证包含select查询语句放回的内容。
CREATE [TEMPORARY] TABLE [IF NOT EXISTS] table_name 
[COMMENT table_comment] 
[ROW FORMAT row_format] 
[STORED AS file_format] 
[LOCATION hdfs_path]
[TBLPROPERTIES (property_name=property_value, ...)]
[AS select_statement]
2）语法：Create Table Like 表 
该语法允许用户复刻一张已经存在的表结构，与上述的CTAS语法不同，该语法创建出来的表中不包含数据。
CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name
[LIKE exist_table_name]
[ROW FORMAT row_format] 
[STORED AS file_format] 
[LOCATION hdfs_path]
[TBLPROPERTIES (property_name=property_value, ...)]
3.2.1.4 案例演示
1）create table as select
hive>
create table teacher1 as select * from teacher;
2）create table like
hive>
create table teacher2 like teacher;
3.2.2 查看表
1）展示所有表
（1）语法
SHOW TABLES [IN database_name] LIKE ['identifier_with_wildcards'];
注：like通配表达式说明：*表示任意个任意字符，|表示或的关系。
（2）案例
hive> show tables like 'stu*';
2）查看表信息
（1）语法
DESCRIBE [EXTENDED | FORMATTED] [db_name.]table_name
注：EXTENDED：展示详细信息。
		FORMATTED：对详细信息进行格式化的展示。
（2）案例
○1查看基本信息
hive> desc stu;
○2查看更多信息
hive> desc formatted stu;
3.2.3 修改表
1）重命名表
（1）语法
ALTER TABLE table_name RENAME TO new_table_name
（2）案例
hive (default)> alter table stu rename to stu1;
2）修改列信息
（1）语法
	增加列
该语句允许用户增加新的列，新增列的位置位于末尾。
ALTER TABLE table_name ADD COLUMNS (col_name data_type [COMMENT col_comment], ...)
	更新列
该语句允许用户修改指定列的列名、数据类型、注释信息以及在表中的位置。
ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name]
	替换列
该语句允许用户用新的列集替换表中原有的全部列。
ALTER TABLE table_name REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...)
2）案例
（1）查询表结构
hive (default)> desc stu;
（2）添加列
hive (default)> alter table stu add columns(age int);
（3）查询表结构
hive (default)> desc stu;
（4）更新列
hive (default)> alter table stu change column age ages double;
（6）替换列
hive (default)> alter table stu replace columns(id int, name string);
3.2.4 删除表
1）语法
DROP TABLE [IF EXISTS] table_name;
2）案例
hive (default)> drop table stu;
3.2.5 清空表
1）语法
TRUNCATE [TABLE] table_name
注意：truncate只能清空管理表，不能删除外部表中数据。
2）案例
hive (default)> truncate table student;
第4章 DML数据操作语言
4.1 Load
Load语句可将文件导入到Hive表中。
1）语法
hive> 
LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)];
关键字说明：
（1）local：表示从本地加载数据到Hive表；否则从HDFS加载数据到Hive表。
（2）overwrite：表示覆盖表中已有数据，否则表示追加。
（3）partition：表示上传到指定分区，若目标是分区表，需指定分区。
2）实操案例
（0）创建一张表
hive (default)> 
create table student(
    id int, 
    name string
) 
row format delimited fields terminated by '\t';
（1）加载本地文件到hive
hive (default)> load data local inpath '/opt/module/datas/student.txt' into table student;
（2）加载HDFS文件到hive中
①上传文件到HDFS
[atguigu@hadoop102 ~]$ hadoop fs -put /opt/module/datas/student.txt /user/atguigu
②加载HDFS上数据，导入完成后去HDFS上查看文件是否还存在
hive (default)> 
load data inpath '/user/atguigu/student.txt' 
into table student;
（3）加载数据覆盖表中已有的数据
①上传文件到HDFS
hive (default)> dfs -put /opt/module/datas/student.txt /user/atguigu;
②加载数据覆盖表中已有的数据
hive (default)> 
load data inpath '/user/atguigu/student.txt' 
overwrite into table student;
4.2 Insert
4.2.1 将查询结果插入表中
1）语法
INSERT (INTO | OVERWRITE) TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement;
关键字说明：
（1）INTO：将结果追加到目标表
（2）OVERWRITE：用结果覆盖原有数据
2）案例
（1）新建一张表
hive (default)> 
create table student1(
    id int, 
    name string
) 
row format delimited fields terminated by '\t';
（2）根据查询结果插入数据
hive (default)> insert overwrite table student3
select 
    id, 
    name 
from student;
4.2.2 将给定Values插入表中
1）语法
INSERT (INTO | OVERWRITE) TABLE tablename [PARTITION (partcol1[=val1], partcol2[=val2] ...)] VALUES values_row [, values_row ...]
2）案例
hive (default)> insert into table  student1 values(1,'wangwu'),(2,'zhaoliu');
4.2.3 将查询结果写入目标路径
1）语法
INSERT OVERWRITE [LOCAL] DIRECTORY directory
  [ROW FORMAT row_format] [STORED AS file_format] select_statement;
2）案例
insert overwrite local directory '/opt/module/datas/student' ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.JsonSerDe'
select id,name from student;
4.3 Export&Import
Export导出语句可将表的数据和元数据信息一并到处的HDFS路径，Import可将Export导出的内容导入Hive，表的数据和元数据信息都会恢复。Export和Import可用于两个Hive实例之间的数据迁移。
1）语法
--导出
EXPORT TABLE tablename TO 'export_target_path'

--导入
IMPORT [EXTERNAL] TABLE new_or_original_tablename FROM 'source_path' [LOCATION 'import_target_path']
2）案例
--导出
hive>
export table default.student to '/user/hive/warehouse/export/student';

--导入
hive>
import table student2 from '/user/hive/warehouse/export/student';
第5章 查询
5.1 基础语法
1）官网地址
https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select
2）查询语句语法：
SELECT [ALL | DISTINCT] select_expr, select_expr, ...
  FROM table_reference       -- 从什么表查
  [WHERE where_condition]   -- 过滤
  [GROUP BY col_list]        -- 分组查询
   [HAVING col_list]          -- 分组后过滤
  [ORDER BY col_list]        -- 排序
  [CLUSTER BY col_list
    | [DISTRIBUTE BY col_list] [SORT BY col_list]
  ]
 [LIMIT number]                -- 限制输出的行数
5.2 基本查询（Select…From）
5.2.1 数据准备
1）原始数据
（1）在/opt/module/hive/datas/路径上创建dept.txt文件，并赋值如下内容：
部门编号 部门名称 部门位置id
[atguigu@hadoop102 datas]$ vim dept.txt

10	行政部	1700
20	财务部	1800
30	教学部	1900
40	销售部	1700
（2）在/opt/module/hive/datas/路径上创建emp.txt文件，并赋值如下内容：
员工编号 姓名 岗位    薪资  部门
[atguigu@hadoop102 datas]$ vim emp.txt

7369	张三	研发	800.00	30
7499	李四	财务	1600.00	20
7521	王五	行政	1250.00	10
7566	赵六	销售	2975.00	40
7654	侯七	研发	1250.00	30
7698	马八	研发	2850.00	30
7782	金九	\N	2450.0	30
7788	银十	行政	3000.00	10
7839	小芳	销售	5000.00	40
7844	小明	销售	1500.00	40
7876	小李	行政	1100.00	10
7900	小元	讲师	950.00	30
7902	小海	行政	3000.00	10
7934	小红明	讲师	1300.00	30
1）创建部门表
hive (default)>
create table if not exists dept(
    deptno int,    -- 部门编号
    dname string,  -- 部门名称
    loc int        -- 部门位置
)
row format delimited fields terminated by '\t';
2）创建员工表
hive (default)>
create table if not exists emp(
    empno int,      -- 员工编号
    ename string,   -- 员工姓名
    job string,     -- 员工岗位（大数据工程师、前端工程师、java工程师）
    sal double,     -- 员工薪资
    deptno int      -- 部门编号
)
row format delimited fields terminated by '\t';
3）导入数据
hive (default)>
load data local inpath '/opt/module/hive/datas/dept.txt' into table dept;
load data local inpath '/opt/module/hive/datas/emp.txt' into table emp;
5.2.2 全表和特定列查询
在Hive SQL中，使用频率最高的查询语句就是全表查询和特定列查询，通俗来讲，就是select语句。Hive的查询语句都是以select为开端的。通过select语句可以从表中选取出特定列或者全部列。
案例一：选择特定列查询
以emp表为例，从emp中选取出empno（员工编号）列、ename（员工姓名）列和deptno（部门编号）列。
hive (default)> select empno, ename, deptno from emp;
查询过程示意图。
 
查询结果如下。
empno	ename	deptno
7789	丁一		70
7989	艾斯		70
7369	张三		30
7499	李四		20
7521	王五		10
7566	赵六		40
7654	侯七		30
7698	马八		30
7782	金九		30
7788	银十		10
7839	小芳		40
7844	小明		40
7876	小李		10
7900	小元		30
7902	小海		10
7934	小红明	30
Time taken: 0.177 seconds, Fetched: 14 row(s)
案例二：全表查询
如果想从表中直接查询所有列，就可以使用星号（*）代替列名。使用select *语句查询出的所有列，列的顺序是固定的，如果用户想按照自己的意愿显示列，则建议将列名按顺序列举出来。
使用select *查询emp表的所有字段数据。
hive (default)> select * from emp;
查询结果如下。
emp.empno	emp.ename	emp.job	emp.sal	emp.deptno
7369		张三			研发	800.0		30
7499		李四			财务	1600.0		20
7521		王五			行政	1250.0		10
7566		赵六			销售	2975.0		40
7654		侯七			研发	1250.0		30
7698		马八			研发	2850.0		30
7782		金九			NULL	2450.0		30
7788		银十			行政	3000.0		10
7839		小芳			销售	5000.0		40
7844		小明			销售	1500.0		40
7876		小李			行政	1100.0		10
7900		小元			讲师	950.0		30
7902		小海			行政	3000.0		10
7934		小红明			讲师	1300.0		30
Time taken: 2.242 seconds, Fetched: 14 row(s)
以上两个示例查询语句都是由select子句和from子句构成的。子句是Hive SQL的关键组成部分，一个完整的查询语句由以下子句构成，子句关键词用加粗进行了展示。需要注意的是，子句的前后顺序是需要严格遵守的，例如本节出现的from子句必须位于select子句后。
select [all | distinct] select_expr, select_expr, ... 
from table_reference 
[where where_condition] 
[group by col_list] 
[having having_condition] 
[order by col_list]] 
[limit number];
select子句示意了要选取出的数据或者列名称，而from子句则指出了查询数据的来源表。select子句并不一定需要与from子句同时出现，在select后可以直接跟常量值或者表达式，通过这种方式可以进行函数的测试等工作。

注意：
（1）SQL 语言大小写不敏感。 
（2）SQL 可以写在一行或者多行。
（3）关键字不能被缩写也不能分行。
（4）各子句一般要分行写。
（5）使用缩进提高语句的可读性。
5.2.3 列别名
1）重命名一个列
2）便于计算
3）紧跟列名，也可以在列名和别名之间加入关键字‘AS’
4）案例实操
查询emp表的ename和deptno列，并为ename列取列别名name，为deptno列取列别名dn。
hive (default)> 
select 
    ename AS name, 
    deptno dn 
from emp;
查询结果如下所示。
name	dn
张三	30
李四	20
王五	10
赵六	40
侯七	30
马八	30
金九	30
银十	10
小芳	40
小明	40
小李	10
小元	30
小海	10
小红明	30
Time taken: 0.16 seconds, Fetched: 14 row(s)
5.2.4 Limit语句
limit子句用于限制返回的行数，一般位于整个查询语句的最后。
案例一：查询emp表，并限制只显示前五行。
hive (default)> select * from emp limit 5; 
查询结果如下所示。
emp.empno	emp.ename	emp.job	emp.sal	emp.deptno
7369		张三			研发		800.0	30
7499		李四			财务		1600.0	20
7521		王五			行政		1250.0	10
7566		赵六			销售		2975.0	40
7654		侯七			研发		1250.0	30
Time taken: 2.886 seconds, Fetched: 5 row(s)
案例二：限制显示指定范围的行
limit子句还可以限制显示指定范围的行，如下图所示，限制显示从索引为2的行开始，向下抓取3行。需要注意的是，行的索引是从0开始的。
 
查询语句如下所示。
hive (default)> select * from emp limit 2,3; 
查询结果如下。
emp.empno	emp.ename	emp.job	emp.sal	emp.deptno
7521		王五			行政		1250.0	10
7566		赵六			销售		2975.0	40
7654		侯七			研发		1250.0	30
Time taken: 2.434 seconds, Fetched: 3 row(s)
5.2.5 Order By全局排序
order by子句用于表数据的全局排序，order by后直接跟列名或列别名。使用order by对结果集进行全局排序，只会生成一个ReduceTask，若数据量较大，势必会造成较大的内存负担，所以需要谨慎使用，或者使用limit子句限制排序结果的数量。在列名后可以加关键字asc，表示升序排序，也可以加关键字desc，表示降序排序，若没有asc或desc，则默认为升序排序。
目前我们已经接触过的子句有select子句、from子句、limit子句和order by子句，一般的书写顺序是：
select子句 → from子句 → order by子句 → limit子句。
子句的书写顺序是必须遵守的，否则查询语句的运行会报错。
	asc（ascend）：升序（默认）。
	desc（descend）：降序。
案例一：查询emp表，按照薪资sal列升序排序
hive (default)> 
select 
    * 
from emp 
order by sal; 
执行过程如下。
 
以上Hive SQL的实际执行过程如下图所示。在各MapTask中分别执行select操作和排序操作，各MapTask生成的结果文件会汇总至一个ReduceTask中执行全局排序。执行过程是根据Hive SQL的执行计划绘制的，关于执行计划的生成和解读，后续详细讲解。
 
案例二：为员工薪资sal列的2倍起别名为doublesal，并按照doublesal降序排序
在列名后使用desc关键字可以实现降序排序。
hive (default)> 
select 
    ename, 
    sal * 2 doublesal 
from emp 
order by doublesal desc; 
执行过程如下。
 
以上Hive SQL的实际执行过程如下图所示。在各MapTask中执行select操作、sal字段乘2的操作和排序操作，各MapTask生成的结果文件汇总至一个ReduceTask中执行全局排序。
 
案例三：使用order by子句指定多个排序键
查询emp表，按照薪资sal列升序，薪资相同时，按照部门编号deptno降序。查询语句如下所示，多个排序键之间使用逗号分隔。
hive (default)> 
select 
    *
from emp 
order by sal asc, deptno desc; 
执行过程如下。
 
以上Hive SQL的实际执行过程如下图所示。在各MapTask中执行select操作、sal字段乘2的操作和排序操作，各MapTask生成的结果文件汇总至一个ReduceTask中执行全局排序。
 
案例四：使用limit关键字，查询所有员工中薪资排前三的员工
hive (default)> 
select 
    ename, 
    sal 
from emp 
order by sal desc
limit 3; 
执行过程如下。
 
以上Hive SQL的实际执行过程如下图所示。在各MapTask中执行select操作，并排序保留各MapTask中sal字段前三的数据，各MapTask生成的结果文件汇总至一个ReduceTask中执行全局排序。可以看到，通过limit关键字，大大减小了各MapTask生成的结果文件的大小，也降低了ReduceTask的计算压力。
 
5.2.6 Where语句
使用where子句可以将不满足条件的行过滤掉。where子句必须紧跟在from子句后。
where子句后面跟的内容被称为“条件表达式”。例如如下查询语句，查询emp表，获得薪资sal大于2000的所有员工，“sal > 2000”就是条件表达式，查询语句会将条件表达式判断为true的行选取出来。
查询出薪水大于2000的所有员工。
hive (default)> select * from emp where sal > 2000;
执行过程如下。
 
查询结果如下所示。
ename	sal
赵六		2975.0
马八		2850.0
金九		2450.0
银十		3000.0
小芳		5000.0
小海		3000.0
Time taken: 0.2 seconds, Fetched: 6 row(s)
注意：where子句中不能使用字段别名。因为在执行到where查询子句的时候，尚未对列进行赋别名的操作。
5.2.7 关系运算函数
1）基本语法
Hive内置关系运算符，关系运算符用于两个操作数之间，返回值为true或false。关系运算符通常用于where和having子句中，或者if函数中。Hive内置的关系运算符如表所示。
操作符	支持的数据类型	描述
A=B	基本数据类型	如果A等于B则返回true，反之返回false
A<=>B	基本数据类型	如果A和B都为null或者都不为null，则返回true，如果只有一边为null，返回false
A<>B, A!=B	基本数据类型	A或者B为null则返回null；如果A不等于B，则返回true，反之返回false
A<B	基本数据类型	A或者B为null，则返回null；如果A小于B，则返回true，反之返回false
A<=B	基本数据类型	A或者B为null，则返回null；如果A小于等于B，则返回true，反之返回false
A>B	基本数据类型	A或者B为null，则返回null；如果A大于B，则返回true，反之返回false
A>=B	基本数据类型	A或者B为null，则返回null；如果A大于等于B，则返回true，反之返回false
A [not] between B and C	基本数据类型	如果A，B或者C任一为null，则结果为null。如果A的值大于等于B而且小于或等于C，则结果为true，反之为false。如果使用not关键字则可达到相反的效果。
A is null	所有数据类型	如果A等于null，则返回true，反之返回false
A is not null	所有数据类型	如果A不等于null，则返回true，反之返回false
in（数值1，数值2）	所有数据类型	使用 in运算显示列表中的值
A [not] like B	string 类型	B是一个SQL下的简单正则表达式，也叫通配符模式，如果A与其匹配的话，则返回true；反之返回false。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母‘x’结尾，而‘%x%’表示A包含有字母‘x’,可以位于开头，结尾或者字符串中间。如果使用not关键字则可达到相反的效果。
A rlike B, A regexp B	string 类型	B是基于java的正则表达式，如果A与其匹配，则返回true；反之返回false。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配。
1. 比较运算符
在where子句的学习中，我们已经通过关系运算符构建了where子句的条件表达式。关系运算符中包括一些比较运算符，例如=、＞、＜、＜＞等，可以用于比较运算符两边的数据，使用如下。
hive (default)> select 100 > 200,
              >        100 = 200,
              >        100 < 200;
查询结果。
_c0	_c1	_c2
false	false	true
Time taken: 0.793 seconds, Fetched: 1 row(s)
接下来使用比较运算符构成条件表达式来从emp表中分别查询出不同的数据。
案例一：选取出emp表中薪资sal列不为3000的员工信息。
hive (default)> select * from emp where sal <> 3000;
查询结果。
emp.empno	emp.ename	emp.job	emp.sal	emp.deptno
7369		张三			研发		800.0	30
7499		李四			财务		1600.0	20
7521		王五			行政		1250.0	10
7566		赵六			销售		2975.0	40
7654		侯七			研发		1250.0	30
7698		马八			研发		2850.0	30
7782		金九			NULL	2450.0	30
7839		小芳			销售		5000.0	40
7844		小明			销售		1500.0	40
7876		小李			行政		1100.0	10
7900		小元			讲师		950.0	30
7934		小红明		讲师		1300.0	30
Time taken: 0.322 seconds, Fetched: 12 row(s)
案例二：选取出emp表中deptno为10的所有员工信息。
hive (default)> select * from emp where deptno = 10;
查询结果。
emp.empno	emp.ename	emp.job	emp.sal	emp.deptno
7521		王五			行政		1250.0	10
7788		银十			行政		3000.0	10
7876		小李			行政		1100.0	10
7902		小海			行政		3000.0	10
Time taken: 0.183 seconds, Fetched: 4 row(s)
比较运算符可以用于比较几乎所有数据类型的数据和列，还可以用于比较表达式。例如，我们如果想查询emp表中薪资sal与部分编号deptno相加大于2000的所有员工（当然这种查询在现实中是没有具体意义的），那么查询语句如下所示。
hive (default)> select * from emp where sal + deptno > 2000;
查询结果如下所示。
emp.empno	emp.ename	emp.job	emp.sal	emp.deptno
7566		赵六			销售		2975.0	40
7698		马八			研发		2850.0	30
7782		金九			NULL	2450.0	30
7788		银十			行政		3000.0	10
7839		小芳			销售		5000.0	40
7902		小海			行政		3000.0	10
Time taken: 0.177 seconds, Fetched: 6 row(s)
2. 对null值的判断
通过表5-1，我们可以得知，当我们对null使用比较运算符时，不能得到true或false，而是null，判断就失效了。若想对null进行判断，需要使用的是is null或is not null，使用方式如下。
hive (default)> select * from emp where job is null;
查询结果。
emp.empno	emp.ename	emp.job	emp.sal	emp.deptno
7782		金九			NULL	2450.0	30
Time taken: 0.159 seconds, Fetched: 1 row(s)
5.2.8 逻辑运算函数
基本语法（and/or/not）
操作符	含义
and	逻辑并
or	逻辑或
not	逻辑否
通过and和or可以将多个条件表达式组合起来。当and运算符两侧的条件表达式均为true时，整个表达式才为true。当or运算符两侧的条件表达式至少有一个为true时，整个表达式才为true。
案例一：查询薪资大于1000且部门编号是30的所有员工。
hive (default)> 
select 
    * 
from emp 
where sal > 1000 and deptno = 30;
查询结果。
emp.empno	emp.ename	emp.job	emp.sal	emp.deptno
7654		侯七			研发		1250.0	30
7698		马八			研发		2850.0	30
7782		金九			NULL	2450.0	30
7934		小红明		讲师		1300.0	30
Time taken: 0.226 seconds, Fetched: 4 row(s)
执行过程如下。
 
案例二：查询薪资大于2000或者部门编号是30的所有员工
hive (default)> 
select 
    * 
from emp 
where sal > 2000 or deptno=30;
查询结果如下所示。
emp.empno	emp.ename	emp.job	emp.sal	emp.deptno
7369	张三	研发	800.0	30
7566	赵六	销售	2975.0	40
7654	侯七	研发	1250.0	30
7698	马八	研发	2850.0	30
7782	金九	NULL	2450.0	30
7788	银十	行政	3000.0	10
7839	小芳	销售	5000.0	40
7900	小元	讲师	950.0	30
7902	小海	行政	3000.0	10
7934	小红明	讲师	1300.0	30
Time taken: 0.173 seconds, Fetched: 10 row(s)
执行过程如下。
 
案例三：查询部门编号不是20和30的所有员工。
hive (default)> 
select 
    * 
from emp 
where deptno not in(30, 20);
查询结果如下所示。
emp.empno	emp.ename	emp.job	emp.sal	emp.deptno
7521		王五			行政		1250.0	10
7566		赵六			销售		2975.0	40
7788		银十			行政		3000.0	10
7839		小芳			销售		5000.0	40
7844		小明			销售		1500.0	40
7876		小李			行政		1100.0	10
7902		小海			行政		3000.0	10
Time taken: 0.284 seconds, Fetched: 7 row(s)
上述查询语句使用了not逻辑运算符，查询了“deptno not in(30,20)”的所有员工，接下来，我们去掉not，看一下查询结果。
hive (default)> select * from emp where deptno in(30,20);
OK
emp.empno	emp.ename	emp.job	emp.sal	emp.deptno
7369		张三			研发		800.0	30
7499		李四			财务		1600.0	20
7654		侯七			研发		1250.0	30
7698		马八			研发		2850.0	30
7782		金九			NULL		2450.0	30
7900		小元			讲师		950.0	30
7934		小红明			讲师		1300.0	30
Time taken: 2.463 seconds, Fetched: 7 row(s)
可以看到两个查询语句的查询结果是完全不同的，并且，两个查询结果组合起来就是完整的emp表，关系如下图所示。
 
查询条件“deptno not in(30,20)”和查询条件“deptno in(40,10)”是完全等价的，为什么还费力用not运算符呢？这个考虑实际上没有问题，两个查询条件完全等价，都可以得到我们想要的结果。但是我们可以继续思考一下，如果deptno字段有10个不同的值呢？显然列举出不是30和20的部门编号就是一件比较麻烦的事了。在符合要求的查询条件比较难以描述，而不符合要求的查询条件相对容易表示时，通过使用not运算符可使查询语句更加简洁且可读性更好。
5.2.9 算术运算符
Hive内置的算术运算符如表所示。算数运算符的操作数必须是数值类型。
运算符	描述
A+B	A和B 相加
A-B	A减去B
A*B	A和B 相乘
A/B	A除以B
A%B	A对B取余
A&B	A和B按位取与
A|B	A和B按位取或
A^B	A和B按位取异或
~A	A按位取反
查询emp表，将所有员工的薪资sal加1。
hive (default)> select ename,sal + 1 as sal_plus_1 from emp;
查询结果。
ename	sal_plus_1
张三		801.0
李四		1601.0
王五		1251.0
赵六		2976.0
侯七		1251.0
马八		2851.0
金九		2451.0
银十		3001.0
小芳		5001.0
小明		1501.0
小李		1101.0
小元		951.0
小海		3001.0
小红明	1301.0
Time taken: 0.169 seconds, Fetched: 14 row(s)
需要注意的是，算数运算符与null值的计算结果都是null，如下。
hive (default)> select 1 + null, 2 * null,null / 0;
查询结果。
_c0		_c1		_c2
NULL	NULL	NULL
Time taken: 0.183 seconds, Fetched: 1 row(s)
5.3 分组聚合
5.3.1 聚合函数
常用的聚合函数
	count(*)，表示统计所有行数，包含null值。
	count(某列)，表示该列一共有多少行，不包含null值。
	max()，求最大值，不包含null，除非所有值都是null。
	min()，求最小值，不包含null，除非所有值都是null。
	sum()，求和，不包含null。
	avg()，求平均值，不包含null。
查询emp表，统计员工人数、最高薪资、最低薪资、薪资总和和平均薪资，查询语句如下所示。
hive (default)> select count(*) cnt,
       max(sal) max_sal,
       min(sal) min_sal,
       sum(sal) sum_sal,
       avg(sal) avg_sal 
from emp;
查询结果如下所示。
cnt	max_sal	min_sal	sum_sal	avg_sal
14	5000.0	800.0	29025.0	2073.214285714286
Time taken: 29.875 seconds, Fetched: 1 row(s)
案例演示一：count函数
count函数用来统计行数，count函数括号中的内容是函数的参数，得到的值是函数的返回值。count函数的参数可以是列名，也可以是*。如下查询语句所示，分别展示了count(*)和count(列名)的使用。
hive (default)> select count(*) total_count, 
	count(job) job_count 
from emp;
得到的查询结果如下所示。
total_count	job_count
14			13
Time taken: 25.46 seconds, Fetched: 1 row(s)
我们可以看到count(*)得到的返回值是14，count(job)得到的返回值是13。在创建emp表时，job字段中存在一个null值。所以可以得到结论，count(*)统计得到的是包含null值的所有数据的行数，count(列名)统计得到的是除null值外的数据行数，如图5-13所示。
 
查询语句select count(*) from emp的MapReduce的实际执行过程如图5-14所示。
 
在上面的例子中，通过count(job)我们得到了job列除null值外所有的岗位个数。在这些岗位中，有很多重复的内容。实际使用中，更有意义的需求是统计不同岗位的个数，这就意味着要对job列去重。此时，我们可以使用distinct关键字，使用如下所示。
hive (default)> select count(distinct job) distinct_job_count from emp;
得到的结果如下所示。
distinct_job_count
5
Time taken: 21.316 seconds, Fetched: 1 row(s)
统计去重后的行数如图5-15所示。
 
distinct关键字除了和count函数搭配使用外，也可以单独使用，或者与其他聚合函数搭配使用。例如单独使用distinct关键字与job列配合，如下所示。
hive (default)> select distinct job distinct_job from emp;
查询结果如下所示。
distinct_job
NULL
研发
行政
讲师
财务
销售
Time taken: 28.725 seconds, Fetched: 6 row(s)
案例演示二：sum和avg函数
sum函数可以用来计算总和，传入的参数是列名，返回值是该列的数值除null之外的总和。
avg函数用来计算平均值，传入的参数是列名，返回值是该列的数值除null之外的平均数。
创建一个表number_test，并插入5个数值，方便我们观察和计算数值。
hive (default)> create table number_test(number int) row format delimited fields terminated by '\t';
hive (default)> insert into table number_test values (100),(200),(300),(null),(null);
分别统计number_test表number列的总和和平均值。
hive (default)> select sum(number) sum,
	avg(number) avg
from number_test;
得到结果如下所示。
sum		avg
600		200.0
Time taken: 32.966 seconds, Fetched: 1 row(s)
通过结果我们可以知道，统计number列总和与平均值均未将null值计算在内，且计算平均值时的分母是3而不是5，如下所示。
 
前文我们曾经提过，distinct关键字可以与聚合函数搭配使用。我们查询emp表，将distinct与sum函数搭配使用，查询语句如下所示。
hive (default)> select sum(sal) total_sal, sum(distinct sal) total_distinct_sal from emp;
查询结果如下所示。
total_sal	total_distinct_sal
29025.0		24775.0
Time taken: 25.216 seconds, Fetched: 1 row(s)
可以明显看到，去重后的sal列总和是小于未去重的sal列总和的，这是去除掉重复数据的结果。
接下来我们来了解一下sum函数和avg函数在MapReduce底层的实际执行过程。
查询语句“select sum(sal) sum_sal from emp;”的执行过程如下图所示。
 
查询语句“select avg(sal) avg_sal from emp;”的执行过程如下图所示。
 
案例演示三：max和min函数
max函数和min函数可以用来计算最大值和最小值，与sum和avg函数只适用于数值类型的列不同的是，max和min函数适用于几乎所有数据类型的列。例如job列是字符串类型的，统计job列的最大最小值如下所示。
hive (default)> select max(job), min(job) from emp;
查询结果如下所示。
_c0		_c1
销售		研发
Time taken: 27.413 seconds, Fetched: 1 row(s)
显然，上述的统计是没有太大实际意义的，对于数值类型的最大最小值统计更常用，使用如下所示。
hive (default)> select max(sal), min(sal) from emp;
查询结果如下所示。
_c0		_c1
5000.0	800.0
Time taken: 21.576 seconds, Fetched: 1 row(s)
以上查询语句中max函数的实际执行过程如下所示。
 
min函数的实际执行过程如下所示。
 
5.3.2 Group By语句
group by子句通常会和聚合函数一起使用，称为分组聚合，将数据表按照一个或者多个字段进行分组，然后对每个组执行聚合操作。分组聚合的结果集的行数取决于分组字段，分组字段可以将数据分为几组，结果集就会有几行。
group by子句是我们接触的第6个子句，一般位于where子句（如果存在where子句的话）之后，order by子句之前。已经接触过的子句的书写顺序如下所示。
select子句 → from子句 → where子句 → group by子句 → order by子句 → limit子句。
我们通过三个案例来理解group by子句与聚合函数的配合应用。
案例一：查询emp表，统计每个部门的平均工资
hive (default)> 
select 
    t.deptno, 
    avg(t.sal) avg_sal 
from emp t 
group by t.deptno;
以上查询语句将emp表根据deptno列进行分组划分，并统计每个组内的平均薪资，如下。
 
查询结果如下所示。
deptno	avg_sal
10		2087.5
20		1600.0
30		1600.0
40		3158.3333333333335
Time taken: 23.709 seconds, Fetched: 4 row(s)
hive sql执行过程：
 
案例二：查询emp表，统计每个部门每个岗位的最高薪资
hive (default)>
select 
    t.deptno, 
    t.job, 
    max(t.sal) max_sal 
from emp t 
group by t.deptno, t.job;
在本案例中，我们使用deptno和job列作为组合键对emp表进行聚合分组，并统计每组中的最高薪资，分组结果如下。
 
hive sql执行过程：
 
案例三：查询emp表，统计部门编号不是10和20的每个部门每个岗位的最高薪资
根据案例需求，要想统计部门编号不是10和20的相关数据，首先需要对emp表进行筛选过滤，这就需要用到where子句。where子句应该位于group by子句之前，具体查询语句如下所示。
hive (default)>
select 
    deptno, 
    job, 
    max(sal) max_sal 
from emp
where deptno not in(10,20)
group by deptno, job;
使用where子句后，会先对emp表进行筛选过滤，然后对数据进行分组聚合，如下。
 
查询结果如下所示。
deptno	job		max_sal
30		NULL	2450.0
30		研发		2850.0
30		讲师		1300.0
40		销售		5000.0
Time taken: 51.517 seconds, Fetched: 4 row(s)
可以看到，同案例二相比，聚合结果只剩下了4行，这是使用where子句过滤过emp表的结果。
注意：
使用group by子句时，最需要记住的一点是，select后的字段只能有2种——分组字段和聚合函数，不能出现其他字段，否则会报错。使用group by子句对数据表分组之后，分组字段与非分组字段形成了一对多的关系，若想形成数据表的一对一的关系，就需要对非分组字段使用聚合函数。
使用group by子句还应注意，分组字段中不能出现列别名，否则会报错。如以下查询语句，在select子句中，为deptno字段起别名为dn，在group by子句中使用了dn，结果就是错误的。
hive (default)>
select 
    deptno dn, 
    job, 
    max(sal) max_sal 
from emp
where deptno not in(10,20)
group by dn, job;
5.3.3 Having语句
having与where不同点
（1）where后面不能写分组聚合函数，而having后面可以使用分组聚合函数。
（2）having只用于group by分组统计语句。
使用having子句可以对不满足条件的结果进行过滤。需要注意的是，having子句只能对group by分组统计之后的结果集进行过滤，不能单独使用。
接下来我们通过两个案例来学习having子句的使用。
案例一：查询emp表，统计平均薪资大于2000的部门
对上述需求进行拆分，首先统计各部门的平均薪资，再筛选平均薪资大于2000的结果。统计各部门的平均薪资很简单，使用分组聚合即可，得到的结果如下所示，我们只想得到平均薪资大于2000的结果。
 
对分组聚合后的结果过滤需要使用having子句，查询语句如下所示。
hive (default)>
select 
    deptno, 
    avg(sal) avg_sal 
from emp
group by deptno
having avg_sal > 2000;
具体的执行过程如下所示。
 
查询结果如下所示。
deptno	avg_sal
10	2087.5
40	3158.3333333333335
Time taken: 29.74 seconds, Fetched: 2 row(s)
本案例在MapReduce中的实际执行过程如下所示。
 
需要注意，having子句位于group by子句的后面。在本案例中，我们使用having子句对聚合结果集进行筛选过滤，得到我们需要的结果。
问题：我们这里能否使用where子句对结果进行筛选？
hive (default)>
select 
    deptno, 
    avg(sal) avg_sal 
from emp
where avg(sal) > 2000
group by deptno;
执行以上查询语句后，Hive立即报错了，主要报错信息如下所示。
FAILED: SemanticException [Error 10128]: Line 5:6 Not yet supported place for UDAF 'avg'
出现以上报错信息的原因是where子句中不能出现聚合函数，不能用来过滤分组聚合后的结果。如果想要使用where子句，需要再嵌套一层子查询，这和having的效果等价，但这种写法增加了我们SQL的复杂程度。
案例二：查询emp表，统计部门编号大于20且平均薪资大于2000的部门
通过案例一，我们已经知道where子句中不能使用聚合函数，要想对分组聚合的结果进行筛选过滤只能使用having子句。所以可以得到本案例的查询语句。
hive (default)>
select 
    deptno, 
    avg(sal) avg_sal 
from emp
where deptno > 20
group by deptno
having avg_sal > 2000;
查询结果如下所示。
deptno	avg_sal
40		3158.3333333333335
Time taken: 23.614 seconds, Fetched: 1 row(s)
问题：可以利用having子句中执行where子句的简单筛选工作吗？答案是可以的。查询语句如下所示。
hive (default)>
select 
    deptno, 
    avg(sal) avg_sal 
from emp
group by deptno
having deptno > 20 and avg_sal > 2000;
同样可以得到查询结果。
但是需要注意，此时写在having子句中的，必须是聚合函数或者group by子句中出现的分组字段，例如上述查询语句中的avg_sal和deptno。
虽然这两种写法都能得到最终结果，但是我们并不建议这样做。group by子句中的分组字段构成的条件表达式写在where子句中是更合理的，我们更推荐第一种写法。对此，我们可以简单总结出一个原则，那就是“where子句用于筛选行，having子句用于筛选组”。
了解了这一原则，那我们就应该明白了having子句与group by子句不可分割的关系，只有使用group by子句，having子句对分组结果的筛选过滤才有意义。

到此为止，我们已经学习完了Hive查询语句中的所有关键子句，包括select子句、from子句、where子句、group by子句、having子句、order by子句和limit子句，这些子句是最常用的，读者应该多加练习，并熟练掌握子句的书写顺序，顺序如下所示。
select [all | distinct] select_expr, select_expr, ... 
from table_reference 
[where where_condition] 
[group by col_list] 
[having having_condition] 
[order by col_list]] 
[limit number];
5.4 Join语句
join连接是标准SQL中的一个重要部分，在Hive中也支持join连接的操作。join连接的作用，是通过连接键将两个表的列组合起来，用于将数据库中的两个或多个表的记录合并起来。join连接可以将其他表的列添加至连接主表，如下所示，表A通过与表B连接，获取到了表B的列。
 
在数据库或数据仓库的日常工作中，通常无法从一张表中获取到全部期望数据。例如在电商系统中，订单表的一行数据包含订单id、用户id、商品id、订单金额等信息，若想获取到用户的详细信息，则需要通过用户id与用户表连接，若想获取到商品的详细信息，需要通过商品id与商品表连接。
5.4.1 join语法简介与表别名
一、join语法讲解
Hive官网提供的join连接的完整语法如下所示，为了与其他关键字区别显示，将join相关的关键字均加粗显示。
join_table:
    table_reference [inner] join table_factor [join_condition]
 	| table_reference {left|right|full} [outer] join table_reference join_condition
	| table_reference left semi join table_reference join_condition
	| table_reference cross join table_reference [join_condition] (as of hive 0.10)
 
table_reference:
    table_factor
	| join_table
 
table_factor:
    tbl_name [alias]
	| table_subquery alias
	| ( table_references )
 
join_condition:
    on expression
上面的join连接完整语法，初次接触的时候可能较难接受理解，在后面的内容中，将会更详细展示和练习。
以上的join连接语法中，有三个重要部分，分别是table_reference、table_factor和join_condition。
	join_condition的概念比较容易理解，是两个表连接的连接条件、由on关键字开头的连接条件表达式，例如on tableA.col_a = tableB.col_b。那么table_reference和table_factor分别代表什么呢？
	table_reference的含义是表引用，在这里指连接的主表，可以是表名、表别名或子查询别名。
	table_factor是对表引用内容上的扩充和功能上的增强，在这里指被连接的从表，可以是表名、表别名或子查询别名。
table_reference、table_factor和join_condition的简单示意如下所示。
 

二、join基础案例
接下来我们展示一个典型的join连接案例。
通过join连接查询所有员工的员工编号、员工姓名和部门名称。员工编号和员工姓名列在emp表中，部门名称字段在dept表中，所以需要使用join连接将emp表和dept表组合起来，连接条件就是emp表的deptno列和dept表的deptno列相等。如此就能查询到emp表中不存在的部门名称dname列了。查询语句如下所示。
hive (default)> 
select 
    emp.empno, 
    emp.ename, 
    dept.dname 
from emp
join dept
on emp.deptno = dept.deptno;
查询结果如下所示。
emp.empno	emp.ename	dept.dname
7369		张三			教学部
7499		李四			财务部
7521		王五			行政部
7566		赵六			销售部
7654		侯七			教学部
7698		马八			教学部
7782		金九			教学部
7788		银十			行政部
7839		小芳			销售部
7844		小明			销售部
7876		小李			行政部
7900		小元			教学部
7902		小海			行政部
7934		小红明		教学部
Time taken: 52.962 seconds, Fetched: 14 row(s)
通过join连接，我们获取到了emp表（连接主表）中不存在的dname列，连接过程如下所示。
 
以上的典型join连接查询语句的实际执行过程如下所示。
 
三、join语法发展
从Hive 0.13.0版本开始，Hive开始支持隐式连接符号。隐式连接符号的含义是，允许使用from子句连接以逗号分隔的表列表，省略join关键字。如下所示，即为省略join关键字的连接语句。
select * 
from tableA, tableB, tableC
where tableA.col_a = tableB.col_b and tableB.col_b = tableC.col_c;
从Hive 0.13.0开始，Hive开始支持不合格的列引用。合格的列引用指的是table_name.col_name类型的引用，合格的列引用可以精确指向对应列。自Hive 0.13.0开始，Hive尝试根据join的输入来解决这些问题，将不合格的列引用对应到正确的表上。当不合格的列引用被解析为多个表时，Hive才会将其标记为不明确的引用。例如，有以下两个表，每个表有两个列。
create table a (k1 string, v1 string);
create table b (k2 string, v2 string);
以下的查询语句即不合格的列引用，k1和k2列分属不同表，即使未标明表名，Hive也可以正确解析。
select k1, v1, k2, v2
from a join b on k1 = k2; 
从Hive 2.2.0版本开始，Hive开始支持on子句中使用复杂表达式，并支持不等值连接。复杂条件表达式和不等值连接如下所示。
select * from a join b on a.id = b.id and a.no = b.no;
select * from a left outer join b on a.id <> b.id；
四、表别名
在前面的讲解中，我们已经提过一个概念——表别名。除了给列起别名，我们还可以给表起别名。使用表别名可以简化查询语句，区分字段的来源。
以本节的案例为例，为emp表起别名为e，为dept表起别名为d，简化后如下所示。
hive (default)> 
select 
    e.empno, 
    e.ename, 
    d.dname 
from emp e
join dept d
on e.deptno = d.deptno;
为表起别名并不是必须的，直接使用表原名也可以完成查询。但是当表名太长时，会影响语句的可读性。从本节开始，后文中的查询语句将开始使用表别名简化查询语句。
5.4.2 数据准备
为了使表连接结果更丰富，我们需要对5.2.1节准备的数据进行一定的扩充，在dept表和emp表中分别插入两条数据，如下所示。
hive (default)> insert into table dept values (50, '运营部', 1800), (60, '人事部', 1600);
hive (default)> insert into table emp values (7789, '丁一', '助教', 1200.00, 70), (7989, '艾斯', '助教', 1300.00, 70);
为了更好地演示多表连接，在5.2.1节准备的数据的基础上，需要再创建一个位置表location。
首先在/opt/module/hive/datas目录下创建文件location.txt，输入以下内容。文件的两列数据分别是部门位置编号和部门位置。
[atguigu@hadoop102 datas]$ vim location.txt

1700	北京
1800	上海
1900	深圳
创建位置表location。
hive (default)>
create table if not exists location(
    loc int,           -- 部门位置编号
    loc_name string   -- 部门位置
)
row format delimited fields terminated by '\t';
加载数据。
hive (default)> load data local inpath '/opt/module/hive/datas/location.txt' into table location;

5.4.3 内连接
内连接中，只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。内连接的示意图如下所示。使用内连接时，inner join中的inner关键字可以省略。
 
例如，使用如下查询语句，统计员工的基本信息和部门信息，则只会返回emp表和dept表的deptno字段成功连接的数据。
hive (default)> 
select 
    e.empno, 
    e.ename, 
    d.deptno 
from emp e 
join dept d 
on e.deptno = d.deptno;
如下所示，是将emp表与dept表执行内连接的示意图。图中画×的数据都未在join结果中保留，而是只保留了连接成功的数据。
 
查询结果如下所示。
e.empno	e.ename	d.deptno
7369	张三		30
7499	李四		20
7521	王五		10
7566	赵六		40
7654	侯七		30
7698	马八		30
7782	金九		30
7788	银十		10
7839	小芳		40
7844	小明		40
7876	小李		10
7900	小元		30
7902	小海		10
7934	小红明	30
Time taken: 36.959 seconds, Fetched: 14 row(s)
在编写内连接语句时，join子句体现了表与表之间的连接关系，表名也可以用子查询替代。
on子句用来指定join连接的连接条件，必须紧跟在join子句之后。连接条件可以使用等号连接，也可以使用其他比较运算符，如>、<>等。当使用等号连接时，就是等值连接，当使用不等号时，就是不等值连接。
join连接发生在 where子句之前。因此，如果要限制连接语句的输出结果，则应在 where子句中提出要求。如下查询语句所示，对inner join的结果进行筛选，只保留薪资大于2000的数据。
hive (default)> 
select 
    e.empno, 
    e.ename,
    e.sal, 
    d.deptno 
from emp e 
join dept d 
on e.deptno = d.deptno
where e.sal > 2000;
查询结果如下所示，可以看到，数据行数相对于上一个案例减少了。
e.empno	e.ename	e.sal	d.deptno
7566	赵六		2975.0	40
7698	马八		2850.0	30
7782	金九		2450.0	30
7788	银十		3000.0	10
7839	小芳		5000.0	40
7902	小海		3000.0	10
Time taken: 29.096 seconds, Fetched: 6 row(s)
5.4.4 左外连接
左外连接的完整语法如下所示，left outer join中的outer关键字可以省略。以下示例语句的结果，将为 tableA 中的每一行返回一行。当存在等于a.key 的b.key 时，该输出行将是a.val、b.val，当没有对应的b.key 时，输出行将是a.val、null。b 中没有相应 a.key 的行将被删除。
select a.val, b.val from tableA a left outer join tableB b on (a.key=b.key)
在左外连接中，join操作符左边表中符合where子句条件（如果存在where子句的话）的所有记录都将会被返回。在连接时，若join操作符右边表中没有找到符合连接条件的记录，则从右边表中选择的字段的值将会是null。
我们需要理解语法中的from tableA a left outer join tableB b，在这个语句中，tableA在tableB的左边，tableA是左表也是连接主表，所以来自 tableA的所有行都被保留，如下所示。
 
依然以统计员工的基本信息和部门信息为例，使用左外连接编写的查询语句如下所示。emp表为左表，dept表为右表。最终结果中，emp表的全部记录都会被返回，若emp中有些数据的deptno字段在dept表中不能找到对应记录，则d.deptno和d.dname的字段处为null，这样的数据表明这些员工的部门编号在dept表中没有记录。
hive (default)> 
select 
    e.empno, 
    e.ename, 
    d.deptno,
    d.dname
from emp e 
left join dept d 
on e.deptno = d.deptno;
如下所示，是emp表与dept表进行左外连接的示意图。两个表中未连接上的数据中，只有左表emp中的数据得以保留，并且以null值补充dept表中没有对应的列。
 
查询结果如下所示。
e.empno	e.ename	d.deptno	d.dname
7789	丁一		NULL		NULL
7989	艾斯		NULL		NULL
7369	张三		30			教学部
7499	李四		20			财务部
7521	王五		10			行政部
7566	赵六		40			销售部
7654	侯七		30			教学部
7698	马八		30			教学部
7782	金九		30			教学部
7788	银十		10			行政部
7839	小芳		40			销售部
7844	小明		40			销售部
7876	小李		10			行政部
7900	小元		30			教学部
7902	小海		10			行政部
7934	小红明	30			教学部
Time taken: 22.408 seconds, Fetched: 16 row(s)
5.4.5 右外连接
右外连接的完整语法如下所示，right outer join中的outer关键字可以省略。
select a.val, b.val from tableA a right outer join tableB b on (a.key=b.key)
可以看到，右外连接与左外连接的语法基本相同。在该语法中的from tableA a right outer join tableB b语句中，tableB为右表也是连接主表，tableB中的所有行都将被保留，如下所示。在连接时，若join操作符左边表中没有找到符合连接条件的记录，则从左边表中选择的字段的值将会是null。
 
继续以emp表和dept表为例，使用右外连接将emp表与dept表连接起来，查询语句如下所示。在以下查询语句中，dept表是右表，也是连接主表，所以dept表中的全部记录都会被返回。若dept表中的deptno列在emp表中不能找到对应记录，则e.empno和e.ename列处为null，这样的数据表明这些部门编号代表的部门在emp表中没有记录。
hive (default)> 
select 
    e.empno, 
    e.ename, 
    d.deptno 
from emp e 
right join dept d 
on e.deptno = d.deptno;
如下所示，是emp表与dept表进行右外连接的示意图。两个表中未连接上的数据，只有右表dept中的数据得以保留，并且以null值补充emp表中没有对应的列。
 
查询结果如下所示。
e.empno	e.ename	d.deptno
NULL	NULL	50
NULL	NULL	60
7521	王五		10
7788	银十		10
7876	小李		10
7902	小海		10
7499	李四		20
7369	张三		30
7654	侯七		30
7698	马八		30
7782	金九		30
7900	小元		30
7934	小红明	30
7566	赵六		40
7839	小芳		40
7844	小明		40
Time taken: 25.57 seconds, Fetched: 16 row(s)
容易想到，将以上右外连接的查询语句改写为dept表左外连接emp表，如下所示，得到的结果将是完全相同的。在实际开发中，左外连接和右外连接是完全可以互换的，开发者可以根据自己的需要选择。通常情况下，左外连接的使用更普遍。
 
5.4.6 满外连接
满外连接，也称为全外连接，将会返回所有表中符合where子句条件的所有记录，如下所示。full outer join中的outer关键字可以省略不写。如果任一表的指定字段没有符合条件的值的话，那么就使用null值替代。
 
继续以emp表和dept表为例，使用满外连接的查询语句如下所示。
hive (default)> 
select 
    e.empno, 
    e.ename, 
    d.deptno 
from emp e 
full join dept d 
on e.deptno = d.deptno;
如下所示，是emp表与dept表的满外连接过程示意图，可以看到两个表中的所有数据都被保留了。
 	查询结果如下所示。
e.empno	e.ename	d.deptno
7521	王五		10
7788	银十		10
7902	小海		10
7876	小李		10
7499	李四		20
7934	小红明	30
7900	小元		30
7782	金九		30
7698	马八		30
7654	侯七		30
7369	张三		30
7839	小芳		40
7844	小明		40
7566	赵六		40
NULL	NULL	50
NULL	NULL	60
7789	丁一		NULL
7989	艾斯		NULL
Time taken: 44.576 seconds, Fetched: 18 row(s)
容易想到的是，满外连接的结果是左外连接与右外连接结果的并集。
5.4.7 多表连接
join除了可以实现2个表之间的连接外，还可以实现多表连接。需要注意的是，连接n个表，至少需要n-1个连接条件。例如，连接3个表，至少需要2个连接条件。
将emp、dept和location三个表连接，查询语句如下所示。
hive (default)> 
select 
    e.ename, 
    d.dname, 
    l.loc_name
from emp e 
join dept d
on d.deptno = e.deptno 
join location l
on d.loc = l.loc;
查询结果如下所示。
e.ename	d.dname	l.loc_name
张三		教学部	深圳
李四		财务部	上海
王五		行政部	北京
赵六		销售部	北京
侯七		教学部	深圳
马八		教学部	深圳
金九		教学部	深圳
银十		行政部	北京
小芳		销售部	北京
小明		销售部	北京
小李		行政部	北京
小元		教学部	深圳
小海		行政部	北京
小红明	教学部	深圳
Time taken: 26.556 seconds, Fetched: 14 row(s)
大多数情况下，Hive会对每对join的连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce任务对emp表和dept表进行连接操作，再启动一个MapReduce任务将第一个MapReduce任务的输出和location表进行连接操作。
思考一个问题，为什么不是dept表和location表先进行连接操作呢？这是因为Hive的查询语句解析总是按照从左到右的顺序执行的。
以上的查询语句中，两个join均选用的内连接inner join。我们来分析以上查询语句的执行过程。首先emp表与dept表进行内连接，获得的结果是两表的共有行，再将结果与location表进行内连接。容易想到，最终结果集中并不包含emp表中所有员工的信息，因为在emp表和dept表的内连接过程中，emp表中的一部分行因为在dept表没有对应值而被丢弃了。
存在如下所示的三张表。
 
针对上述三张表执行以下查询语句。
select a.val1, a.val2, b.val, c.val
from a
join b on (a.key = b.key)
left outer join c on (a.key = c.key)
分析以上查询语句的执行过程，如下所示，首先执行a内连接b，会丢弃掉a表在b表中没有对应键的内容，也就是图中a表的key为3的数据。当a与b的内连接结果再与c表进行左外连接时，c表与a表的共同键所有行（key为3的数据）将不会出现，因为这一行数据在a表与b表的内连接过程中已经被丢弃掉了。这样的结果显然是不完整的。
 
为了达到更完整直观的效果，我们应该将查询语句做如下修改。
select a.val1, a.val2, b.val, c.val
from a
left join b on (a.key = b.key)
left join c on (a.key = c.key)
修改后，查询语句的执行过程如下所示，这样的结果是完整的。
 	以上案例的讲解，希望能令读者感受到，在多表连接时，表的连接顺序和选用的连接类型都会影响到最终的结果集，读者在使用时，应该谨慎分析。
5.4.8 笛卡尔积
Hive中提供了cross join关键字，用于实现笛卡尔积。
在hive.strict.checks.cartesian.product参数设置为true的严格模式下，以上语法是不能实现的，只有将该参数设置为false，以上语法才可以使用。
以emp表和dept表为例，使用cross join完成笛卡尔积连接。
hive (default)> 
select 
    empno, 
    dname 
from emp cross join dept;
笛卡尔积连接不需要使用连接条件。省略cross join关键字，也可以达到笛卡尔积连接的效果，如下所示。
hive (default)> 
select 
    empno, 
    dname 
from emp ,dept;
笛卡尔积连接时，Hive SQL的执行过程如下所示。从图中可以看出，执行笛卡尔连接的两表，若A表的数据量为n条，B表的数据量为m条，则最终的结果集的数据量将为n*m条，所以应谨慎使用，尽量避免造成笛卡尔积的结果。
 
5.4.9 join与MapReduce程序
我们已经知道，Hive SQL会被解析成MapReduce任务。一个简单的双表join查询语句，会被解析成一个MapReduce任务。
对于多表连接中的每个表，如果在on子句中使用相同的列组成连接条件， Hive 会将多个表的连接转换为单个MapReduce任务，如下所示。
select a.val, b.val, c.val from a join b on (a.key = b.key1) join c on (c.key = b.key1)
以上查询语句被转换为单个 MapReduce任务，因为连接中只涉及了 b 表的 key1 列。
如下所示的查询语句，将会被转换为两个MapReduce任务，因为来自 b 的 key1 列用于第一个连接条件，而来自 b 的 key2 列用于第二个连接条件。第一个 MapReduce任务连接 a 和 b，然后在第二个 MapReduce任务中将结果与 c 连接。
select a.val, b.val, c.val from a join b on (a.key = b.key1) join c on (c.key = b.key2)
在连接的每个 map/reduce 阶段，序列中的最后一个表会通过 reducer 进行流式传输，而其他表则会被缓冲。因此，通过合理安排join顺序，使得最大的表出现在序列的最后，有助于减少 reducer 中缓冲连接键的特定值的行所需的内存。如以下查询语句。
select a.val, b.val, c.val from a join b on (a.key = b.key1) join c on (c.key = b.key1)
所有三个表都在一个单独的MapReduce任务中进行连接，对于表 a 和表 b 的键值的特定值，他们的值会在 reducers 的内存中被缓冲。然后对于从 表c中 检索到的每一行，都会使用缓冲行进行连接。同样对于以下查询语句。
select a.val, b.val, c.val from a join b on (a.key = b.key1) join c on (c.key = b.key2)
计算连接涉及两个 MapReduce任务。其中，第一个作业将表a与表b进行连接，并在reducers中流式传输表b的值时缓冲表a的值。第二个作业则在reducers中缓冲第一个连接的结果，同时通过reducers流式传输表c的值。
5.4.10 联合（union & union all）
除表与表之间有条件的连接外，Hive还支持表之间的联合，关键字是union或union all。表之间的联合的含义是，将两个查询语句的查询结果直接拼接在一起，关键语法如下所示。
select_statement union [all | distinct] select_statement union [all | distinct] select_statement ...
通过以上语法我们可以得知，union用于将多个select语句的的结果组成一个结果集。Hive 1.2.0之前版本只支持union all，也就是不去重的表联合。
使用union和union all，要注意以下几点。
	union和union all都是将查询语句的查询结果上下联合，这点和join是有区别的，join是两表的左右连接，union和union all是上下拼接。
	union关键字会对联合结果去重，union all不去重。
	union和union all在上下拼接查询语句时要求，两个查询语句的结果，列的个数和名称必须相同，且上下对应列的类型必须一致。
如下查询语句所示，将emp表中部门编号为30的员工信息和部门编号为40的员工信息，利用union进行拼接显示。
hive (default)> 
select 
    *
from emp
where deptno=30
union
select 
    *
from emp
where deptno=40;
使用union对两个select语句进行拼接的过程如下所示。
 	查询结果如下所示。
_u1.empno	_u1.ename	_u1.job	_u1.sal	_u1.deptno
7369		张三			研发		800.0	30
7566		赵六			销售		2975.0	40
7654		侯七			研发		1250.0	30
7698		马八			研发		2850.0	30
7782		金九			NULL	2450.0	30
7839		小芳			销售		5000.0	40
7844		小明			销售		1500.0	40
7900		小元			讲师		950.0	30
7934		小红明		讲师		1300.0	30
Time taken: 39.639 seconds, Fetched: 9 row(s)
当用户需要对union的结果进行额外处理时，可以将整个union语句嵌入到from子句中，语法如下所示。
select *
from (
  select_statement
  union [all | distinct]
  select_statement
) union_result
例如，我们利用union查询emp表中部门编号为30和40的所有员工信息，并将查询结果与dept表进行join连接，获取到所有员工的部门名称dname，则查询语句如下所示。
hive (default)> 
select
    union_result.ename,
    union_result.deptno,
    dept.dname
from(
    select 
        *
    from emp
    where deptno=30
    union
    select 
        *
    from emp
    where deptno=40
) union_result 
join dept
on union_result.deptno = dept.deptno;
查询结果如下所示。
union_result.ename	union_result.deptno	dept.dname
张三					30					教学部
侯七					30					教学部
马八					30					教学部
金九					30					教学部
小元					30					教学部
小红明					30					教学部
赵六					40					销售部
小芳					40					销售部
小明					40					销售部
Time taken: 52.786 seconds, Fetched: 9 row(s)
5.5 综合案例练习——基础查询（8道题）
 
第6章 函数初级
6.1 函数简介
Hive会将常用的逻辑封装成函数给用户进行使用，类似于Java中的函数。
好处：避免用户反复写逻辑，可以直接拿来使用。
重点：用户需要知道函数叫什么，能做什么。
Hive提供了大量的内置函数，按照其特点可大致分为如下几类：单行函数、聚合函数、炸裂函数、窗口函数。
以下命令可用于查询所有内置函数的相关信息。
1）查看系统内置函数
hive> show functions;
2）查看内置函数用法
hive> desc function upper;
3）查看内置函数详细信息
hive> desc function extended upper;
6.2 单行函数
单行函数的特点是一进一出，即输入一行，输出一行。
单行函数按照功能可分为如下几类：日期函数、字符串函数、集合函数、数学函数、流程控制函数等。
6.2.1 算术运算函数
运算符	描述
A+B	A和B 相加
A-B	A减去B
A*B	A和B 相乘
A/B	A除以B
A%B	A对B取余
A&B	A和B按位取与
A|B	A和B按位取或
A^B	A和B按位取异或
~A	A按位取反
案例实操：查询出所有员工的薪水后加1显示。
hive (default)> select sal + 1 from emp;
6.2.2 数值函数
Hive提供了很多数值函数，方便用户对数值类型的字段进行处理。常用的数值函数有以下几个。
（1）round：取整函数。
语法：round(double A)或round(double A,int b)。
说明：round函数只传入一个double类型的参数时，会遵循四舍五入原则返回double类型数值的整数部分。当传入第二个整数类型的参数时，会遵循四舍五入原则返回指定精度的double类型的结果。
 
round函数的使用如下。
hive (default)> select round(3.3);
3
hive (default)> select round(3.33336,4);
3.3334
（2）ceil：向上取整函数。
说明：传入double类型的参数，遵循向上取整的原则返回整数值。
 
ceil函数的使用如下。
hive (default)> select ceil(3.1) ;
4
（3）floor：向下取整函数。
说明：传入double类型的参数，遵循向下取整的原则返回整数值。
 
floor函数的使用如下。
hive (default)> select floor(4.8);
4
（4）rand：随机数函数。
说明：用来生成0到1之间的随机数。
 
若需要生成0至100之间的随机数，使用方式如下。
hive (default)> select round(100 * rand());
15.0
（5）abs：绝对值函数。
说明：用来返回数值的绝对值。
 
abs函数的使用如下。
hive (default)> select abs(-20);
20
hive (default)> select abs(20);
20
6.2.3 字符串函数
1）substring：截取字符串
语法一：substring(string A, int start) 
返回值：string 
说明：返回字符串A从start位置到结尾的字符串。
语法二：substring(string A, int start, int len) 
返回值：string
说明：返回字符串A从start位置开始，长度为len的字符串。
 
案例实操：
	（1）获取第二个字符以后的所有字符
hive> select substring("atguigu",2);
输出：
tguigu
	（2）获取倒数第三个字符以后的所有字符
hive> select substring("atguigu",-3);
输出：
igu
	（3）从第3个字符开始，向后获取2个字符
hive> select substring("atguigu",3,2);
输出：
gu
2）replace ：替换
语法：replace(string A, string B, string C) 
返回值：string
说明：将字符串A中的子字符串B替换为C。
 
hive> select replace('atguigu', 'a', 'A')
输出：
hive> Atguigu
3）regexp_replace：正则替换
语法：regexp_replace(string A, string B, string C) 
返回值：string
说明：将字符串A中的符合Java正则表达式pattern的部分替换为replacement。注意，在有些情况下要使用转义字符。
 
案例实操：将字符串中的数字全部替换为“num”。“\d+”表示一个或多个数字字符，“\d”需要使用转义字符。
hive> select regexp_replace('100-200', '(\\d+)', 'num') 
输出：
hive> num-num
4）regexp：正则匹配
语法：字符串 regexp 正则表达式
返回值：boolean
说明：若字符串符合正则表达式，则返回true，否则返回false。
	（1）正则匹配成功，输出true
hive> select 'dfsaaaa' regexp 'dfsa+'
输出：
hive> true
	（2）正则匹配失败，输出false
hive> select 'dfsaaaa' regexp 'dfsb+';
输出：
hive> false
5）repeat：重复字符串
语法：repeat(string A, int n)
返回值：string
说明：将字符串A重复n遍，组成一个新的字符串。
 
案例：将字符串“123”重复3次
hive> select repeat('123', 3);
输出：
hive> 123123123
6）split ：字符串切割
语法：split(string str, string pat) 
返回值：array
说明：按照正则表达式pat的内容切割字符串str，切割后的字符串，以数组的形式返回。需要注意的是，如果选择的分隔符在正则表达式中有特殊含义，则需要对分隔符进行转义。例如，对字符串“192.168.11.12”按照“.”进行切割，则函数使用方式为split('192.168.11.12', '\\.')。
 
案例：将字符串“a-b-c-d”按照“-”进行分割。
hive> select split('a-b-c-d','-');
输出：
hive> ["a","b","c","d"]
7）concat ：拼接字符串
语法：concat(string A, string B, string C, ……) 
返回：string
说明：将A,B,C……等字符拼接为一个字符串。
 
hive> select concat('beijing','-','shanghai','-','shenzhen');
输出：
hive> beijing-shanghai-shenzhen
8）concat_ws：以指定分隔符拼接字符串或者字符串数组
	语法：concat_ws(string A, string…| array(string)) 
返回值：string
说明：使用分隔符A拼接多个字符串，或者一个数组的所有元素。
 
案例一：使用分隔符“-”拼接多个字符串
hive>select concat_ws('-','beijing','shanghai','shenzhen');
输出：
hive> beijing-shanghai-shenzhen
案例二：使用分隔符“-”拼接字符串数组的所有元素
hive> select concat_ws('-',array('beijing','shenzhen','shanghai'));
输出：
hive> beijing-shanghai-shenzhen
10）get_json_object：解析json字符串
	语法：get_json_object(string json_string, string path) 
返回值：string
说明：解析json的字符串json_string，返回path指定的内容。如果输入的json字符串无效，那么返回NULL。
 
案例实操：
	（1）获取JSON字符串数组里的第1个对象的name属性
hive> select get_json_object('[{"name":"大海海","sex":"男","age":"25"},{"name":"小宋宋","sex":"男","age":"47"}]','$.[0].name');
输出：
hive> 大海海
	（2）获取JSON字符串数组里的第1个对象
hive> select get_json_object('[{"name":"大海海","sex":"男","age":"25"},{"name":"小宋宋","sex":"男","age":"47"}]','$.[0]');
输出：
hive> {"name":"大海海","sex":"男","age":"25"}
（10）length：字符串长度函数。
语法：length(string A)。
返回值：int。
说明：返回字符串A的长度。
hive (default)> select length('helloatguigu');
12
（11）lower、upper：字符串大小写转换函数。
语法：lower(string A)、upper(string A)。
返回值：string。
说明：lower函数可以将字符串中的所有字母转换为小写，upper函数可以将字符串中的所有字母转换为大写。
 
upper和lower函数的使用代码如下所示。
hive (default)> select lower('ATGUIGU');
atguigu
hive (default)> select upper('hello');
HELLO
（12）ltrim、rtrim、trim：空格截断函数。
语法：ltrim(string A)、rtrim(string A)、trim(string A)。
返回值：string。
说明：ltrim函数可以截断字符串左边的空格，rtrim函数可以截断字符串右边的空格，trim函数可以同时截断字符串左边和右边的空格。在Hive的使用中，有时需要对数据进行清洗，部分字符串可能会因为不正确的字段分隔造成首尾存在空格，影响数据分析，这时候就需要截断字符串首尾的空格。
 
三种空格截断函数的使用代码如下所示。
hive (default)> select ltrim('   ATGUIGU   ');
ATGUIGU   
hive (default)> select rtrim('   ATGUIGU   ');
   ATGUIGU
hive (default)> select trim('   ATGUIGU   ');
ATGUIGU
6.2.4 日期函数
1）unix_timestamp函数
语法一：unix_timestamp()。
说明：返回当前时间的UNIX时间戳，这个用法已经被弃用，改用current_timestamp函数获取当前时间戳。
语法二：unix_timestamp(string date)。
说明：返回指定时间date的UNIX时间戳，时间date需要是yyyy-MM-dd HH:mm:ss格式，如果转换失败，则返回0。
语法三：unix_timestamp(string date, string pattern)。
说明：转换指定的pattern格式的日期为UNIX时间戳。
案例实操：
hive> select unix_timestamp('2022/08/08 08-08-08','yyyy/MM/dd HH-mm-ss');  
输出：
1659946088
说明：-前面是日期后面是指，日期传进来的具体格式 。
2）from_unixtime：转化UNIX时间戳（从 1970-01-01 00:00:00 UTC 到指定时间的秒数）到当前时区的时间格式
语法：from_unixtime(bigint unixtime[, string format]) 
返回值：string 
案例实操：
hive> select from_unixtime(1659946088);
输出：
2022-08-08 08:08:08
3）current_date：函数。
说明：返回当前的日期，格式为yyyy-MM-dd。
hive> select current_date;
输出：
2022-07-11
4）current_timestamp：当前的日期加时间，并且精确的毫秒 
hive> select current_timestamp;
输出：
2022-07-11 15:32:22.402
5）month：获取日期中的月
语法：month (string date) 
返回值：int 
案例实操：
hive> select month('2022-08-08 08:08:08');
输出：
8
6）day：获取日期中的日
语法：day (string date) 
返回值：int 
案例实操：
hive> select day('2022-08-08 08:08:08')
输出：
8
7）hour：获取日期中的小时
语法：hour (string date) 
返回值：int 
案例实操：
hive> select hour('2022-08-08 08:08:08');
输出：
8
8）datediff：两个日期相差的天数（结束日期减去开始日期的天数）
语法：datediff(string enddate, string startdate) 。
返回值：int。
说明：计算两个日期相差的天数（enddate减去startdate的天数）。
 
案例实操：
hive> select datediff('2021-08-08','2022-10-09');
输出：
-427
9）date_add：日期加天数
语法：date_add(string startdate, int days) 
返回值：string 
说明：返回开始日期 startdate 增加 days 天后的日期。
 
案例实操：
hive> select date_add('2022-08-08',2);
输出：
2022-08-10
10）date_sub：日期减天数
语法：date_sub (string startdate, int days) 
返回值：string 
说明：返回开始日期startdate减少days天后的日期。
 
案例实操：
hive> select date_sub('2022-08-08',2);
输出：
2022-08-06
11）date_format:将标准日期解析成指定格式字符串
语法：date_sub (date/timestamp/string ts, string fmt)
返回值：string。
说明：将标准日期、时间戳或日期字符串解析成指定格式的字符串，日期和时间格式由日期和时间模式字符串fmt指定。常用的模式字母如下。
	y，代表年份，yyyy会将日期格式化成四位数年份，yy会将日期格式化成年份后两位。
	M，代表月份，MMM会将日期中的月份格式化成文本，如Jul、Jan；MM会将日期中的月份格式化成数字，如07、01。
	d，代表月份中的天数，使用方式为dd。
	u，星期几的天数（1代表星期一，2代表星期二，以此类推）。
	H，一天中的小时数，使用方式为HH。
	m，小时中的分钟数，使用方式为mm。
	s，分钟中的秒数，使用方式为ss。
通过上述常见模式字母的讲解，我们了解到，模式字母通常会重复，重复的数量决定了格式化的形式。通过模式字母的灵活的组合可以将日期时间格式化为用户所需要的各种格式。
 
hive> select date_format('2022-08-08','yyyy年-MM月-dd日')
输出：
2022年-08月-08日
6.2.5 流程控制函数
1）nvl：控制查找函数
语法：nvl(A,B)。 
说明：若A的值不为null，则返回A，否则返回B。
 
nvl函数的使用代码如下所示。
hive (default)> select nvl(null,1); 
hive (default)> 1
2）case when：条件判断函数
语法一：case when a then b [when c then d]* [else e] end 
返回值：T 
说明：如果a为true，则返回b；如果c为true，则返回d；否则返回 e 。
hive> select case when 1=2 then 'tom' when 2=2 then 'mary' else 'tim' end from tabl eName;
mary
语法二： case a when b then c [when d then e]* [else f] end 
返回值： T 
说明：如果a等于b，那么返回c；如果a等于d，那么返回e；否则返回f 。
hive> select case 100 when 50 then 'tom' when 100 then 'mary' else 'tim' end from t ableName;
mary
 
3）if: 条件判断，类似于Java中三元运算符
语法：if（boolean testCondition, T valueTrue, T valueFalseOrNull）
返回值：T 
说明：当条件testCondition为true时，返回valueTrue；否则返回valueFalseOrNull。
 
（1）条件满足，输出正确
hive> select if(10 > 5,'正确','错误'); 
输出：正确
（2）条件满足，输出错误
hive> select if(10 < 5,'正确','错误');
输出：错误
4）coalesce：非空值查找函数
语法：coalesce(T v1, T v2, ……)。
返回值：T。
说明：coalesce函数可以传入多个参数，函数会依次判断各参数，遇到非null值即停止查找，并返回该值。如果所有的参数都是null值，则会返回null。
coalesce函数的使用场景是：需要依次查找多个变量，我们只需要在这多个变量中取一个非空的即可。例如：在用户表user_info中，记录有用户的家庭住址family_address和办公地址office_address，我们需要查询出用户的一个地址，若家庭地址和办公地址均未记录的话，则显示“未登记地址”，使用方式如下。
 
查询语句如下代码所示。
select
	user_name，
	coalesce(family_address, office_address, '未登记')
from user_info;
6.2.6 集合函数
1）array 声明array集合
语法：array(val1, val2, …) 
说明：根据输入的参数构建数组array类。
案例实操：
hive> select array('1','2','3','4');
输出：
hive>["1","2","3","4"]
2）array_contains: 判断array中是否包含某个元素
语法：array_contains(Array<T>, value)。
返回值：boolean。
说明: 判断array中是否包含value元素，如果存在返回true，如果不存在，返回false。
 
hive> select array_contains(array('a','b','c','d'),'a');
输出：
hive> true
3）sort_array：将array中的元素排序
语法：sort_array(array<T>)。
返回值：array<T>。
说明：按照数组中元素类型的自然顺序升序将array中的元素排序。
 
hive> select sort_array(array('a','d','c'));
输出：
hive> ["a","c","d"]
4）size：集合中元素的个数
语法：size(array<T>)。
返回值：int。
说明：用于获取数组中元素的个数。
 
hive> select size(friends) from test;  --2/2  每一行数据中的friends集合里的个数
5）map：创建map集合
语法：map (key1, value1, key2, value2, …) 
说明：根据输入的key和value对构建map类型。
案例实操：
hive> select map('xiaohai',1,'dahai',2);  
输出：
hive> {"xiaohai":1,"dahai":2}
6）map_keys： 返回map中的key
语法：map_keys(map<K.V>)。
返回值：array<K>。
说明：以不排序数组的形式返回map中的所有key。
 
hive> select map_keys(map('xiaohai',1,'dahai',2));
输出：
hive>["xiaohai","dahai"]
7）map_values: 返回map中的value
语法：map_values(map<K.V>)。
返回值：array<V>。
说明：以不排序数组的形式返回map中的所有value。
 
hive> select map_values(map('xiaohai',1,'dahai',2));
输出：
hive>[1,2]
8）struct声明struct中的各属性
语法：struct(val1, val2, val3, …) 
说明：根据输入的参数构建结构体struct类。
案例实操：
hive> select struct('name','age','weight');
输出：
hive> {"col1":"name","col2":"age","col3":"weight"}
9）named_struct声明struct的属性和值
hive> select named_struct('name','xiaosong','age',18,'weight',80);
输出：
hive> {"name":"xiaosong","age":18,"weight":80}
6.2.7 案例演示
1. 数据准备
1）表结构
name	sex	birthday	hiredate	job	salary	bonus	friends	children
张无忌	男	1980/02/12	2022/08/09	销售	3000	12000	[阿朱，小昭]	{张小无:8,张小忌:9}
赵敏	女	1982/05/18	2022/09/10	行政	9000	2000	[阿三，阿四]	{赵小敏:8}
黄蓉	女	1982/04/13	2022/06/11	行政	12000	Null	[东邪，西毒]	{郭芙:5,郭襄:4}
2）建表语句
hive> 
create  table  employee(
    name string,  --姓名
    sex  string,  --性别
    birthday string, --出生年月
    hiredate string, --入职日期
    job string,   --岗位
    salary double, --薪资
    bonus double,  --奖金
    friends array<string>, --朋友
    children map<string,int> --孩子
)
3）插入数据
hive> insert into employee  
  values('张无忌','男','1980/02/12','2022/08/09','销售',3000,12000,array('阿朱','小昭'),map('张小无',8,'张小忌',9)),
        ('赵敏','女','1982/05/18','2022/09/10','行政',9000,2000,array('阿三','阿四'),map('赵小敏',8)),
        ('宋青书','男','1981/03/15','2022/04/09','研发',18000,1000,array('王五','赵六'),map('宋小青',7,'宋小书',5)),
        ('周芷若','女','1981/03/17','2022/04/10','研发',18000,1000,array('王五','赵六'),map('宋小青',7,'宋小书',5)),
        ('郭靖','男','1985/03/11','2022/07/19','销售',2000,13000,array('南帝','北丐'),map('郭芙',5,'郭襄',4)),
        ('黄蓉','女','1982/12/13','2022/06/11','行政',12000,null,array('东邪','西毒'),map('郭芙',5,'郭襄',4)),
        ('杨过','男','1988/01/30','2022/08/13','前台',5000,null,array('郭靖','黄蓉'),map('杨小过',2)),
        ('小龙女','女','1985/02/12','2022/09/24','前台',6000,null,array('张三','李四'),map('杨小过',2))
2. 需求
案例一：统计每个月的入职人数
（1）期望结果
month	cnt
4	2
6	1
7	1
8	2
9	2
（2）需求分析
需要统计每个月的入职人数，那么首先应该从入职日期字段中取得入职月份。选用month函数完成此功能。但是month函数要求输入的日期格式以“-”分隔，员工表中的入职日期字段以“/”分隔，所以，首先要使用replace函数，将入职日期字段中的“/”替换为“-”。在获得入职月份后，按照入职月份分组，使用count函数，统计每个月的入职人数。
 	（3）查询语句
select
  month(replace(hiredate,'/','-')) as month,
  count(*) as cn
from
  employee
group by
  month(replace(hiredate,'/','-'))
案例二：查询每个人的年龄（年 + 月）
（1）期望结果
name	age
张无忌	42年8月
赵敏	40年5月
宋青书	41年7月
周芷若	41年7月
郭靖	37年7月
黄蓉	39年10月
杨过	34年9月
小龙女	37年8月
（2）需求分析
本案例分三步完成。
第一步：将出生日期birthday字段转换成以“-”分隔的标准日期格式。
 
第二步：用当前日期（通过current_date函数获得）的年份减去出生日期的年份，用当前日期的月份减去出生日期的月份，年份通过year函数获得，月份通过month函数获得。
 
第三步：通过第二步计算得到的月份差值有可能为负，对这种情况进行处理。若月份差值为正，则直接将年份差值和月份差值拼接，得到员工年龄。若月份差值为负，则将年份差值减1，月份差值加12，拼接后得到员工年龄。对月份差值正负的判断使用if函数完成。
 
（3）查询语句
-- 转换日期
select
  name,
  replace(birthday,'/','-') birthday
from
  employee  t1

-- 求出年和月
select
  name,
  year(current_date())-year(t1.birthday) year,
  month(current_date())-month(t1.birthday) month
from
  (
    select
      name,
      replace(birthday,'/','-') birthday
    from
      employee
)t1 t2

-- 根据月份正负决定年龄

select
  name,
  concat(if(month>=0,year,year-1),'年',if(month>=0,month,12+month),'月') age
from
  (
    select
      name,
      year(current_date())-year(t1.birthday) year,
      month(current_date())-month(t1.birthday) month
    from
      (
        select
          name,
          replace(birthday,'/','-') birthday
        from
          employee
    )t1
)t2 
案例三：按照薪资，奖金的和进行倒序排序，如果奖金为null，置位0
（1）期望结果
name	sal
周芷若	19000
宋青书	19000
郭靖	15000
张无忌	15000
黄蓉	12000
赵敏	11000
小龙女	6000
杨过	5000
（2）需求分析
本案例关键之处在于对奖金bonus字段null值的处理，由于奖金字段存在null值，若null值与数值相加，得到的结果将为null，所以不能直接与薪资salary字段相加。使用nvl函数，对bonus字段进行判断，若bonus为null，赋默认值为0。
 
（3）查询语句
select
  name,
  salary + nvl(bonus,0) sal
from
  employee
order by
   sal desc
案例四：查询每个人有多少个朋友
（1）期望结果
name	cnt
张无忌	2
赵敏	2
宋青书	2
周芷若	2
郭靖	2
黄蓉	2
杨过	2
小龙女	2
（2）需求分析
员工的朋友friends字段的类型是array，统计朋友个数，可以使用size函数。
 
（3）查询语句
select 
name,
size(friends) cnt
from 
employee; 
案例五：查询每个人的孩子的姓名
（1）期望结果
name	ch_name
张无忌	["张小无","张小忌"]
赵敏	["赵小敏"]
宋青书	["宋小青","宋小书"]
周芷若	["宋小青","宋小书"]
郭靖	["郭芙","郭襄"]
黄蓉	["郭芙","郭襄"]
杨过	["杨小过"]
小龙女	["杨小过"]
（2）需求分析
员工的孩子children字段是map类型的，map的key即姓名，所以统计每个员工的孩子姓名可以直接使用map_keys函数，即可返回员工孩子姓名组成的array。
 
（3）查询语句
hive>
select 
name,
map_keys(children) ch_name
from 
employee; 
案例六：查询每个岗位男女各多少人
（1）期望结果
job	male	female
前台	1	1
研发	1	1
行政	0	2
销售	2	0
（2）需求分析
从案例需求来看，容易想到使用分组聚合来完成。但是需要注意的是，直接按照job和sex字段分组group by，可以计算出结果，但是与表7-7所示的期望结果是不符的。
本案例我们使用if函数与sum函数联合完成，称之为有条件聚合。
使用if函数对sex字段进行判断，若sex字段是男，返回值为1，否则为0，再对返回值求和，得到的即为男性员工个数。
同样的思路得到女性员工个数。
 
（3）查询语句
select
  job,
  sum(if(sex='男',1,0)) male,
  sum(if(sex='女',1,0)) female
from
  employee
group by 
  job
6.3 高级聚合函数
多进一出 （多行传入，一个行输出）。
1）普通聚合 count/sum.... 见第6章 6.2.4
2）collect_list
语法：collect_list(col)。
返回值：array
说明：将分组后，每个分组内的所有值收集形成array数组，结果不去重。
案例代码如下。
hive>
select 
  sex,
  collect_list(job)
from
  employee
group by 
  sex
过程如下。
 
结果：
女	["行政","研发","行政","前台"]
男	["销售","研发","销售","前台"]
3）collect_set
语法：collect_set(col)。
返回值：array。
说明：将分组后，每个分组内的所有值收集形成array数组，结果去重。
案例代码如下。
hive>
select 
  sex,
  collect_set(job)
from
  employee
group by 
  sex
过程如下。
 
结果：
女	["行政","研发","前台"]
男	["销售","研发","前台"]

4）案例演示
（1）每个月的入职人数以及姓名
hive> 
select
  month(replace(hiredate,'/','-')) as month,
  count(*) as cn,
  Collect_list(name) as name_list
from
  employee
group by
  month(replace(hiredate,'/','-'))
 
结果：
month  cn  name_list
4	    2	["宋青书","周芷若"]
6	    1	["黄蓉"]
7	    1	["郭靖"]
8	    2	["张无忌","杨过"]
9	    2	["赵敏","小龙女"]
6.4 综合案例练习——基础函数（6道题目）
 
第7章 函数高级
7.1 炸裂函数
UDTF的全称是User Defined Table-Generation Function，即用户定义的表生成函数。简单理解，UDTF就是接收一行数据，输出一行或者多行数据。系统内置的常用的UDTF有explode、posexplode、inline等，接下来将分别介绍。
 
7.1.1 常用的UDTF
1. explode函数
explode函数有两种使用方式，区别在于传入参数类型的不同。
（1）语法一：explode(array<T> a)
说明：传入参数为array数组类型，返回一行或多行结果，每行对应array数组中的一个元素。
 
使用方式如下所示。
hive (default)> select explode(array("a","b","c")) as item;
item
a
b
c
（2）语法二：explode(map<K,V> m)
说明：传入参数为map类型，由于map是key-value结构的，所以explode函数会将map参数转换为两列，一列是key，一列是value。
 
使用方式如下所示。
hive (default)> select explode(map("a",1,"b",2,"c",3)) as (key,value);
key	value
a	1
b	2
c	3
注意：as (key,value)用于给转换后的两列数据起列名，两个列名用括号括起来并用逗号分隔。
2. posexplode函数
语法：posexplode(array<T> a)。
说明：posexplode函数的用法与explode函数相似，增加了pos前缀，表明在返回array数组的每一个元素的同时，还会返回元素在数据所处的位置。
 
使用方式如下所示。
hive (default)> select posexplode(array("a","b","c")) as (pos,item);
pos	item
0	a
1	b
2	c
3. inline函数
语法：inline(array<struct<f1:T1,...,fn:Tn>> a)
说明：inline函数接受的参数结构体数组，其可将数组中的每个结构体输出为一行，每个结构体中的列，会展开为一个个单独的列。
 
使用方式如下所示。
hive (default)> select inline(array(named_struct("id", 1, "name", "zs"),
                             named_struct("id", 2, "name", "ls"),
                             named_struct("id", 3, "name", "ww"))) as (id, name);
id	name
1	zs
2	ls
3	ww
4. lateral view
UDTF函数可以将一行数据转换为多行，出现在select语句中时，不能与其他列同时出现，会报如下所示错误信息。
org.apache.hadoop.hive.ql.parse.SemanticException:UDTF's are not supported outside the SELECT clause, nor nested in expressions
这时候需要用到lateral view关键字。lateral view通常与UDTF配合使用。lateral view可以将UDTF应用到原表的每行数据，将每行数据转换为一行或多行，并将源表中每行的输出结果与该行连接起来，形成一个虚拟表。
lateral view的使用语法如下所示。
select 
	col1 [,col2,col3……] 
from 表名 
lateral view udtf(expression) 虚拟表别名 as col1 [,col2,col3……]
lateral view一般在from子句后使用，紧跟在UDTF后面的是虚拟表的别名，虚拟表别名不可省略。as关键字后为执行UDTF后的列的别名，UDTF函数生成几列就要给出几个列别名，多个列别名间使用逗号分隔。
person表结构和数据如下所示。
id	name	hobbies
1	zs	[reading,coding]
2	ls	[coding,running]
3	ww	[sleeping]
执行如下所示的查询语句，对hobbies列执行explode函数，虚拟表别名为tmp，得到的列为hobby。
hive (default)> select
    id,
    name,
    hobbies,
    hobby
from person lateral view explode(hobbies) tmp as hobby;
得到执行结果如下所示。
id	name	hobbies	hobby
1	zs	[reading,coding]	reding
1	zs	[reading,coding]	coding
2	ls	[coding,running]	coding
2	ls	[coding,running]	running
3	ww	[sleeping]	sleeping
7.1.2 案例演示
1）数据准备
（1）表结构
movie	category
《疑犯追踪》	悬疑，动作，科幻，剧情
《Lie to me》	悬疑，警匪，动作，心理，剧情
《战狼2》	战争，动作，灾难
（2）建表语句
hive (default)>
create table movie_info(
    movie string,     --电影名称
    category string   --电影分类
) 
row format delimited fields terminated by "\t";
（3）装载语句
insert overwrite table movie_info
values ("《疑犯追踪》", "悬疑,动作,科幻,剧情"),
       ("《Lie to me》", "悬疑,警匪,动作,心理,剧情"),
       ("《战狼2》", "战争,动作,灾难");
2）需求
（1）需求说明：根据上述电影信息表，统计各分类的电影数量，期望结果如下：
剧情	2
动作	3
心理	1
悬疑	2
战争	1
灾难	1
科幻	1
警匪	1
（2）答案
select
    cate,
    count(*)
from
(
    select
        movie,
        cate
    from
    (
        select
            movie,
            split(category,',') cates
        from movie_info
    )t1 lateral view explode(cates) tmp as cate
)t2
group by cate;
（3）思路分析
使用split函数将category列转换成array数组类型，array数组中存放的是每个电影所属的电影类型，得到子查询t1。
 
然后将lateral view和explode函数结合使用，即可得到子查询t2。在子查询t2中，电影与电影类型是一对一的关系。
 
根据子查询t2使用分组聚合即可得到需求结果。
 
7.2 窗口函数（开窗函数）
窗口函数能够为每行数据划分一个窗口，然后对窗口范围内的数据进行计算，最后将计算结果返回给该行数据。
如图所示，原表中有7行数据，分别是1-7，为每行数据划分的窗口范围是当前行数据的前一行至当前行，并对窗口范围内的数据进行加和。以第2行数据为例，划分的窗口范围就是1-2。最终得到的计算结果如图右侧所示。需要注意，此案例仅为一种简单的窗口范围划分方式，Hive的窗口函数还可以进行更丰富的窗口范围划分，在后文中将会详细讲解。
 
窗口函数在实际开发中应用非常广泛，本节将重点讲解窗口函数的语法和使用。
7.2.1 概述
 
窗口函数的语法示例如下所示，主要包括“窗口”和“函数”两部分。其中“窗口”部分使用over关键字定义，用于定义窗口范围，“函数”部分用于定义对窗口范围内的数据执行的计算逻辑。
select 
    col_1, 
    col_2, 
    col_3, 
    函数(col_1) over (窗口范围) as 别名
from table_name; 
1. 函数
根据窗口函数的特点，不难发现，每个窗口中的计算逻辑，都是多（行）进一（行）出，因此绝大多数的聚合函数都可以配合窗口使用，例如max、min、sum、count、avg等。
以sum函数为例，使用方式如下。
select 
    order_id, 
    order_date, 
    amount, 
    sum(amount) over (窗口范围) total_amount
from order_info;
若order_info表的数据如下所示，且将当前行的窗口范围定义为上一行到当前行，请读者思考，上述查询语句的结果是什么。
order_info
order_id	order_date	amount
1	2022-01-01	10
2	2022-01-02	20
3	2022-01-03	10
4	2022-01-04	30
5	2022-01-05	40
6	2022-01-06	20
得到的结果如下所示，可以看出新增的total_amount列的值，是每一行数据的上一行至当前行的amount列的值的加和。
使用开窗函数的查询结果
order_id	order_date	amount	total_amount
1	2022-01-01	10	10
2	2022-01-02	20	30
3	2022-01-03	10	30
4	2022-01-04	30	40
5	2022-01-05	40	70
6	2022-01-06	20	60
2. 窗口
窗口范围的定义分为两种类型，一种是基于行进行定义，一种是基于值进行定义。它们都用来确定一个窗口中应该包含哪些行，但是确定的逻辑有所不同。
基于行的窗口范围定义，是通过行数的偏移量，来确定窗口范围，例如：某行的窗口范围可以包含当前行的前一行到当前行。
基于值的窗口范围定义，是通过某个列值的偏移量，来确定窗口范围，例如：若某行A列的值为10，其窗口范围可以包含，A列值大于等于10-1且小于等于10的所有行。 
（1）基于行
基于行的窗口范围定义语法如下。
sum(amount) over(order by <column> rows between <start> and <end>)
其中start用于定义窗口范围的起点，end用于定义窗口范围的终点，order by用于声明划分窗口范围时数据的顺序，因为基于行的窗口范围定义，对数据的顺序是敏感的，故使用该方式定义窗口范围时，读者需根据需要指定排序列。
如下所示，是基于行的窗口范围如何定义起点和终点的详细语法。between关键字后可选的是窗口起点，and关键字后可选的是窗口终点。图中的[num]为相对当前行的偏移量。需要注意的是，窗口起点不能超过终点，例如当起点是current row时，终点不能是[num] preceding。
  
以下案例使用基于行的方式定义窗口范围，请读者思考，查询语句的结果是什么？
hive (default)> select 
    order_id, 
    order_date, 
    amount, 
    sum(amount) over (order by order_date rows between unbounded preceding and current row) total_amount
from order_info;
order_info表数据
order_id	user_id	order_date	amount
1	1001	2022-01-01	10
2	1001	2022-01-03	20
3	1001	2022-01-03	10
4	1002	2022-01-04	30
5	1002	2022-01-05	40
6	1002	2022-01-05	20
查询结果如下所示。
案例查询结果
order_id	user_id	order_date	amount	total_amount
1	1001	2022-01-01	10	10
2	1001	2022-01-03	20	40
3	1001	2022-01-03	10	40
4	1002	2022-01-04	30	70
5	1002	2022-01-05	40	130
6	1002	2022-01-05	20	130
（2）基于值
基于值的窗口范围定义语法如下，与基于行的窗口范围定义不同的是，使用的是range关键字，而不是rows。
sum(amount) over(order by <column> range between <start> and <end>)
其中start用于定义窗口范围的起点，end用于定义窗口范围的终点，order by指定的列作为划分窗口范围的依据。
如下所示，是基于值的窗口范围如何定义起点和终点的详细语法。between关键字后可选的是窗口起点，and关键字后可选的是窗口终点。
图中的[num]为相对当前值的偏移量。例如：若某行的窗口定义为order by A range between 1 preceding and 1 following，且该行中A列的值为10，则其窗口范围是A列的值位于9（10-1）到11（10+1）之间的所有行。因此，若窗口范围的起点或终点，使用[num] preceding或者[num] following进行定义，那么order by后就只能指定一个列，并且该列的类型只能为数字类型。
与基于行的窗口范围定义相同的是，窗口起点不能超过终点，例如当起点是current row时，终点不能是[num] preceding。使用不同深浅的底色对可以匹配使用的起点终点进行了区分。
 	以下案例使用基于值的方式定义窗口范围，请读者思考，查询语句的结果是什么？
hive (default)> select 
    order_id, 
    order_date, 
    amount, 
    sum(amount) over (order by order_date range between unbounded preceding and current row) total_amount
from order_info; 
上文提到过，我们将order by指定的列作为划分窗口范围的依据。第一行的窗口范围划分依据是-∞<order_date≤2022-01-01，也就是只包含第一行，所以sum(amount)的结果为10。第二行的窗口范围划分依据是-∞<order_date≤2022-01-03，也就是包含前三行，所以sum(amount)的结果为40。第三行的窗口范围划分依据是-∞<order_date≤2022-01-03，与第二行的窗口范围相同，sum(amount)的结果也为40。以此类推。
基于值的窗口范围案例结果
order_id	user_id	order_date	amount	total_amount
1	1001	2022-01-01	10	10
2	1001	2022-01-03	20	40
3	1001	2022-01-03	10	40
4	1002	2022-01-04	30	70
5	1002	2022-01-05	40	130
6	1002	2022-01-05	20	130

（3）分区
定义窗口范围时，我们还可以使用partition by关键字指定分区列，将每个分区单独划分为窗口。如下所示，是一个简单的partition by关键字与order by关键字结合使用的示例。
 
如下查询语句所示，是一个partition by语句的完整使用示例，对order_info表按照user_id列分区，并按照order_date列排序，统计从起始行至当前行的所有amount列的加和。
hive (default)> select 
    order_id, 
    order_date, 
    amount, 
    sum(amount) over (partition by user_id order by order_date rows between unbounded preceding and current row) total_amount
from order_info; 
请读者思考，上述查询语句的结果是什么。
上述查询语句的查询过程如下所示。
 
查询结果如下所示。
包含分区的窗口函数案例查询结果
order_id	user_id	order_date	amount	total_amount
1	1001	2022-01-01	10	10
2	1001	2022-01-03	20	30
3	1001	2022-01-03	10	40
4	1002	2022-01-04	30	30
5	1002	2022-01-05	40	70
6	1002	2022-01-05	20	90
（4）缺省
窗口范围的划分语法讲解基本结束，我们已经讲解过的关键字（也就是over后可以使用的窗口划分语句）包括partition by、order by、(rows|range) between … and …。实际上over后括号中包含的三部分内容partition by、order by和(rows|range) between … and … 均可省略不写。
①partition by省略不写，表示不分区。在不进行分区的情况下，将会把整张表的全部内容作为窗口进行划分。
②order by 省略不写，表示不排序。
③(rows|range) between … and … 省略不写，则使用其默认值，默认值分以下两种情况。
若over()中包含order by，则默认值为range between unbounded preceding and current row。
若over()中不包含order by，则默认值为rows between unbounded preceding and unbounded following。
7.2.2 常用窗口函数
按照功能，常用窗口可划分为如下几类：聚合函数、跨行取值函数、排名函数。
1）聚合函数
max：最大值。
min：最小值。
sum：求和。
avg：平均值。
count：计数。
2）跨行取值函数
（1）lead和lag
①lead函数：用于获取窗口内当前行往下第n行的值。
语法：lead(col, n, default)
说明：第一个参数为列名，第二参数为往下第几行，第三个参数为当往下第几行遇到null值时，所取的默认值。第二个参数可选，其默认值为1，第三个参数的默认值为null。
②lag函数：用于获取窗口内当前行往上第n行的值。
语法：lag(col, n, default)
说明：第一个参数为列名，第二参数为往上第几行，第三个参数为当往上第几行遇到null值时，所取的默认值。第二个参数可选，其默认值为1，第三个参数的默认值为null。例如：当当前行为第一行时，往上没有任何数据，所以这时候就需要赋予第三个参数给出的默认值。
如下查询语句，展示了lead和lag函数的使用，分别获取当前order_date列的上一个日期和下一个日期。
hive (default)> select 
    order_id, 
    user_id,
    order_date, 
    amount, 
    lag(order_date,1, '1970-01-01') over (partition by user_id order by order_date) last_date,
    lead(order_date,1, '9999-12-31') over (partition by user_id order by order_date) next_date
from order_info;
查询过程如下
 
注意：
lag和lead函数不支持使用rows between和range between的自定义窗口。
 
注：lag和lead函数不支持自定义窗口。
（2）first_value和last_value
first_value函数：取分组内排序后，截止到当前行的第一个值。
语法：first_value (col, boolean)
说明：第一个参数为列名，第二个参数说明是否跳过null值，可不写。
last_value函数：。
语法：last_value (col, boolean)
说明：第一个参数为列名，第二个参数说明是否跳过null值，可不写。
如下查询语句，展示了first_value和last_value函数的使用。
hive (default)> select 
    order_id, 
    user_id,
    order_date, 
    amount, 
    first_value(order_date,false) over (partition by user_id order by order_date) first_date,
    last_value(order_date,false) over (partition by user_id order by order_date) last_date
from order_info;
查询过程如下。
 
 
3）排名函数
排名函数是窗口函数的使用中非常频繁的一种，主要包括rank 、dense_rank、row_number，以上三个函数不支持自定义函数的使用。排名函数的使用方式如下。
rank()/dense_rank()/row_number() over (partition by col1 order by col2)
排名函数会对窗口范围内的数据按照order by后的列进行排名。如上所示对排名函数的使用，会先根据col1列对数据进行分区，在分区内根据col2列进行升序或降序排序，然后生成一列新的排序序号列，生成序号的规则区别如下。
	rank函数，生成的序号从1开始，若col2列的值相同，则排序序号相同，且会在序号中留下空位。
	dense_rank函数，生成的序号从1开始，若col2列的值相同，则排序序号相同，单不会在序号中留下空位。
	row_number函数，生成的序号从1开始，按照顺序生成序号，不会存在相同的序号。
如下查询语句，展示了三种排名函数的使用。
hive (default)> select 
    stu_id, 
    course,
    score,
    rank() over(partition by course order by score desc) rk,
    dense_rank() over(partition by course order by score desc) dense_rk,
    row_number() over(partition by course order by score desc) rn
from score_info;
score_info表的数据如下所示。
score_info表数据
stu_id	course	score
1	语文	99
2	语文	98
3	语文	95
4	数学	100
5	数学	100
6	数学	99
查询结果如下所示，通过查询结果，体会三种排名函数的不同。
stu_id	course	score	rank	dense_rank	row_number
1	语文	99	1	1	1
2	语文	98	2	2	2
3	语文	95	3	3	3
4	数学	100	1	1	1
5	数学	100	1	1	2
6	数学	99	3	2	3
 	注：rank 、dense_rank、row_number不支持自定义窗口。
7.2.3 案例演示
1）数据准备
（1）表结构
order_id	user_id	user_name	order_date	order_amount
1	1001	小元	2022-01-01	10
2	1002	小海	2022-01-02	15
3	1001	小元	2022-02-03	23
4	1002	小海	2022-01-04	29
5	1001	小元	2022-01-05	46
（2）建表语句
create table order_info
(
    order_id     string, --订单id
    user_id      string, -- 用户id
    user_name    string, -- 用户姓名
    order_date   string, -- 下单日期
    order_amount int     -- 订单金额
);
（3）装载语句
insert overwrite table order_info
values ('1', '1001', '小元', '2022-01-01', '10'),
       ('2', '1002', '小海', '2022-01-02', '15'),
       ('3', '1001', '小元', '2022-02-03', '23'),
       ('4', '1002', '小海', '2022-01-04', '29'),
       ('5', '1001', '小元', '2022-01-05', '46'),
       ('6', '1001', '小元', '2022-04-06', '42'),
       ('7', '1002', '小海', '2022-01-07', '50'),
       ('8', '1001', '小元', '2022-01-08', '50'),
       ('9', '1003', '小辉', '2022-04-08', '62'),
       ('10', '1003', '小辉', '2022-04-09', '62'),
       ('11', '1004', '小猛', '2022-05-10', '12'),
       ('12', '1003', '小辉', '2022-04-11', '75'),
       ('13', '1004', '小猛', '2022-06-12', '80'),
       ('14', '1003', '小辉', '2022-04-13', '94');
2）需求
（1）统计每个用户截至每次下单的累积下单总额
①期望结果
order_id	user_id	user_name	order_date	order_amount	sum_so_far
1	1001	小元	2022-01-01	10	10
5	1001	小元	2022-01-05	46	56
8	1001	小元	2022-01-08	50	106
3	1001	小元	2022-02-03	23	129
6	1001	小元	2022-04-06	42	171
2	1002	小海	2022-01-02	15	15
4	1002	小海	2022-01-04	29	44
7	1002	小海	2022-01-07	50	94
9	1003	小辉	2022-04-08	62	62
10	1003	小辉	2022-04-09	62	124
12	1003	小辉	2022-04-11	75	199
14	1003	小辉	2022-04-13	94	293
11	1004	小猛	2022-05-10	12	12
13	1004	小猛	2022-06-12	80	92
②思路分析
需求要求统计“每个用户”的累积下单金额，所以很显然，在使用窗口函数时，应该使用partition by user_id来划定排序的对象范围。
“截至每次下单”的累积下单金额的要求，很容易想到，使用order by order_date来指定按照哪一个列进行排序，按照下单的先后顺序排序，则order_date列使用默认升序，不需要指定asc/desc关键字。
 
截至当前的累积下单总额，则说明需要统计每个用户的订单在按照下单日期升序排序后，从第一笔订单到当前订单的所有订单的加和。可以得出两个结论，一是使用聚合函数sum对order_amount列求和，二是窗口的范围划分按照行，且起点是unbounded preceding，终点是current row。
综合以上分析，可以得出完整的窗口函数如下所示。
sum(order_amount) over(partition by user_id order by order_date rows between unbounded preceding and current row)
 	③需求实现
select
    order_id,
    user_id,
    user_name,
    order_date,
    order_amount,
    sum(order_amount) over(partition by user_id order by order_date rows between unbounded preceding and current row) sum_so_far
from order_info;
（2）统计每个用户截至每次下单的当月累积下单总额
①期望结果
order_id	user_id	user_name	order_date	order_amount	sum_so_far
1	1001	小元	2022-01-01	10	10
5	1001	小元	2022-01-05	46	56
8	1001	小元	2022-01-08	50	106
3	1001	小元	2022-02-03	23	23
6	1001	小元	2022-04-06	42	42
2	1002	小海	2022-01-02	15	15
4	1002	小海	2022-01-04	29	44
7	1002	小海	2022-01-07	50	94
9	1003	小辉	2022-04-08	62	62
10	1003	小辉	2022-04-09	62	124
12	1003	小辉	2022-04-11	75	199
14	1003	小辉	2022-04-13	94	293
11	1004	小猛	2022-05-10	12	12
13	1004	小猛	2022-06-12	80	80
②思路分析
对本案例的需求进行分析，可以发现，本案例需求可以调整为统计某个用户每个月截至每次下单的累积下单总额。观察到与案例一的需求描述有相似的地方，修改partition by后的分区划分列，将“每个用户”修改成“每个用户每个月”，所以最终的分区划分为partition by user_id, substring(order_date,1,7)。
 
 	③需求实现
select
    order_id,
    user_id,
    user_name,
    order_date,
    order_amount,
    sum(order_amount) over(partition by user_id,substring(order_date,1,7) order by order_date rows between unbounded preceding and current row) sum_so_far
from order_info;
（3）统计每个用户每次下单距离上次下单相隔的天数（首次下单按0天算）
①期望结果
order_id	user_id	user_name	order_date	order_amount	diff
1	1001	小元	2022-01-01	10	0
5	1001	小元	2022-01-05	46	4
8	1001	小元	2022-01-08	50	3
3	1001	小元	2022-02-03	23	26
6	1001	小元	2022-04-06	42	62
2	1002	小海	2022-01-02	15	0
4	1002	小海	2022-01-04	29	2
7	1002	小海	2022-01-07	50	3
9	1003	小辉	2022-04-08	62	0
10	1003	小辉	2022-04-09	62	1
12	1003	小辉	2022-04-11	75	2
14	1003	小辉	2022-04-13	94	2
11	1004	小猛	2022-05-10	12	0
13	1004	小猛	2022-06-12	80	33
②思路分析
统计每个用户每次下单距离上次下单相隔的天数（首次下单按0天算）。
分析案例需求，可以发现本题的关键是统计每个用户的上次下单时间。
第一步：子查询t1，获取每个用户的上次下单时间。
使用partition by user_id将窗口范围划定为每个用户的数据。使用order by order_date，将下单时间升序排序，使用lag函数，获取每行数据的上一行的order_date，也就是用户的上一次下单时间。lag函数的三个参数分别是order_date列，1表示获取上一行数据，null是当没有上一行数据时给出的默认值。
 
第二步：得到最终结果。
窗口函数计算得到的结果列不能直接用于当次查询，也就是说，我们计算本次下单时间与上次下单时间的差值时，需要在子查询t1的基础上，执行第二层查询。
容易想到，计算两个日期的差值，可以使用datediff函数。使用datediff函数计算日期差值时，若其中一个日期为null，则结果为null。所以需要使用nvl函数对datediff的计算结果做处理，当datediff函数的计算结果为null时，赋默认值0。
 	③需求实现
select
    order_id,
    user_id,
    user_name,
    order_date,
    order_amount,
    nvl(datediff(order_date,last_order_date),0) diff
from
(
    select
        order_id,
        user_id,
        user_name,
        order_date,
        order_amount,
        lag(order_date,1,null) over(partition by user_id order by order_date) last_order_date
    from order_info
)t1
（4）查询所有下单记录以及每个用户的每个下单记录所在月份的首/末次下单日期
①期望结果
order_id	user_id	user_name	order_date	order_amount	first_date	last_date
1	1001	小元	2022-01-01	10	2022-01-01	2022-01-08
5	1001	小元	2022-01-05	46	2022-01-01	2022-01-08
8	1001	小元	2022-01-08	50	2022-01-01	2022-01-08
3	1001	小元	2022-02-03	23	2022-02-03	2022-02-03
6	1001	小元	2022-04-06	42	2022-04-06	2022-04-06
2	1002	小海	2022-01-02	15	2022-01-02	2022-01-07
4	1002	小海	2022-01-04	29	2022-01-02	2022-01-07
7	1002	小海	2022-01-07	50	2022-01-02	2022-01-07
9	1003	小辉	2022-04-08	62	2022-04-08	2022-04-13
10	1003	小辉	2022-04-09	62	2022-04-08	2022-04-13
12	1003	小辉	2022-04-11	75	2022-04-08	2022-04-13
14	1003	小辉	2022-04-13	94	2022-04-08	2022-04-13
11	1004	小猛	2022-05-10	12	2022-05-10	2022-05-10
13	1004	小猛	2022-06-12	80	2022-06-12	2022-06-12
②思路分析
对案例需求分析后发现，主要是统计每个用户每个月的首次下单日期和末次下单日期。容易想到，可以使用典型的窗口函数first_value和last_value。partition by关键字配合user_id和切割得到的下单月份substring(order_date,1,7)。order by关键字配合order_date列，将每个用户每个月的订单按照下单日期升序排序。
更关键之处在于窗口范围的划分，统计首次下单日期时，使用first_value函数，可以省略窗口范围，只使用partition by和order by，此时默认值是range between unbounded preceding and current row，对于统计首次下单日期没有影响。
统计末次下单日期时，使用last_value函数，纳入统计范围的应该是每个用户每个月的所有订单，所以窗口范围是rows between unbounded preceding and unbounded following。
 	③需求实现
select
    order_id,
    user_id,
    user_name,
    order_date,
    order_amount,
    first_value(order_date) over(partition by user_id,substring(order_date,1,7) order by order_date) first_date,
    last_value(order_date) over(partition by user_id,substring(order_date,1,7) order by order_date rows between unbounded preceding and unbounded following) last_date
from order_info;
（5）为每个用户的所有下单记录按照订单金额进行排名
①期望结果
order_id	user_id	user_name	order_date	order_amount	rk	drk	rn
8	1001	小元	2022-01-08	50	1	1	1
5	1001	小元	2022-01-05	46	2	2	2
6	1001	小元	2022-04-06	42	3	3	3
3	1001	小元	2022-02-03	23	4	4	4
1	1001	小元	2022-01-01	10	5	5	5
7	1002	小海	2022-01-07	50	1	1	1
4	1002	小海	2022-01-04	29	2	2	2
2	1002	小海	2022-01-02	15	3	3	3
14	1003	小辉	2022-04-13	94	1	1	1
12	1003	小辉	2022-04-11	75	2	2	2
9	1003	小辉	2022-04-08	62	3	3	3
10	1003	小辉	2022-04-09	62	3	3	4
13	1004	小猛	2022-06-12	80	1	1	1
11	1004	小猛	2022-05-10	12	2	2	2
②思路分析
本案例主要考察三个排名函数的使用。partition by user_id order_by order_amount结合使用，完成对每个用户的所有下单记录的下单金额的排序，分别调用rank、dense_rank和row_number函数，可以获得不同的排名效果。
 
③需求实现
select
    order_id,
    user_id,
    user_name,
    order_date,
    order_amount,
    rank() over(partition by user_id order by order_amount desc) rk,
    dense_rank() over(partition by user_id order by order_amount desc) drk,
    row_number() over(partition by user_id order by order_amount desc) rn
from order_info;
7.3 自定义函数
1）Hive自带了一些函数，比如：max/min等，但是数量有限，自己可以通过自定义UDF来方便的扩展。
2）当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数（UDF：user-defined function）。
3）根据用户自定义函数类别分为以下三种：
（1）UDF（User-Defined-Function）
	一进一出。
（2）UDAF（User-Defined Aggregation Function）
	用户自定义聚合函数，多进一出。
	类似于：count/max/min
（3）UDTF（User-Defined Table-Generating Functions）
	用户自定义表生成函数，一进多出。
	如lateral view explode()
以上用户自定义函数的分类标准可以扩大到Hive的所有函数，包括内置函数和自定义函数，这称为UDF分类标准扩大化。例如，max、min函数也可以称为UDAF，explode称为UDTF。不难理解，无论内置函数还是自定义函数，一定满足以上任一个输入输出值的要求。
4）官方文档地址
https://cwiki.apache.org/confluence/display/Hive/HivePlugins
5）编程步骤
（1）继承Hive提供的类
org.apache.hadoop.hive.ql.udf.generic.GenericUDF
org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;
（2）实现类中的抽象方法
（3）在hive的命令行窗口创建函数
添加jar。
add jar linux_jar_path
创建function。
create [temporary] function [dbname.]function_name AS class_name;
（4）在hive的命令行窗口删除函数
drop [temporary] function [if exists] [dbname.]function_name;
7.4 自定义UDF函数
0）需求
自定义一个UDF实现计算给定基本数据类型的长度，例如：
hive(default)> select my_len("abcd");
4
1）创建一个Maven工程Hive
2）导入依赖
<dependencies>
	<dependency>
		<groupId>org.apache.hive</groupId>
		<artifactId>hive-exec</artifactId>
		<version>3.1.3</version>
	</dependency>
</dependencies>
3）创建一个类
package com.atguigu.hive.udf;

import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;

/**
 * 我们需计算一个要给定基本数据类型的长度
 */
public class MyUDF extends GenericUDF {
    /**
     * 判断传进来的参数的类型和长度
     * 约定返回的数据类型
     */
    @Override
    public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {

        if (arguments.length !=1) {
            throw  new UDFArgumentLengthException("please give me  only one arg");
        }

        if (!arguments[0].getCategory().equals(ObjectInspector.Category.PRIMITIVE)){
            throw  new UDFArgumentTypeException(1, "i need primitive type arg");
        }

        return PrimitiveObjectInspectorFactory.javaIntObjectInspector;
    }

    /**
     * 解决具体逻辑的
     */
    @Override
    public Object evaluate(DeferredObject[] arguments) throws HiveException {

        Object o = arguments[0].get();
        if(o==null){
            return 0;
        }

        return o.toString().length();
    }

    @Override
    // 用于获取解释的字符串
    public String getDisplayString(String[] children) {
        return "";
    }
}
4）创建临时函数
（1）打成jar包上传到服务器/opt/module/hive/datas/myudf.jar
（2）将jar包添加到hive的classpath，临时生效
hive (default)> add jar /opt/module/hive/datas/myudf.jar;
（3）创建临时函数与开发好的java class关联
hive (default)> 
create temporary function my_len 
as "com.atguigu.hive.udf.MyUDF";
（4）即可在hql中使用自定义的临时函数
hive (default)> 
select 
    ename,
    my_len(ename) ename_len 
from emp;
	（5）删除临时函数
hive (default)> drop temporary function my_len;
	注意：临时函数只跟会话有关系，跟库没有关系。只要创建临时函数的会话不断，在当前会话下，任意一个库都可以使用，其他会话全都不能使用。
	创建永久函数
（1）创建永久函数
注意：因为add jar本身也是临时生效，所以在创建永久函数的时候，需要制定路径（并且因为元数据的原因，这个路径还得是HDFS上的路径）。
hive (default)> 
create function my_len2 
as "com.atguigu.hive.udf.MyUDF" 
using jar "hdfs://hadoop102:8020/udf/myudf.jar";
（2）即可在hql中使用自定义的永久函数 
hive (default)> 
select 
    ename,
    my_len2(ename) ename_len 
from emp;
（3）删除永久函数 
hive (default)> drop function my_len2;
注意：永久函数跟会话没有关系，创建函数的会话断了以后，其他会话也可以使用。
永久函数创建的时候，在函数名之前需要自己加上库名，如果不指定库名的话，会默认把当前库的库名给加上。
永久函数使用的时候，需要在指定的库里面操作，或者在其他库里面使用的话加上，库名.函数名。
7.5 常用函数大全（附录）
 
7.6 综合案例练习——高级函数（6道题目）
           
7.7 综合案例练习——大厂真题（4道题目）
 s
第8章 分区表和分桶表
8.1 分区表
通过使用partitioned by子句可以创建分区表，partitioned by后面是分区字段，一个表可以有一个或多个分区字段，Hive可以为分区字段的每个不同的字段组合创建一个单独的数据目录。Hive通过分区，可以将一张大表的数据根据业务需要分散存储到多个目录中，当用户通过where子句选择要查询的分区后，就不会查询其他分区的数据，大大提高了查询效率，分散了Hive的查询压力。
8.1.1 分区表基本语法
1）创建分区表
hive (default)> 
create table dept_partition(
    deptno int,    --部门编号
    dname  string, --部门名称
    loc    string  --部门位置
)
partitioned by (day string)
row format delimited fields terminated by '\t';
2）分区表写数据
（1）load
○1数据准备
在/opt/module/hive/datas/路径上创建文件dept_20220401.log，并输入如下内容。
[atguigu@hadoop102 datas]$ vim dept_20220401.log

10	行政部	1700
20	财务部	1800
○2装载语句
hive (default)> 
load data local inpath '/opt/module/hive/datas/dept_20220401.log' 
into table dept_partition 
partition(day='20220401');
（2）insert
将day='20220401'分区的数据插入到day='20220402'分区，可执行如下装载语句。
hive (default)> 
insert overwrite table dept_partition partition (day = '20220402')
select deptno, dname, loc
from dept_partition
where day = '2020-04-01';
3）分区表读数据
查询分区表数据时，可以将分区字段看作表的伪列，可像使用其他字段一样使用分区字段。
select deptno, dname, loc ,day
from dept_partition
where day = '2020-04-01';
4）分区表基本操作
（1）查看所有分区信息
hive> show partitions dept_partition;
（2）增加分区
①创建单个分区
hive (default)> 
alter table dept_partition 
add partition(day='20220403');
②同时创建多个分区（分区之间不能有逗号）
hive (default)> 
alter table dept_partition 
add partition(day='20220404') partition(day='20220405');
（3）删除分区
①删除单个分区
hive (default)> 
alter table dept_partition 
drop partition (day='20220403');
②同时删除多个分区（分区之间必须有逗号）
hive (default)> 
alter table dept_partition 
drop partition (day='20220404'), partition(day='20220405');
（4）修复分区
Hive将分区表的所有分区信息都保存在了元数据中，只有元数据与HDFS上的分区路径一致时，分区表才能正常读写数据。若用户手动创建/删除分区路径，Hive都是感知不到的，这样就会导致Hive的元数据和HDFS的分区路径不一致。再比如，若分区表为外部表，用户执行drop partition命令后，分区元数据会被删除，而HDFS的分区路径不会被删除，同样会导致Hive的元数据和HDFS的分区路径不一致。
若出现元数据和HDFS路径不一致的情况，可通过如下几种手段进行修复。
①add partition
若手动创建HDFS的分区路径，Hive无法识别，可通过add partition命令增加分区元数据信息，从而使元数据和分区路径保持一致。
②drop partition
若手动删除HDFS的分区路径，Hive无法识别，可通过drop partition命令删除分区元数据信息，从而使元数据和分区路径保持一致。
③msck
若分区元数据和HDFS的分区路径不一致，还可使用msck命令进行修复，以下是该命令的用法说明。
hive (default)> 
msck repair table table_name [add/drop/sync partitions];
说明：
msck repair table table_name add partitions：该命令会增加HDFS路径存在但元数据缺失的分区信息。
msck repair table table_name drop partitions：该命令会删除HDFS路径已经删除但元数据仍然存在的分区信息。
msck repair table table_name sync partitions：该命令会同步HDFS路径和元数据分区信息，相当于同时执行上述的两个命令。
msck repair table table_name：等价于msck repair table table_name add partitions命令。
8.1.2 二级分区表
思考：如果一天内的日志数据量也很大，如何再将数据拆分?答案是二级分区表，例如可以在按天分区的基础上，再对每天的数据按小时进行分区。
1）二级分区表建表语句
hive (default)>
create table dept_partition2(
    deptno int,    -- 部门编号
    dname string, -- 部门名称
    loc string     -- 部门位置
)
partitioned by (day string, hour string)
row format delimited fields terminated by '\t';
2）数据装载语句
hive (default)> 
load data local inpath '/opt/module/hive/datas/dept_20220401.log' 
into table dept_partition2 
partition(day='20220401', hour='12');
3）查询分区数据
hive (default)> 
select 
    * 
from dept_partition2 
where day='20220401' and hour='12';
8.1.3 动态分区
动态分区是指向分区表insert数据时，被写往的分区不由用户指定，而是由每行数据的最后一个字段的值来动态的决定。使用动态分区，可只用一个insert语句将数据写入多个分区。
1）动态分区相关参数
（1）动态分区功能总开关（默认true，开启）
set hive.exec.dynamic.partition=true
（2）严格模式和非严格模式
动态分区的模式，默认strict（严格模式），要求必须指定至少一个分区为静态分区，nonstrict（非严格模式）允许所有的分区字段都使用动态分区。
set hive.exec.dynamic.partition.mode=nonstrict
（3）一条insert语句可同时创建的最大的分区个数，默认为1000。
set hive.exec.max.dynamic.partitions=1000
（4）单个Mapper或者Reducer可同时创建的最大的分区个数，默认为100。
set hive.exec.max.dynamic.partitions.pernode=100
（5）一条insert语句可以创建的最大的文件个数，默认100000。
hive.exec.max.created.files=100000
（6）当查询结果为空时且进行动态分区时，是否抛出异常，默认false。
hive.error.on.empty.partition=false
2）案例实操
需求：将dept表中的数据按照地区（loc字段），插入到目标表dept_partition_dynamic的相应分区中。
（1）创建目标分区表
hive (default)> 
create table dept_partition_dynamic(
    id int, 
    name string
) 
partitioned by (loc int) 
row format delimited fields terminated by '\t';
（2）设置动态分区
set hive.exec.dynamic.partition.mode = nonstrict;
hive (default)> 
insert into table dept_partition_dynamic 
partition(loc) 
select 
    deptno, 
    dname, 
    loc 
from dept;
（3）查看目标分区表的分区情况
hive (default)> show partitions dept_partition_dynamic;
8.2 分桶表
分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区。对于一张表或者分区，Hive 可以进一步组织成桶，也就是更为细粒度的数据范围划分，分区针对的是数据的存储路径，分桶针对的是数据文件。
分桶表的基本原理是，首先为每行数据计算一个指定字段的数据的hash值，然后模以一个指定的分桶数，最后将取模运算结果相同的行，写入同一个文件中，这个文件就称为一个分桶（bucket）。
8.2.1 分桶表基本语法
1）建表语句
hive (default)> 
create table stu_buck(
    id int, 
    name string
)
clustered by(id) 
into 4 buckets
row format delimited fields terminated by '\t';
2）数据装载
（1）数据准备
在/opt/module/hive/datas/路径上创建student.txt文件，并输入如下内容。
1001	student1
1002	student2
1003	student3
1004	student4
1005	student5
1006	student6
1007	student7
1008	student8
1009	student9
1010	student10
1011	student11
1012	student12
1013	student13
1014	student14
1015	student15
1016	student16
（2）导入数据到分桶表中
说明：Hive新版本load数据可以直接跑MapReduce，老版的Hive需要将数据传到一张表里，再通过查询的方式导入到分桶表里面。
hive (default)> 
load data local inpath '/opt/module/hive/datas/student.txt' 
into table stu_buck;
（3）查看创建的分桶表中是否分成4个桶
 
（4）观察每个分桶中的数据
8.2.2 分桶排序表
Hive的分桶排序表是一种优化技术，用于提高大数据存储和查询的效率。它将数据表按照指定的列进行分桶（bucket），每个桶内的数据再按照指定的列进行排序，这样就可以在查询时快速定位到需要的数据，减少数据扫描的时间。
1）建表语句
hive (default)> 
create table stu_buck_sort(
    id int, 
    name string
)
clustered by(id) sorted by(id)
into 4 buckets
row format delimited fields terminated by '\t';
其中，clustered by关键字指定按照哪个列进行分桶，sorted by关键字指定在每个桶内按照哪个列进行排序。
使用分桶排序表的主要优点是可以提高查询效率，特别是在大数据量的情况下。相比于无序表，分桶排序表在查询时可以跳过不需要的数据，减少数据扫描的时间。
需要注意的是，分桶排序表的创建需要根据实际情况选择合适的分桶和排序列，不当的选择可能会导致查询效率降低甚至无法使用。另外，分桶排序表的维护需要消耗一定的资源，包括磁盘空间和计算资源，因此需要在性能和成本之间做出权衡。
2）数据装载
（1）导入数据到分桶表中
hive (default)> 
load data local inpath '/opt/module/hive/datas/student.txt' 
into table stu_buck_sort;
（2）查看创建的分桶表中是否分成4个桶
 
（3）观察每个分桶中的数据
第9章 文件格式和压缩
9.1文件格式
为Hive表中的数据选择一个合适的文件格式，对提高查询性能的提高是十分有益的。Hive表数据的存储格式，可以选择text file、orc、parquet、sequence file等。
9.1.1 Text File
文本文件是Hive默认使用的文件格式，文本文件中的一行内容，就对应Hive表中的一行记录。
可通过以下建表语句指定文件格式为文本文件：
create table textfile_table
(column_specs)
stored as textfile;
9.1.2 ORC
1）文件格式
ORC（Optimized Row Columnar）file format是Hive 0.11版里引入的一种列式存储的文件格式。ORC文件能够提高Hive读写数据和处理数据的性能。
与列式存储相对的是行式存储，下图是两者的对比：
 
如图所示左边为逻辑表，右边第一个为行式存储，第二个为列式存储。
（1）行存储的特点
查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。
（2）列存储的特点
因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。
前文提到的text file和sequence file都是基于行存储的，orc和parquet是基于列式存储的。
orc文件的具体结构如下图所示：
 
每个Orc文件由Header、Body和Tail三部分组成。
其中Header内容为ORC，用于表示文件类型。
Body由1个或多个stripe组成，每个stripe一般为HDFS的块大小，每一个stripe包含多条记录，这些记录按照列进行独立存储，每个stripe里有三部分组成，分别是Index Data，Row Data，Stripe Footer。
Index Data：一个轻量级的index，默认是为各列每隔1W行做一个索引。每个索引会记录第n万行的位置，和最近一万行的最大值和最小值等信息。
Row Data：存的是具体的数据，按列进行存储，并对每个列进行编码，分成多个Stream来存储。
Stripe Footer：存放的是各个Stream的位置以及各column的编码信息。
Tail由File Footer和PostScript组成。File Footer中保存了各Stripe的其实位置、索引长度、数据长度等信息，各Column的统计信息等；PostScript记录了整个文件的压缩类型以及File Footer的长度信息等。
在读取ORC文件时，会先从最后一个字节读取PostScript长度，进而读取到PostScript，从里面解析到File Footer长度，进而读取FileFooter，从中解析到各个Stripe信息，再读各个Stripe，即从后往前读。
3）建表语句
create table orc_table
(column_specs)
stored as orc
tblproperties (property_name=property_value, ...);
ORC文件格式支持的参数如下：
参数	默认值	说明
orc.compress	ZLIB	压缩格式，可选项：NONE、ZLIB,、SNAPPY
orc.compress.size	262,144	每个压缩块的大小（ORC文件是分块压缩的）
orc.stripe.size	67,108,864	每个stripe的大小
orc.row.index.stride	10,000	索引步长（每隔多少行数据建一条索引）
9.1.3 Parquet
Parquet文件是Hadoop生态中的一个通用的文件格式，它也是一个列式存储的文件格式。
Parquet文件的格式如下图所示：
 
上图展示了一个Parquet文件的基本结构，文件的首尾都是该文件的Magic Code，用于校验它是否是一个Parquet文件。
首尾中间由若干个Row Group和一个Footer（File Meta Data）组成。
每个Row Group包含多个Column Chunk，每个Column Chunk包含多个Page。以下是Row Group、Column Chunk和Page三个概念的说明：
行组（Row Group）：一个行组对应逻辑表中的若干行。 
列块（Column Chunk）：一个行组中的一列保存在一个列块中。 
页（Page）：一个列块的数据会划分为若干个页。 
Footer（File Meta Data）中存储了每个行组（Row Group）中的每个列快（Column Chunk）的元数据信息，元数据信息包含了该列的数据类型、该列的编码方式、该类的Data Page位置等信息。
3）建表语句
Create table parquet_table
(column_specs)
stored as parquet
tblproperties (property_name=property_value, ...);
支持的参数如下：
参数	默认值	说明
parquet.compression	uncompressed	压缩格式，可选项：uncompressed，snappy，gzip，lzo，lz4
parquet.block.size	134217728 	行组大小，通常与HDFS块大小保持一致
parquet.page.size	1048576	页大小
9.1.4 企业中选择
在数据分析的场景下，使用列式存储格式（orc和parquet）的查询性能和存储效率都要优于默认的文本文件格式（textfile），其中orc的性能略微优于parquet。故优先选用orc格式。
参考资料：
	以下是一个使用TPC-DS数据集（TPC-DS是大数据领域最为知名的Benchmark标准），对textfile、orc、parquet三种文件格式的压力测试报告，可作为参考https://codeantenna.com/a/QNO99FqYme
	以下是Hortonworks公司开发的业界最常用的TPC-DS测试工具：https://github.com/hortonworks/hive-testbench 
9.2 压缩
在Hive表中和计算过程中，保持数据的压缩，对磁盘空间的有效利用和提高查询性能都是十分有益的。
9.2.1 Hadoop压缩概述
压缩格式	算法	文件扩展名	是否可切分
DEFLATE	DEFLATE	.deflate	否
Gzip	DEFLATE	.gz	否
bzip2	bzip2	.bz2	是
LZO	LZO	.lzo	是
Snappy	Snappy	.snappy	否
为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示：
Hadoop查看支持压缩的方式hadoop checknative。
Hadoop在driver端设置压缩。
压缩格式	对应的编码/解码器
DEFLATE	org.apache.hadoop.io.compress.DefaultCodec
gzip	org.apache.hadoop.io.compress.GzipCodec
bzip2	org.apache.hadoop.io.compress.BZip2Codec
LZO	com.hadoop.compression.lzo.LzopCodec
Snappy	org.apache.hadoop.io.compress.SnappyCodec
压缩性能的比较：
压缩算法	原始文件大小	压缩文件大小	压缩速度	解压速度
gzip	8.3GB	1.8GB	17.5MB/s	58MB/s
bzip2	8.3GB	1.1GB	2.4MB/s	9.5MB/s
LZO	8.3GB	2.9GB	49.3MB/s	74.6MB/s
http://google.github.io/snappy/
On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more.
9.2.2 Hive表数据进行压缩
在Hive中，不同文件类型的表，声明数据压缩的方式是不同的。
1）TextFile
若一张表的文件类型为TextFile，若需要对该表中的数据进行压缩，多数情况下，无需在建表语句做出声明。直接将压缩后的文件导入到该表即可，Hive在查询表中数据时，可自动识别其压缩格式，进行解压。
需要注意的是，在执行往表中导入数据的SQL语句时，用户需设置以下参数，来保证写入表中的数据是被压缩的。
--SQL语句的最终输出结果是否压缩
set hive.exec.compress.output=true;
--输出结果的压缩格式（以下示例为snappy）
set mapreduce.output.fileoutputformat.compress.codec =org.apache.hadoop.io.compress.SnappyCodec;
2）ORC
若一张表的文件类型为ORC，若需要对该表数据进行压缩，需在建表语句中声明压缩格式如下：
create table orc_table
(column_specs)
stored as orc
tblproperties ("orc.compress"="snappy");
3）Parquet
若一张表的文件类型为Parquet，若需要对该表数据进行压缩，需在建表语句中声明压缩格式如下：
create table orc_table
(column_specs)
stored as parquet
tblproperties ("parquet.compression"="snappy");
9.2.3 计算过程中使用压缩
1）单个MR的中间结果进行压缩
单个MR的中间结果是指Mapper输出的数据，对其进行压缩可降低shuffle阶段的网络IO，可通过以下参数进行配置：
--开启MapReduce中间数据压缩功能
set mapreduce.map.output.compress=true;
--设置MapReduce中间数据数据的压缩方式（以下示例为snappy）
set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;
2）单条SQL语句的中间结果进行压缩
单条SQL语句的中间结果是指，两个MR（一条SQL语句可能需要通过MR进行计算）之间的临时数据，可通过以下参数进行配置：
--是否对两个MR之间的临时数据进行压缩
set hive.exec.compress.intermediate=true;
--压缩格式（以下示例为snappy）
set hive.intermediate.compression.codec= org.apache.hadoop.io.compress.SnappyCodec;
第10章 企业级调优
10.1 计算资源配置
本教程的计算环境为Hive on MR。计算资源的调整主要包括Yarn和MR。
10.1.1 Yarn资源配置
1）Yarn配置说明
YARN的内存调优的相关参数可以在yarn-site.xml文件中修改，需要调整的YARN参数均与CPU、内存等资源有关，核心配置参数如下。
（1）yarn.nodemanager.resource.memory-mb
该参数的含义是，一个NodeManager节点分配给Container使用的内存。该参数的配置，取决于NodeManager所在节点的总内存容量和该节点运行的其他服务的数量。
考虑上述因素，本书所搭建集群的服务器的内存资源为64GB，且未运行其他服务，此处可将该参数设置为64G，如下：
<property>
    <name>yarn.nodemanager.resource.memory-mb</name>
    <value>65536</value>
</property>
（2）yarn.nodemanager.resource.cpu-vcores
该参数的含义是，一个NodeManager节点分配给Container使用的CPU核数。该参数的配置，同样取决于NodeManager所在节点的总CPU核数和该节点运行的其他服务。
考虑上述因素，本书所搭建集群的服务器的CPU核数为16，且未运行其他服务，此处可将该参数设置为16。
<property>
    <name>yarn.nodemanager.resource.cpu-vcores</name>
    <value>16</value>
</property>
（3）yarn.scheduler.maximum-allocation-mb
该参数的含义是，单个Container能够使用的最大内存。推荐配置如下：
<property>
    <name>yarn.scheduler.maximum-allocation-mb</name>
    <value>16384</value>
</property>
（4）yarn.scheduler.minimum-allocation-mb
该参数的含义是，单个Container能够使用的最小内存，推荐配置如下：
<property>
    <name>yarn.scheduler.minimum-allocation-mb</name>
    <value>512</value>
</property>
2）Yarn配置实操
（1）修改$HADOOP_HOME/etc/hadoop/yarn-site.xml文件
（2）修改如下参数
<property>
    <name>yarn.nodemanager.resource.memory-mb</name>
    <value>65536</value>
</property>
<property>
    <name>yarn.nodemanager.resource.cpu-vcores</name>
    <value>16</value>
</property>
<property>
    <name>yarn.scheduler.maximum-allocation-mb</name>
    <value>16384</value>
</property>
<property>
    <name>yarn.scheduler.minimum-allocation-mb</name>
    <value>512</value>
</property>
（3）分发该配置文件
（4）重启Yarn。
10.1.2 MapReduce资源配置
MapReduce资源配置主要包括Map Task的内存和CPU核数，以及Reduce Task的内存和CPU核数。核心配置参数如下：
1）mapreduce.map.memory.mb	
该参数的含义是，单个Map Task申请的container容器内存大小，其默认值为1024。该值不能超出yarn.scheduler.maximum-allocation-mb和yarn.scheduler.minimum-allocation-mb规定的范围。
该参数需要根据不同的计算任务单独进行配置，在hive中，可直接使用如下方式为每个SQL语句单独进行配置：
set  mapreduce.map.memory.mb=2048;
2）mapreduce.map.cpu.vcores	
该参数的含义是，单个Map Task申请的container容器cpu核数，其默认值为1。该值一般无需调整。
3）mapreduce.reduce.memory.mb	
该参数的含义是，单个Reduce Task申请的container容器内存大小，其默认值为1024。该值同样不能超出yarn.scheduler.maximum-allocation-mb和yarn.scheduler.minimum-allocation-mb规定的范围。
该参数需要根据不同的计算任务单独进行配置，在hive中，可直接使用如下方式为每个SQL语句单独进行配置：
set  mapreduce.reduce.memory.mb=2048;
4）mapreduce.reduce.cpu.vcores	
该参数的含义是，单个Reduce Task申请的container容器cpu核数，其默认值为1。该值一般无需调整。
10.2 测试用表
10.2.1 订单表(2000w条数据)
1）表结构
id
(订单id)	user_id
(用户id)	product_id
(商品id)	province_id
(省份id)	create_time
(下单时间)	product_num
(商品id)	total_amount
(订单金额)
10000001	125442354	15003199	1	2020-06-14 03:54:29	3	100.58
10000002	192758405	17210367	1	2020-06-14 01:19:47	8	677.18
2）建表语句
hive (default)>
drop table if exists order_detail;
create table order_detail(
    id           string comment '订单id',
    user_id      string comment '用户id',
    product_id   string comment '商品id',
    province_id  string comment '省份id',
    create_time  string comment '下单时间',
    product_num  int comment '商品件数',
    total_amount decimal(16, 2) comment '下单金额'
)
partitioned by (dt string)
row format delimited fields terminated by '\t';
3）数据装载
将order_detail.txt文件上传到hadoop102节点的/opt/module/hive/datas/目录，并执行以下导入语句。
注：文件较大，请耐心等待。
hive (default)> 
load data local inpath '/opt/module/hive/datas/order_detail.txt' overwrite into table order_detail partition(dt='2020-06-14');
10.2.2 支付表(600w条数据)
1）表结构
id
(支付id)	order_detail_id
(订单id)	user_id
(用户id)	payment_time
(支付时间)	total_amount
(订单金额)
10000001	17403042	131508758	2020-06-14 13:55:44	391.72
10000002	19198884	133018075	2020-06-14 08:46:23	657.10
2）建表语句
hive (default)>
drop table if exists payment_detail;
create table payment_detail(
    id              string comment '支付id',
    order_detail_id string comment '订单明细id',
    user_id         string comment '用户id',
    payment_time    string comment '支付时间',
    total_amount    decimal(16, 2) comment '支付金额'
)
partitioned by (dt string)
row format delimited fields terminated by '\t';
3）数据装载
将payment_detail.txt文件上传到hadoop102节点的/opt/module/hive/datas/目录，并执行以下导入语句。
注：文件较大，请耐心等待。
hive (default)> 
load data local inpath '/opt/module/hive/datas/payment_detail.txt' overwrite into table payment_detail partition(dt='2020-06-14');
10.2.3 商品信息表（100w条数据）
1）表结构
id
(商品id)	product_name
(商品名称)	price
(价格)	category_id
(分类id)
1000001	CuisW	4517.00	219
1000002	TBtbp	9357.00	208
2）建表语句
hive (default)> 
drop table if exists product_info;
create table product_info(
    id           string comment '商品id',
    product_name string comment '商品名称',
    price        decimal(16, 2) comment '价格',
    category_id  string comment '分类id'
)
row format delimited fields terminated by '\t';
3）数据装载
将product_info.txt文件上传到hadoop102节点的/opt/module/hive/datas/目录，并执行以下导入语句。
hive (default)> 
load data local inpath '/opt/module/hive/datas/product_info.txt' overwrite into table product_info;
10.2.4 省份信息表（34条数据）
1）表结构
id（省份id）	province_name（省份名称）
1	北京
2	天津
2）建表语句
hive (default)> 
drop table if exists province_info;
create table province_info(
    id            string comment '省份id',
    province_name string comment '省份名称'
)
row format delimited fields terminated by '\t';
3）数据装载
将province_info.txt文件上传到hadoop102节点的/opt/module/hive/datas/目录，并执行以下导入语句。
hive (default)> 
load data local inpath '/opt/module/hive/datas/province_info.txt' overwrite into table province_info;
10.3 Explain查看执行计划（重点）
10.3.2 基本语法
Hive中可以使用explain命令来查看Hive SQL的执行计划，用户通过分析执行计划可以看到该条HQL的执行情况，了解性能瓶颈，最后对Hive SQL进行优化。
Hive的expalin命令的具体语法如下。
EXPLAIN [FORMATTED | EXTENDED | DEPENDENCY] query-sql
注：FORMATTED、EXTENDED、DEPENDENCY关键字为可选项，各自作用如下。
	FORMATTED：将执行计划以JSON字符串的形式输出
	EXTENDED：输出执行计划中的额外信息，通常是读写的文件名等信息
	DEPENDENCY：输出执行计划读取的表及分区
10.3.3 案例实操
1）查看下面这条语句的执行计划
hive (default)> 
explain
select
    user_id,
    count(*)
from order_detail
group by user_id; 
结果如下
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: order_detail
            Statistics: Num rows: 13066777 Data size: 11760099340 Basic stats: COMPLETE Column stats: NONE
            Select Operator
              expressions: user_id (type: string)
              outputColumnNames: user_id
              Statistics: Num rows: 13066777 Data size: 11760099340 Basic stats: COMPLETE Column stats: NONE
              Group By Operator
                aggregations: count()
                keys: user_id (type: string)
                mode: hash
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 13066777 Data size: 11760099340 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: string)
                  sort order: +
                  Map-reduce partition columns: _col0 (type: string)
                  Statistics: Num rows: 13066777 Data size: 11760099340 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col1 (type: bigint)
      Execution mode: vectorized
      Reduce Operator Tree:
        Group By Operator
          aggregations: count(VALUE._col0)
          keys: KEY._col0 (type: string)
          mode: mergepartial
          outputColumnNames: _col0, _col1
          Statistics: Num rows: 6533388 Data size: 5880049219 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            Statistics: Num rows: 6533388 Data size: 5880049219 Basic stats: COMPLETE Column stats: NONE
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

2）执行计划如下图
 
10.3.1 Explain执行计划概述
Explain呈现的执行计划，由一系列Stage组成，这一系列Stage具有依赖关系，每个Stage对应一个MapReduce Job，或者一个文件系统操作等。
若某个Stage对应的一个MapReduce Job，其Map端和Reduce端的计算逻辑分别由Map Operator Tree和Reduce Operator Tree进行描述，Operator Tree由一系列的Operator组成，一个Operator代表在Map或Reduce阶段的一个单一的逻辑操作，例如TableScan Operator，Select Operator，Join Operator等。
常见的Operator及其作用如下：
	TableScan：表扫描操作，通常map端第一个操作肯定是表扫描操作
	Select Operator：选取操作
	Group By Operator：分组聚合操作
	Reduce Output Operator：输出到 reduce 操作
	Filter Operator：过滤操作
	Join Operator：join 操作
	File Output Operator：文件输出操作
	Fetch Operator 客户端获取数据操作

对10.3.3节的执行计划进行解读。
这个执行计划分为两个部分，STAGE DEPENDENCIES和STAGE PLANS。
1. STAGE DEPENDENCIES
STAGE DEPENDENCIES展示了执行计划中包含的Stage，以及Stage之间的依赖关系。STAGE DEPENDENCIES的内容如下，表示这个查询过程包含Stage-1和Stage-0，其中Stage-1是根节点，Stage-0依赖于Stage-1，如图13-1所示。
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1
 
图13-1 STAGE DEPENDENCIES展示Stage依赖关系
2. STAGE PLANS
STAGE PLANS展示了各Stage的具体操作细节。
1）stage-1
将描述Stage-1的执行计划提取出来，并进行适当化简，如下所示。以下内容表示Stage-1使用MapReduce模式，并包含Map Operator Tree和Reduce Operator Tree。“Execution mode: vectorized”表示Hive将使用矢量化查询模式来处理数据。在这种模式下，Hive会将一批行数据打包成列向量，然后对每个列向量进行批量处理，从而提高数据处理的效率。矢量化查询将在10.9.3节详细讲解。
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
		……
		……
      Execution mode: vectorized
      Reduce Operator Tree:
		……
		……
① Map Operator Tree
将Map Operator Tree部分提取出来，如下所示。
      Map Operator Tree:
          TableScan
            alias: order_detail
            Statistics: Num rows: 13066777 Data size: 11760099340 Basic stats: COMPLETE Column stats: NONE
            Select Operator
              expressions: user_id (type: string)
              outputColumnNames: user_id
              Statistics: Num rows: 13066777 Data size: 11760099340 Basic stats: COMPLETE Column stats: NONE
              Group By Operator
                aggregations: count()
                keys: user_id (type: string)
                mode: hash
                outputColumnNames: _col0, _col1
                Statistics: Num rows: 13066777 Data size: 11760099340 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: string)
                  sort order: +
                  Map-reduce partition columns: _col0 (type: string)
                  Statistics: Num rows: 13066777 Data size: 11760099340 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col1 (type: bigint)
Map Operator Tree包含以下步骤。
	TableScan：表示扫描名为order_detail的表。
	Select Operator：表示选择user_id这一列。
	Group By Operator：表示按照user_id聚合，统计每个用户的数量。
	Reduce Output Operator：表示将结果按照user_id排序，并将其输出。
结构如图13-2所示。
 
图13-2 Map Operator Tree的结构
② Reduce Operator Tree
将Reduce Operator Tree部分提取出来，如下所示。
      Reduce Operator Tree:
        Group By Operator
          aggregations: count(VALUE._col0)
          keys: KEY._col0 (type: string)
          mode: mergepartial
          outputColumnNames: _col0, _col1
          Statistics: Num rows: 6533388 Data size: 5880049219 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            Statistics: Num rows: 6533388 Data size: 5880049219 Basic stats: COMPLETE Column stats: NONE
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
Reduce Operator Tree包含以下步骤。
	Group By Operator：表示按照user_id聚合，统计每个用户的数量。
	File Output Operator：表示将结果输出到文件中。
结构如图13-3所示。
 
图13-3 Reduce Operator Tree的结构
以上是完整的Stage-1的结构分析。
2）stage-0
STAGE PLANS中Stage-0部分的内容如下所示。
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink
这部分执行计划表示最终的结果输出阶段。Fetch Operator用于从上一个Stage获取数据，并将结果输出至文件。

将以上执行计划的分析过程进一步化简，绘制成可视化图，
 

10.4 HQL语法优化之分组聚合优化
10.4.1 优化说明
Hive中未经优化的分组聚合，是通过一个MapReduce Job实现的。Map端负责读取数据，并按照分组字段分区，通过Shuffle，将数据发往Reduce端，各组数据在Reduce端完成最终的聚合运算。
Hive对分组聚合的优化主要围绕着减少Shuffle数据量进行，具体做法是map-side聚合。所谓map-side聚合，就是在map端维护一个hash table，利用其完成部分的聚合，然后将部分聚合的结果，按照分组字段分区，发送至reduce端，完成最终的聚合。map-side聚合能有效减少shuffle的数据量，提高分组聚合运算的效率。
map-side 聚合相关的参数如下：
--启用map-side聚合
set hive.map.aggr=true;

--用于检测源表数据是否适合进行map-side聚合。检测的方法是：先对若干条数据进行map-side聚合，若聚合后的条数和聚合前的条数比值小于该值，则认为该表适合进行map-side聚合；否则，认为该表数据不适合进行map-side聚合，后续数据便不再进行map-side聚合。
set hive.map.aggr.hash.min.reduction=0.5;

--用于检测源表是否适合map-side聚合的条数。
set hive.groupby.mapaggr.checkinterval=100000;

--map-side聚合所用的hash table，占用map task堆内存的最大比例，若超出该值，则会对hash table进行一次flush。
set hive.map.aggr.hash.force.flush.memory.threshold=0.9;
10.4.2 优化案例
1）示例SQL
hive (default)> 
select
    product_id,
    count(*)
from order_detail
group by product_id;
2）优化前
未经优化的分组聚合，执行计划如下图所示：
 
3）优化思路
可以考虑开启map-side聚合，配置以下参数：
--启用map-side聚合，默认是true
set hive.map.aggr=true;

--用于检测源表数据是否适合进行map-side聚合。检测的方法是：先对若干条数据进行map-side聚合，若聚合后的条数和聚合前的条数比值小于该值，则认为该表适合进行map-side聚合；否则，认为该表数据不适合进行map-side聚合，后续数据便不再进行map-side聚合。
set hive.map.aggr.hash.min.reduction=0.5;

--用于检测源表是否适合map-side聚合的条数。
set hive.groupby.mapaggr.checkinterval=100000;

--map-side聚合所用的hash table，占用map task堆内存的最大比例，若超出该值，则会对hash table进行一次flush。
set hive.map.aggr.hash.force.flush.memory.threshold=0.9;
优化后的执行计划如图所示：
 
10.5 HQL语法优化之Join优化
10.5.1 Join算法概述
Hive拥有多种join算法，包括Common Join，Map Join，Bucket Map Join，Sort Merge Buckt Map Join等，下面对每种join算法做简要说明：
1）Common Join
Common Join是Hive中最稳定的join算法，其通过一个MapReduce Job完成一个join操作。Map端负责读取join操作所需表的数据，并按照关联字段进行分区，通过Shuffle，将其发送到Reduce端，相同key的数据在Reduce端完成最终的Join操作。如下图所示：
 
需要注意的是，sql语句中的join操作和执行计划中的Common Join任务并非一对一的关系，一个sql语句中的相邻的且关联字段相同的多个join操作可以合并为一个Common Join任务。
例如：
hive (default)> 
select 
    a.val, 
    b.val, 
    c.val 
from a 
join b on (a.key = b.key1) 
join c on (c.key = b.key1)
上述sql语句中两个join操作的关联字段均为b表的key1字段，则该语句中的两个join操作可由一个Common Join任务实现，也就是可通过一个Map Reduce任务实现。
hive (default)> 
select 
    a.val, 
    b.val, 
    c.val 
from a 
join b on (a.key = b.key1) 
join c on (c.key = b.key2)
上述sql语句中的两个join操作关联字段各不相同，则该语句的两个join操作需要各自通过一个Common Join任务实现，也就是通过两个Map Reduce任务实现。
2）Map Join
Map Join算法可以通过两个只有map阶段的Job完成一个join操作。其适用场景为大表join小表。若某join操作满足要求，则第一个Job会读取小表数据，将其制作为hash table，并上传至Hadoop分布式缓存（本质上是上传至HDFS）。第二个Job会先从分布式缓存中读取小表数据，并缓存在Map Task的内存中，然后扫描大表数据，这样在map端即可完成关联操作。如下图所示：
 
3）Bucket Map Join
Bucket Map Join是对Map Join算法的改进，其打破了Map Join只适用于大表join小表的限制，可用于大表join大表的场景。
Bucket Map Join的核心思想是：若能保证参与join的表均为分桶表，且关联字段为分桶字段，且其中一张表的分桶数量是另外一张表分桶数量的整数倍，就能保证参与join的两张表的分桶之间具有明确的关联关系，所以就可以在两表的分桶间进行Map Join操作了。这样一来，第二个Job的Map端就无需再缓存小表的全表数据了，而只需缓存其所需的分桶即可。其原理如图所示：
 4）Sort Merge Bucket Map Join
Sort Merge Bucket Map Join（简称SMB Map Join）基于Bucket Map Join。SMB Map Join要求，参与join的表均为分桶表，且需保证分桶内的数据是有序的，且分桶字段、排序字段和关联字段为相同字段，且其中一张表的分桶数量是另外一张表分桶数量的整数倍。
SMB Map Join同Bucket Join一样，同样是利用两表各分桶之间的关联关系，在分桶之间进行join操作，不同的是，分桶之间的join操作的实现原理。Bucket Map Join，两个分桶之间的join实现原理为Hash Join算法；而SMB Map Join，两个分桶之间的join实现原理为Sort Merge Join算法。
Hash Join和Sort Merge Join均为关系型数据库中常见的Join实现算法。Hash Join的原理相对简单，就是对参与join的一张表构建hash table，然后扫描另外一张表，然后进行逐行匹配。Sort Merge Join需要在两张按照关联字段排好序的表中进行，其原理如图所示：
 
Hive中的SMB Map Join就是对两个分桶的数据按照上述思路进行Join操作。可以看出，SMB Map Join与Bucket Map Join相比，在进行Join操作时，Map端是无需对整个Bucket构建hash table，也无需在Map端缓存整个Bucket数据的，每个Mapper只需按顺序逐个key读取两个分桶的数据进行join即可。
10.5.2 Map Join
10.5.2.1 优化说明
Map Join有两种触发方式，一种是用户在SQL语句中增加hint提示，另外一种是Hive优化器根据参与join表的数据量大小，自动触发。
1）Hint提示
用户可通过如下方式，指定通过map join算法，并且ta将作为map join中的小表。这种方式已经过时，不推荐使用。
hive (default)> 
select /*+ mapjoin(ta) */
    ta.id,
    tb.id
from table_a ta
join table_b tb
on ta.id=tb.id;
2）自动触发
Hive在编译SQL语句阶段，起初所有的join操作均采用Common Join算法实现。
之后在物理优化阶段，Hive会根据每个Common Join任务所需表的大小判断该Common Join任务是否能够转换为Map Join任务，若满足要求，便将Common Join任务自动转换为Map Join任务。
但有些Common Join任务所需的表大小，在SQL的编译阶段是未知的（例如对子查询进行join操作），所以这种Common Join任务是否能转换成Map Join任务在编译阶是无法确定的。
针对这种情况，Hive会在编译阶段生成一个条件任务（Conditional Task），其下会包含一个计划列表，计划列表中包含转换后的Map Join任务以及原有的Common Join任务。最终具体采用哪个计划，是在运行时决定的。大致思路如下图所示：
 
Map join自动转换的具体判断逻辑如下图所示：
 
图中涉及到的参数如下：
--启动Map Join自动转换
set hive.auto.convert.join=true;

--一个Common Join operator转为Map Join operator的判断条件,若该Common Join相关的表中,存在n-1张表的已知大小总和<=该值,则生成一个Map Join计划,此时可能存在多种n-1张表的组合均满足该条件,则hive会为每种满足条件的组合均生成一个Map Join计划,同时还会保留原有的Common Join计划作为后备(back up)计划,实际运行时,优先执行Map Join计划，若不能执行成功，则启动Common Join后备计划。
set hive.mapjoin.smalltable.filesize=250000;

--开启无条件转Map Join
set hive.auto.convert.join.noconditionaltask=true;

--无条件转Map Join时的小表之和阈值,若一个Common Join operator相关的表中，存在n-1张表的大小总和<=该值,此时hive便不会再为每种n-1张表的组合均生成Map Join计划,同时也不会保留Common Join作为后备计划。而是只生成一个最优的Map Join计划。
set hive.auto.convert.join.noconditionaltask.size=10000000;
10.5.2.2 优化案例
1）示例SQL
hive (default)> 
select
    *
from order_detail od
join product_info product on od.product_id = product.id
join province_info province on od.province_id = province.id;
2）优化前
上述SQL语句共有三张表进行两次join操作，且两次join操作的关联字段不同。故优化前的执行计划应该包含两个Common Join operator，也就是由两个MapReduce任务实现。执行计划如下图所示：
 
3）优化思路
经分析，参与join的三张表，数据量如下
表名	大小
order_detail	1176009934（约1122M）
product_info	25285707（约24M）
province_info	369（约0.36K）
注：可使用如下语句获取表/分区的大小信息
hive (default)> 
desc formatted table_name partition(partition_col='partition');
三张表中，product_info和province_info数据量较小，可考虑将其作为小表，进行Map Join优化。
根据前文Common Join任务转Map Join任务的判断逻辑图，可得出以下优化方案：
方案一：
启用Map Join自动转换。
hive (default)> 
set hive.auto.convert.join=true;
不使用无条件转Map Join。
hive (default)> 
set hive.auto.convert.join.noconditionaltask=false;
调整hive.mapjoin.smalltable.filesize参数，使其大于等于product_info。
hive (default)> 
set hive.mapjoin.smalltable.filesize=25285707;
这样可保证将两个Common Join operator均可转为Map Join operator，并保留Common Join作为后备计划，保证计算任务的稳定。调整完的执行计划如下图：
 
方案二：
启用Map Join自动转换。
hive (default)> 
set hive.auto.convert.join=true;
使用无条件转Map Join。
hive (default)> 
set hive.auto.convert.join.noconditionaltask=true;
调整hive.auto.convert.join.noconditionaltask.size参数，使其大于等于product_info和province_info之和。
hive (default)> 
set hive.auto.convert.join.noconditionaltask.size=25286076;
这样可直接将两个Common Join operator转为两个Map Join operator，并且由于两个Map Join operator的小表大小之和小于等于hive.auto.convert.join.noconditionaltask.size，故两个Map Join operator任务可合并为同一个。这个方案计算效率最高，但需要的内存也是最多的。
调整完的执行计划如下图：
 
方案三：
启用Map Join自动转换。
hive (default)> 
set hive.auto.convert.join=true;
使用无条件转Map Join。
hive (default)> 
set hive.auto.convert.join.noconditionaltask=true;
调整hive.auto.convert.join.noconditionaltask.size参数，使其等于product_info。
hive (default)> 
set hive.auto.convert.join.noconditionaltask.size=25285707;
这样可直接将两个Common Join operator转为Map Join operator，但不会将两个Map Join的任务合并。该方案计算效率比方案二低，但需要的内存也更少。
调整完的执行计划如下图：
 
10.5.3 Bucket Map Join
10.5.3.1 优化说明
Bucket Map Join不支持自动转换，发须通过用户在SQL语句中提供如下Hint提示，并配置如下相关参数，方可使用。
1）Hint提示
hive (default)> 
select /*+ mapjoin(ta) */
    ta.id,
    tb.id
from table_a ta
join table_b tb on ta.id=tb.id;
2）相关参数
--关闭cbo优化，cbo会导致hint信息被忽略
set hive.cbo.enable=false;
--map join hint默认会被忽略(因为已经过时)，需将如下参数设置为false
set hive.ignore.mapjoin.hint=false;
--启用bucket map join优化功能
set hive.optimize.bucketmapjoin = true;
10.5.3.2 优化案例
1）示例SQL
hive (default)> 
select
    *
from(
    select
        *
    from order_detail
    where dt='2020-06-14'
)od
join(
    select
        *
    from payment_detail
    where dt='2020-06-14'
)pd
on od.id=pd.order_detail_id;
2）优化前
上述SQL语句共有两张表一次join操作，故优化前的执行计划应包含一个Common Join任务，通过一个MapReduce Job实现。执行计划如下图所示：
 
3）优化思路
经分析，参与join的两张表，数据量如下。
表名	大小
order_detail	1176009934（约1122M）
payment_detail	334198480（约319M）
两张表都相对较大，若采用普通的Map Join算法，则Map端需要较多的内存来缓存数据，当然可以选择为Map段分配更多的内存，来保证任务运行成功。但是，Map端的内存不可能无上限的分配，所以当参与Join的表数据量均过大时，就可以考虑采用Bucket Map Join算法。下面演示如何使用Bucket Map Join。
首先需要依据源表创建两个分桶表，order_detail建议分16个bucket，payment_detail建议分8个bucket,注意分桶个数的倍数关系以及分桶字段。
--订单表
hive (default)> 
drop table if exists order_detail_bucketed;
create table order_detail_bucketed(
    id           string comment '订单id',
    user_id      string comment '用户id',
    product_id   string comment '商品id',
    province_id  string comment '省份id',
    create_time  string comment '下单时间',
    product_num  int comment '商品件数',
    total_amount decimal(16, 2) comment '下单金额'
)
clustered by (id) into 16 buckets
row format delimited fields terminated by '\t';

--支付表
hive (default)> 
drop table if exists payment_detail_bucketed;
create table payment_detail_bucketed(
    id              string comment '支付id',
    order_detail_id string comment '订单明细id',
    user_id         string comment '用户id',
    payment_time    string comment '支付时间',
    total_amount    decimal(16, 2) comment '支付金额'
)
clustered by (order_detail_id) into 8 buckets
row format delimited fields terminated by '\t';
然后向两个分桶表导入数据。
--订单表
hive (default)> 
insert overwrite table order_detail_bucketed
select
    id,
    user_id,
    product_id,
    province_id,
    create_time,
    product_num,
    total_amount   
from order_detail
where dt='2020-06-14';

--分桶表
hive (default)> 
insert overwrite table payment_detail_bucketed
select
    id,
    order_detail_id,
    user_id,
    payment_time,
    total_amount
from payment_detail
where dt='2020-06-14';
然后设置以下参数：
--关闭cbo优化，cbo会导致hint信息被忽略，需将如下参数修改为false
set hive.cbo.enable=false;
--map join hint默认会被忽略(因为已经过时)，需将如下参数修改为false
set hive.ignore.mapjoin.hint=false;
--启用bucket map join优化功能,默认不启用，需将如下参数修改为true
set hive.optimize.bucketmapjoin = true;
最后在重写SQL语句，如下：
hive (default)> 
select /*+ mapjoin(pd) */
    *
from order_detail_bucketed od
join payment_detail_bucketed pd on od.id = pd.order_detail_id;
优化后的执行计划如图所示：
 
需要注意的是，Bucket Map Join的执行计划的基本信息和普通的Map Join无异，若想看到差异，可执行如下语句，查看执行计划的详细信息。详细执行计划中，如在Map Join Operator中看到 “BucketMapJoin: true”，则表明使用的Join算法为Bucket Map Join。
hive (default)> 
explain extended select /*+ mapjoin(pd) */
    *
from order_detail_bucketed od
join payment_detail_bucketed pd on od.id = pd.order_detail_id;
10.5.4 Sort Merge Bucket Map Join
10.5.4.1 优化说明
Sort Merge Bucket Map Join有两种触发方式，包括Hint提示和自动转换。Hint提示已过时，不推荐使用。下面是自动转换的相关参数：
--启动Sort Merge Bucket Map Join优化
set hive.optimize.bucketmapjoin.sortedmerge=true;
--使用自动转换SMB Join
set hive.auto.convert.sortmerge.join=true;
10.5.4.2 优化案例
1）示例SQL语句
hive (default)> 
select
    *
from(
    select
        *
    from order_detail
    where dt='2020-06-14'
)od
join(
    select
        *
    from payment_detail
    where dt='2020-06-14'
)pd
on od.id=pd.order_detail_id;
2）优化前
上述SQL语句共有两张表一次join操作，故优化前的执行计划应包含一个Common Join任务，通过一个MapReduce Job实现。
3）优化思路
经分析，参与join的两张表，数据量如下。
表名	大小
order_detail	1176009934（约1122M）
payment_detail	334198480（约319M）
两张表都相对较大，除了可以考虑采用Bucket Map Join算法，还可以考虑SMB Join。相较于Bucket Map Join，SMB Map Join对分桶大小是没有要求的。下面演示如何使用SMB Map Join。
首先需要依据源表创建两个的有序的分桶表，order_detail建议分16个bucket，payment_detail建议分8个bucket,注意分桶个数的倍数关系以及分桶字段和排序字段。
--订单表
hive (default)> 
drop table if exists order_detail_sorted_bucketed;
create table order_detail_sorted_bucketed(
    id           string comment '订单id',
    user_id      string comment '用户id',
    product_id   string comment '商品id',
    province_id  string comment '省份id',
    create_time  string comment '下单时间',
    product_num  int comment '商品件数',
    total_amount decimal(16, 2) comment '下单金额'
)
clustered by (id) sorted by(id) into 16 buckets
row format delimited fields terminated by '\t';

--支付表
hive (default)> 
drop table if exists payment_detail_sorted_bucketed;
create table payment_detail_sorted_bucketed(
    id              string comment '支付id',
    order_detail_id string comment '订单明细id',
    user_id         string comment '用户id',
    payment_time    string comment '支付时间',
    total_amount    decimal(16, 2) comment '支付金额'
)
clustered by (order_detail_id) sorted by(order_detail_id) into 8 buckets
row format delimited fields terminated by '\t';
然后向两个分桶表导入数据。
--订单表
hive (default)> 
insert overwrite table order_detail_sorted_bucketed
select
    id,
    user_id,
    product_id,
    province_id,
    create_time,
    product_num,
    total_amount   
from order_detail
where dt='2020-06-14';

--分桶表
hive (default)> 
insert overwrite table payment_detail_sorted_bucketed
select
    id,
    order_detail_id,
    user_id,
    payment_time,
    total_amount
from payment_detail
where dt='2020-06-14';
然后设置以下参数：
--启动Sort Merge Bucket Map Join优化
set hive.optimize.bucketmapjoin.sortedmerge=true;
--使用自动转换SMB Join
set hive.auto.convert.sortmerge.join=true;
最后在重写SQL语句，如下：
hive (default)> 
select
    *
from order_detail_sorted_bucketed od
join payment_detail_sorted_bucketed pd
on od.id = pd.order_detail_id;
优化后的执行计如图所示：
 
10.6 HQL语法优化之数据倾斜
10.6.1 数据倾斜概述
数据倾斜问题，通常是指参与计算的数据分布不均，即某个key或者某些key的数据量远超其他key，导致在shuffle阶段，大量相同key的数据被发往同一个Reduce，进而导致该Reduce所需的时间远超其他Reduce，成为整个任务的瓶颈。
Hive中的数据倾斜常出现在分组聚合和join操作的场景中，下面分别介绍在上述两种场景下的优化思路。
10.6.2 分组聚合导致的数据倾斜
10.6.2.1 优化说明
前文提到过，Hive中未经优化的分组聚合，是通过一个MapReduce Job实现的。Map端负责读取数据，并按照分组字段分区，通过Shuffle，将数据发往Reduce端，各组数据在Reduce端完成最终的聚合运算。
如果group by分组字段的值分布不均，就可能导致大量相同的key进入同一Reduce，从而导致数据倾斜问题。
由分组聚合导致的数据倾斜问题，有以下两种解决思路：
1）Map-Side聚合
开启Map-Side聚合后，数据会现在Map端完成部分聚合工作。这样一来即便原始数据是倾斜的，经过Map端的初步聚合后，发往Reduce的数据也就不再倾斜了。最佳状态下，Map-端聚合能完全屏蔽数据倾斜问题。
相关参数如下：
--启用map-side聚合
set hive.map.aggr=true;

--用于检测源表数据是否适合进行map-side聚合。检测的方法是：先对若干条数据进行map-side聚合，若聚合后的条数和聚合前的条数比值小于该值，则认为该表适合进行map-side聚合；否则，认为该表数据不适合进行map-side聚合，后续数据便不再进行map-side聚合。
set hive.map.aggr.hash.min.reduction=0.5;

--用于检测源表是否适合map-side聚合的条数。
set hive.groupby.mapaggr.checkinterval=100000;

--map-side聚合所用的hash table，占用map task堆内存的最大比例，若超出该值，则会对hash table进行一次flush。
set hive.map.aggr.hash.force.flush.memory.threshold=0.9;
2）Skew-GroupBy优化
Skew-GroupBy的原理是启动两个MR任务，第一个MR按照随机数分区，将数据分散发送到Reduce，完成部分聚合，第二个MR按照分组字段分区，完成最终聚合。
相关参数如下：
--启用分组聚合数据倾斜优化
set hive.groupby.skewindata=true;
10.6.2.2 优化案例
1）示例SQL语句
hive (default)>
select
    province_id,
    count(*)
from order_detail
group by province_id;
2）优化前
该表数据中的province_id字段是存在倾斜的，若不经过优化，通过观察任务的执行过程，是能够看出数据倾斜现象的。
 
需要注意的是，hive中的map-side聚合是默认开启的，若想看到数据倾斜的现象，需要先将hive.map.aggr参数设置为false。
3）优化思路
通过上述两种思路均可解决数据倾斜的问题。下面分别进行说明：
（1）Map-Side聚合
设置如下参数
--启用map-side聚合
set hive.map.aggr=true;
--关闭skew-groupby
set hive.groupby.skewindata=false;
开启map-side聚合后的执行计划如下图所示：
 
很明显可以看到开启map-side聚合后，reduce数据不再倾斜。
 
（2）Skew-GroupBy优化
设置如下参数
--启用skew-groupby
set hive.groupby.skewindata=true;
--关闭map-side聚合
set hive.map.aggr=false;
开启Skew-GroupBy优化后，可以很明显看到该sql执行在yarn上启动了两个mr任务，第一个mr打散数据，第二个mr按照打散后的数据进行分组聚合。
 
10.6.3 Join导致的数据倾斜
10.6.3.1 优化说明
前文提到过，未经优化的join操作，默认是使用common join算法，也就是通过一个MapReduce Job完成计算。Map端负责读取join操作所需表的数据，并按照关联字段进行分区，通过Shuffle，将其发送到Reduce端，相同key的数据在Reduce端完成最终的Join操作。
如果关联字段的值分布不均，就可能导致大量相同的key进入同一Reduce，从而导致数据倾斜问题。
由join导致的数据倾斜问题，有如下三种解决方案：
1）map join
使用map join算法，join操作仅在map端就能完成，没有shuffle操作，没有reduce阶段，自然不会产生reduce端的数据倾斜。该方案适用于大表join小表时发生数据倾斜的场景。
相关参数如下：
--启动Map Join自动转换
set hive.auto.convert.join=true;

--一个Common Join operator转为Map Join operator的判断条件,若该Common Join相关的表中,存在n-1张表的大小总和<=该值,则生成一个Map Join计划,此时可能存在多种n-1张表的组合均满足该条件,则hive会为每种满足条件的组合均生成一个Map Join计划,同时还会保留原有的Common Join计划作为后备(back up)计划,实际运行时,优先执行Map Join计划，若不能执行成功，则启动Common Join后备计划。
set hive.mapjoin.smalltable.filesize=250000;

--开启无条件转Map Join
set hive.auto.convert.join.noconditionaltask=true;

--无条件转Map Join时的小表之和阈值,若一个Common Join operator相关的表中，存在n-1张表的大小总和<=该值,此时hive便不会再为每种n-1张表的组合均生成Map Join计划,同时也不会保留Common Join作为后备计划。而是只生成一个最优的Map Join计划。
set hive.auto.convert.join.noconditionaltask.size=10000000;
2）skew join
skew join的原理是，为倾斜的大key单独启动一个map join任务进行计算，其余key进行正常的common join。原理图如下：
 
相关参数如下：
--启用skew join优化
set hive.optimize.skewjoin=true;
--触发skew join的阈值，若某个key的行数超过该参数值，则触发
set hive.skewjoin.key=100000;
这种方案对参与join的源表大小没有要求，但是对两表中倾斜的key的数据量有要求，要求一张表中的倾斜key的数据量比较小（方便走mapjoin）。
3）调整SQL语句
若参与join的两表均为大表，其中一张表的数据是倾斜的，此时也可通过以下方式对SQL语句进行相应的调整。
假设原始SQL语句如下：A，B两表均为大表，且其中一张表的数据是倾斜的。
hive (default)>
select
    *
from A
join B
on A.id=B.id;
其join过程如下：
 	图中1001为倾斜的大key，可以看到，其被发往了同一个Reduce进行处理。
调整SQL语句如下：
hive (default)>
select
    *
from(
    select --打散操作
        concat(id,'_',cast(rand()*2 as int)) id,
        value
    from A
)ta
join(
    select --扩容操作
        concat(id,'_',0) id,
        value
    from B
    union all
    select
        concat(id,'_',1) id,
        value
    from B
)tb
on ta.id=tb.id;
调整之后的SQL语句执行计划如下图所示：
 
10.6.3.2 优化案例
1）示例SQL语句
hive (default)>
select
    *
from order_detail od
join province_info pi
on od.province_id=pi.id;
2）优化前
order_detail表中的province_id字段是存在倾斜的，若不经过优化，通过观察任务的执行过程，是能够看出数据倾斜现象的。
 
需要注意的是，hive中的map join自动转换是默认开启的，若想看到数据倾斜的现象，需要先将hive.auto.convert.join参数设置为false。
3）优化思路
上述两种优化思路均可解决该数据倾斜问题，下面分别进行说明：
（1）map join
设置如下参数。
--启用map join
set hive.auto.convert.join=true;
--关闭skew join
set hive.optimize.skewjoin=false;
可以很明显看到开启map join以后，mr任务只有map阶段，没有reduce阶段，自然也就不会有数据倾斜发生。
 
 
（2）skew join
设置如下参数。
--启动skew join
set hive.optimize.skewjoin=true;
--关闭map join
set hive.auto.convert.join=false;
开启skew join后，使用explain可以很明显看到执行计划如下图所示，说明skew join生效，任务既有common join，又有部分key走了map join。
 
并且该sql在yarn上最终启动了两个mr任务，而且第二个任务只有map没有reduce阶段，说明第二个任务是对倾斜的key进行了map join。
 
 
10.7 HQL语法优化之任务并行度
10.7.1 优化说明
对于一个分布式的计算任务而言，设置一个合适的并行度十分重要。Hive的计算任务由MapReduce完成，故并行度的调整需要分为Map端和Reduce端。
10.7.1.1 Map端并行度
Map端的并行度，也就是Map的个数。是由输入文件的切片数决定的。一般情况下，Map端的并行度无需手动调整。
以下特殊情况可考虑调整map端并行度：
1）查询的表中存在大量小文件
按照Hadoop默认的切片策略，一个小文件会单独启动一个map task负责计算。若查询的表中存在大量小文件，则会启动大量map task，造成计算资源的浪费。这种情况下，可以使用Hive提供的CombineHiveInputFormat，多个小文件合并为一个切片，从而控制map task个数。相关参数如下：
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
2）map端有复杂的查询逻辑
若SQL语句中有正则替换、json解析等复杂耗时的查询逻辑时，map端的计算会相对慢一些。若想加快计算速度，在计算资源充足的情况下，可考虑增大map端的并行度，令map task多一些，每个map task计算的数据少一些。相关参数如下：
--一个切片的最大值
set mapreduce.input.fileinputformat.split.maxsize=256000000;
10.7.1.2 Reduce端并行度
Reduce端的并行度，也就是Reduce个数。相对来说，更需要关注。Reduce端的并行度，可由用户自己指定，也可由Hive自行根据该MR Job输入的文件大小进行估算。
Reduce端的并行度的相关参数如下：
--指定Reduce端并行度，默认值为-1，表示用户未指定
set mapreduce.job.reduces;
--Reduce端并行度最大值
set hive.exec.reducers.max;
--单个Reduce Task计算的数据量，用于估算Reduce并行度
set hive.exec.reducers.bytes.per.reducer;
Reduce端并行度的确定逻辑如下：
若指定参数mapreduce.job.reduces的值为一个非负整数，则Reduce并行度为指定值。否则，Hive自行估算Reduce并行度，估算逻辑如下：
假设Job输入的文件大小为totalInputBytes
参数hive.exec.reducers.bytes.per.reducer的值为bytesPerReducer。
参数hive.exec.reducers.max的值为maxReducers。
则Reduce端的并行度为：
min⁡(ceil(totalInputBytes/bytesPerReducer),maxReducers)
根据上述描述，可以看出，Hive自行估算Reduce并行度时，是以整个MR Job输入的文件大小作为依据的。因此，在某些情况下其估计的并行度很可能并不准确，此时就需要用户根据实际情况来指定Reduce并行度了。
10.7.2 优化案例
1）示例SQL语句
hive (default)>
select
    province_id,
    count(*)
from order_detail
group by province_id;
2）优化前
上述sql语句，在不指定Reduce并行度时，Hive自行估算并行度的逻辑如下：
totalInputBytes= 1136009934
bytesPerReducer=256000000
maxReducers=1009
经计算，Reduce并行度为
numReducers=min(ceil(1136009934/256000000),1009)=5
3）优化思路
上述sql语句，在默认情况下，是会进行map-side聚合的，也就是Reduce端接收的数据，实际上是map端完成聚合之后的结果。观察任务的执行过程，会发现，每个map端输出的数据只有34条记录，共有5个map task。
 
也就是说Reduce端实际只会接收170（34*5）条记录，故理论上Reduce端并行度设置为1就足够了。这种情况下，用户可通过以下参数，自行设置Reduce端并行度为1。
--指定Reduce端并行度，默认值为-1，表示用户未指定
set mapreduce.job.reduces=1;
10.8 HQL语法优化之小文件合并
10.8.1 优化说明
小文件合并优化，分为两个方面，分别是Map端输入的小文件合并，和Reduce端输出的小文件合并。
10.8.1.1 Map端输入文件合并
合并Map端输入的小文件，是指将多个小文件划分到一个切片中，进而由一个Map Task去处理。目的是防止为单个小文件启动一个Map Task，浪费计算资源。
相关参数为：
--可将多个小文件切片，合并为一个切片，进而由一个map任务处理
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; 
10.8.1.2 Reduce输出文件合并
合并Reduce端输出的小文件，是指将多个小文件合并成大文件。目的是减少HDFS小文件数量。其原理是根据计算任务输出文件的平均大小进行判断，若符合条件，则单独启动一个额外的任务进行合并。
相关参数为：
--开启合并map only任务输出的小文件
set hive.merge.mapfiles=true;

--开启合并map reduce任务输出的小文件
set hive.merge.mapredfiles=true;

--合并后的文件大小
set hive.merge.size.per.task=256000000;

--触发小文件合并任务的阈值，若某计算任务输出的文件平均大小低于该值，则触发合并
set hive.merge.smallfiles.avgsize=16000000;
10.8.2 优化案例
1）示例用表
现有一个需求，计算各省份订单金额总和，下表为结果表。
hive (default)>
drop table if exists order_amount_by_province;
create table order_amount_by_province(
    province_id string comment '省份id',
    order_amount decimal(16,2) comment '订单金额'
)
location '/order_amount_by_province';
2）示例SQL语句
hive (default)>
insert overwrite table order_amount_by_province
select
    province_id,
    sum(total_amount)
from order_detail
group by province_id;
3）优化前
根据任务并行度一节所需内容，可分析出，默认情况下，该sql语句的Reduce端并行度为5，故最终输出文件个数也为5，下图为输出文件，可以看出，5个均为小文件。
 
4）优化思路
若想避免小文件的产生，可采取方案有两个。
（1）合理设置任务的Reduce端并行度
若将上述计算任务的并行度设置为1，就能保证其输出结果只有一个文件。
（2）启用Hive合并小文件优化
设置以下参数：
--开启合并map reduce任务输出的小文件
set hive.merge.mapredfiles=true;

--合并后的文件大小
set hive.merge.size.per.task=256000000;

--触发小文件合并任务的阈值，若某计算任务输出的文件平均大小低于该值，则触发合并
set hive.merge.smallfiles.avgsize=16000000;
再次执行上述的insert语句，观察结果表中的文件，只剩一个了。
 
10.9 其他优化
10.9.1 CBO优化
10.9.1.1 优化说明
CBO是指Cost based Optimizer，即基于计算成本的优化。
在Hive中，计算成本模型考虑到了：数据的行数、CPU、本地IO、HDFS IO、网络IO等方面。Hive会计算同一SQL语句的不同执行计划的计算成本，并选出成本最低的执行计划。目前CBO在hive的MR引擎下主要用于join的优化，例如多表join的join顺序。
相关参数为：
--是否启用cbo优化 
set hive.cbo.enable=true;
10.9.2.2 优化案例
1）示例SQL语句
hive (default)>
select
    *
from order_detail od
join product_info product on od.product_id=product.id
join province_info province on od.province_id=province.id;
2）关闭CBO优化
--关闭cbo优化 
set hive.cbo.enable=false;

--为了测试效果更加直观，关闭map join自动转换
set hive.auto.convert.join=false;
根据执行计划，可以看出，三张表的join顺序如下：
 
3）开启CBO优化
--开启cbo优化 
set hive.cbo.enable=true;
--为了测试效果更加直观，关闭map join自动转换
set hive.auto.convert.join=false;
根据执行计划，可以看出，三张表的join顺序如下：
 
4）总结
根据上述案例可以看出，CBO优化对于执行计划中join顺序是有影响的，其之所以会将province_info的join顺序提前，是因为province info的数据量较小，将其提前，会有更大的概率使得中间结果的数据量变小，从而使整个计算任务的数据量减小，也就是使计算成本变小。
10.9.2 谓词下推
10.9.2.1 优化说明
谓词下推（predicate pushdown）是指，尽量将过滤操作前移，以减少后续计算步骤的数据量。
相关参数为：
--是否启动谓词下推（predicate pushdown）优化
set hive.optimize.ppd = true;
需要注意的是：
CBO优化也会完成一部分的谓词下推优化工作，因为在执行计划中，谓词越靠前，整个计划的计算成本就会越低。
10.9.2.2 优化案例
1）示例SQL语句
hive (default)>
select
    *
from order_detail
join province_info
where order_detail.province_id='2';
2）关闭谓词下推优化
--是否启动谓词下推（predicate pushdown）优化
set hive.optimize.ppd = false;

--为了测试效果更加直观，关闭cbo优化
set hive.cbo.enable=false;
通过执行计划可以看到，过滤操作位于执行计划中的join操作之后。
3）开启谓词下推优化
--是否启动谓词下推（predicate pushdown）优化
set hive.optimize.ppd = true;

--为了测试效果更加直观，关闭cbo优化
set hive.cbo.enable=false;
通过执行计划可以看出，过滤操作位于执行计划中的join操作之前。
10.9.3 矢量化查询
Hive的矢量化查询优化，依赖于CPU的矢量化计算，CPU的矢量化计算的基本原理如下图：
 
Hive的矢量化查询，可以极大的提高一些典型查询场景（例如scans, filters, aggregates, and joins）下的CPU使用效率。
相关参数如下：
set hive.vectorized.execution.enabled=true;
若执行计划中，出现“Execution mode: vectorized”字样，即表明使用了矢量化计算。
官网参考连接：
https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution#VectorizedQueryExecution-Limitations
10.9.4 Fetch抓取
Fetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算。例如：select * from emp;在这种情况下，Hive可以简单地读取emp对应的存储目录下的文件，然后输出查询结果到控制台。
相关参数如下：
--是否在特定场景转换为fetch 任务
--设置为none表示不转换
--设置为minimal表示支持select *，分区字段过滤，Limit等
--设置为more表示支持select 任意字段,包括函数，过滤，和limit等
set hive.fetch.task.conversion=more;
10.9.5 本地模式
10.9.5.1 优化说明
大多数的Hadoop Job是需要Hadoop提供的完整的可扩展性来处理大数据集的。不过，有时Hive的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能会比实际job的执行时间要多的多。对于大多数这种情况，Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。
相关参数如下：
--开启自动转换为本地模式
set hive.exec.mode.local.auto=true;  

--设置local MapReduce的最大输入数据量，当输入数据量小于这个值时采用local  MapReduce的方式，默认为134217728，即128M
set hive.exec.mode.local.auto.inputbytes.max=50000000;

--设置local MapReduce的最大输入文件个数，当输入文件个数小于这个值时采用local MapReduce的方式，默认为4
set hive.exec.mode.local.auto.input.files.max=10;
10.9.5.2 优化案例
1）示例SQL语句
hive (default)>
select
    count(*)
from product_info
group by category_id;
2）关闭本地模式
set hive.exec.mode.local.auto=false;
3）开启本地模式
set hive.exec.mode.local.auto=true;
10.9.6 并行执行
Hive会将一个SQL语句转化成一个或者多个Stage，每个Stage对应一个MR Job。默认情况下，Hive同时只会执行一个Stage。但是某SQL语句可能会包含多个Stage，但这多个Stage可能并非完全互相依赖，也就是说有些Stage是可以并行执行的。此处提到的并行执行就是指这些Stage的并行执行。相关参数如下：
--启用并行执行优化
set hive.exec.parallel=true;       
    
--同一个sql允许最大并行度，默认为8
set hive.exec.parallel.thread.number=8; 
10.9.7 严格模式
Hive可以通过设置某些参数防止危险操作：
1）分区表不使用分区过滤
将hive.strict.checks.no.partition.filter设置为true时，对于分区表，除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行。换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表。
2）使用order by没有limit过滤
将hive.strict.checks.orderby.no.limit设置为true时，对于使用了order by语句的查询，要求必须使用limit语句。因为order by为了执行排序过程会将所有的结果数据分发到同一个Reduce中进行处理，强制要求用户增加这个limit语句可以防止Reduce额外执行很长一段时间（开启了limit可以在数据进入到Reduce之前就减少一部分数据）。
3）笛卡尔积
将hive.strict.checks.cartesian.product设置为true时，会限制笛卡尔积的查询。对关系型数据库非常了解的用户可能期望在执行JOIN查询的时候不使用ON语句而是使用where语句，这样关系数据库的执行优化器就可以高效地将WHERE语句转化成那个ON语句。不幸的是，Hive并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况。
附录：常见错误及解决方案
1）连接不上MySQL数据库
（1）导错驱动包，应该把mysql-connector-java-5.1.27-bin.jar导入/opt/module/hive/lib的不是这个包。错把mysql-connector-java-5.1.27.tar.gz导入hive/lib包下。
（2）修改user表中的主机名称没有都修改为%，而是修改为localhost
2）Hive默认的输入格式处理是CombineHiveInputFormat，会对小文件进行合并。
hive (default)> set hive.input.format;
hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat
可以采用HiveInputFormat就会根据分区数输出相应的文件。
hive (default)> set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
3）不能执行MapReduce程序
可能是Hadoop的Yarn没开启。
4）启动MySQL服务时，报MySQL server PID file could not be found! 异常。
在/var/lock/subsys/mysql路径下创建hadoop102.pid，并在文件中添加内容：4396
5）报service mysql status MySQL is not running, but lock file (/var/lock/subsys/mysql[失败])异常。
解决方案：在/var/lib/mysql目录下创建：-rw-rw----. 1 mysql mysql     5 12月 22 16:41 hadoop102.pid文件，并修改权限为777。
6）JVM堆内存溢出（Hive集群运行模式）
描述：java.lang.OutOfMemoryError: Java heap space
解决：在yarn-site.xml中加入如下代码。
<property>
	<name>yarn.scheduler.maximum-allocation-mb</name>
	<value>2048</value>
</property>
<property>
  	<name>yarn.scheduler.minimum-allocation-mb</name>
  	<value>2048</value>
</property>
<property>
	<name>yarn.nodemanager.vmem-pmem-ratio</name>
	<value>2.1</value>
</property>
<property>
	<name>mapred.child.java.opts</name>
	<value>-Xmx1024m</value>
</property>
7）JVM堆内存溢出（Hive本地运行模式）
描述：在启用Hive本地模式后，hive.log报错java.lang.OutOfMemoryError: Java heap space
解决方案1（临时）：
在Hive客户端临时设置io.sort.mb和mapreduce.task.io.sort.mb两个参数的值为10。
0: jdbc:hive2://hadoop102:10000> set io.sort.mb;
+-----------------+
|       set         |
+-----------------+
| io.sort.mb=100  |
+-----------------+
1 row selected (0.008 seconds)
0: jdbc:hive2://hadoop102:10000> set mapreduce.task.io.sort.mb;
+--------------------------------+
|              set                     |
+--------------------------------+
| mapreduce.task.io.sort.mb=100   |
+--------------------------------+
1 row selected (0.008 seconds)
0: jdbc:hive2://hadoop102:10000> set io.sort.mb = 10;
No rows affected (0.005 seconds)
0: jdbc:hive2://hadoop102:10000> set mapreduce.task.io.sort.mb = 10;
No rows affected (0.004 seconds)
解决方案2（永久生效）：
在$HIVE_HOME/conf下添加hive-env.sh。
[atguigu@hadoop102 conf]$ pwd
/opt/module/hive/conf
[atguigu@hadoop102 conf]$ cp hive-env.sh.template hive-env.sh
然后将其中的参数export HADOOP_HEAPSIZE=1024的注释放开，然后重启Hive。
 
 
8）虚拟内存限制
	在yarn-site.xml中添加如下配置:
<property>
    <name>yarn.nodemanager.vmem-check-enabled</name>
    <value>false</value>
 </property>
